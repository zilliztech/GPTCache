## ðŸ¤— Modules Overview

![GPTCache Struct](./GPTCacheStructure.png)

- **LLM Adapter**: The user interface to adapt different LLM model requests to the GPT cache protocol. 
    - [x]  Support OpenAI chatGPT API
    - [ ]  Support for other LLMs, such as Hugging Face Hub, Anthropic, and self-hosted models like LLaMa.
- **Pre-processor**: Extracts the key information from the request and preprocess
    - [x]  Basic analysis and parse of the request
    - [ ]  Apply user-defined preprocessing logic.
- [ ] **Context Buffer**: Maintains session context.
    - [ ] Store conversation windows.
    - [ ] Store conversation summary.
    - [ ] Store prompts.
- **Encoder**: Embed the text into a dense vector for similarity search.
    - [x]  Use [ONNX](https://onnx.ai/) with the GPTCache/paraphrase-albert-onnx for English.
    - [x]  Use the OpenAI embedding API.
    - [x]  Keep the text as a string without any changes.
    - [ ]  Use the [cohere](https://docs.cohere.ai/reference/embed) embedding API.
    - [ ]  Support [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) embedding API.
    - [ ]  Support [fastText](https://fasttext.cc) embedding API.
    - [ ]  Support [Hugging Face](https://huggingface.co/) embedding API.
    - [ ]  Support [SentenceTransformers](https://www.sbert.net) embedding API.
- **Cache manager**: which includes searching, saving, or evicting data. Additional storage support will be added in the future, and contributions are welcome.
    - Scalar store:
        - [x]  Use [SQLite](https://sqlite.org/docs.html).
        - [ ]  Use [PostgreSQL](https://www.postgresql.org/).
        - [ ]  Use [MySQL](https://www.mysql.com/).
        - [ ]  Use [MongoDB](https://www.mongodb.com/)
        - [ ]  Use [MariaDB](https://mariadb.org/).
        - [ ]  Use [SQL Server](https://www.microsoft.com/en-us/sql-server/).
        - [ ]  Use [Oracle](https://www.oracle.com/).
        - [ ]  Use [Redis](https://redis.io/).
        - [ ]  Use [Minio](https://min.io/).
        - [ ]  Use [Habse](https://hbase.apache.org//).
        - [ ]  Use [ElasticSearch](https://www.elastic.co/)
        - [ ]  Use [zincsearch](https://zinc.dev/) 
        - [ ]  Use other scalar databases
    - Vector store:
        - [x]  Use [Milvus](https://milvus.io/).
        - [x]  Use [Zilliz Cloud](https://cloud.zilliz.com/).
        - [x]  Use [FAISS](https://faiss.ai/).
        - [ ]  Use [Qdrant](https://qdrant.tech/)
        - [ ]  Use [Chroma](https://www.trychroma.com/)
        - [ ]  Use [PGVector](https://github.com/pgvector/pgvector)
        - [ ]  Use other vector databases
    - Eviction Policy
        - [x]  LRU eviction policy
        - [x]  FIFO eviction policy
        - [ ]  More complicated eviction policies
- **Ranker**: Evaluate similarity by judging the quality of cached answers.
    - [x] Use the search distance, as described in `simple.py#pair_evaluation`.
    - [x] [ONNX](https://onnx.ai/) uses the GPTCache/albert-duplicate-onnx model for precise comparison between questions and answers. It supports only 512 tokens.
    - [x] Exact string comparison, judge the cache request and the original request based on the exact match of characters.
    - [x] For numpy arrays, use `linalg.norm`.
    - [ ] BM25 and other similarity measurements
    - [ ] Other deap learning models
- **Post-processor**: Determine which cached answers to the user, and generate the response.
    - [X] Choose the most similar answer.
    - [X] Choose randomly.
    - [ ] Other Probability-based policy 