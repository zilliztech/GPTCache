[
    {
        "origin": "Hugging Face Hub is a platform to host Git-based models, datasets, and Spaces.",
        "similar": "Hugging Face Hub serves as a repository for Git-based models, datasets, and Spaces."
    },
    {
        "origin": "Transformers is a state-of-the-art machine learning library for Pytorch, TensorFlow, and JAX.",
        "similar": "Transformers is a cutting-edge machine learning library for Pytorch, TensorFlow, and JAX."
    },
    {
        "origin": "Diffusers are state-of-the-art diffusion models for image and audio generation in PyTorch.",
        "similar": "PyTorch has cutting-edge diffusers for the production of images and sound."
    },
    {
        "origin": "Datasets are a platform to access and share datasets for computer vision, audio, and NLP tasks.",
        "similar": "Datasets provide a means to access and distribute data for computer vision, audio, and NLP applications."
    },
    {
        "origin": "Gradio is a tool to build machine learning demos and other web apps in just a few lines of Python.",
        "similar": "Gradio enables developers to create machine learning demos and web applications with a few lines of Python code."
    },
    {
        "origin": "The Hub Python Library is a client library for the HF Hub that allows you to manage repositories from your Python runtime.",
        "similar": "The Python Library for the HF Hub provides the ability to manage repositories from within a Python environment."
    },
    {
        "origin": "Huggingface.js is a collection of JS libraries to interact with Hugging Face, with TS types included.",
        "similar": "Hugging Face.js is a set of JavaScript libraries that allow for interaction with Hugging Face, complete with TypeScript types."
    },
    {
        "origin": "The Inference API is a platform that allows you to use more than 50k models through a public inference API, with scalability built-in.",
        "similar": "The Inference API provides a platform with the capacity to access over 50k models through a public API, and scalability is already incorporated."
    },
    {
        "origin": "Inference Endpoints are a platform that allows you to easily deploy your model to production on dedicated, fully managed infrastructure.",
        "similar": "Inference Endpoints provide a convenient way to deploy your model to production on dedicated, managed infrastructure."
    },
    {
        "origin": "Accelerate is a tool that allows you to easily train and use PyTorch models with multi-GPU, TPU, mixed-precision.",
        "similar": "Accelerate facilitates the training and utilization of PyTorch models with multi-GPU, TPU, and mixed-precision in a straightforward manner."
    },
    {
        "origin": "Optimum is a tool that allows for fast training and inference of HF Transformers with easy-to-use hardware optimization tools.",
        "similar": "Optimum is a platform that facilitates the swift training and application of HF Transformers with user-friendly hardware optimization capabilities."
    },
    {
        "origin": "Tokenizers are fast tokenizers optimized for both research and production.",
        "similar": "Tokenizers that are designed to be both efficient and effective for both research and production purposes are available."
    },
    {
        "origin": "The Course is a platform that teaches about natural language processing using libraries from the HF ecosystem.",
        "similar": "This Course provides instruction on natural language processing, utilizing libraries from the HF environment."
    },
    {
        "origin": "The Deep RL Course is a platform that teaches about deep reinforcement learning using libraries from the HF ecosystem.",
        "similar": "HF ecosystem libraries are employed to instruct deep reinforcement learning in the Deep RL Course platform."
    },
    {
        "origin": "Evaluate is a tool that allows for easier and more standardized evaluation and reporting of model performance.",
        "similar": "Assessing is a tool that facilitates simpler and more consistent assessment and reporting of model performance."
    },
    {
        "origin": "Tasks are a platform that provides demos, use cases, models, datasets, and more for ML tasks.",
        "similar": "Tasks is a platform that furnishes demos, examples, models, datasets, and more for Machine Learning projects."
    },
    {
        "origin": "Datasets-server is an API that allows access to the contents, metadata, and basic statistics of all Hugging Face Hub datasets.",
        "similar": "Datasets-server provides an API that enables users to access the data, metadata, and basic statistics of all Hugging Face Hub datasets."
    },
    {
        "origin": "Simulate is a tool that allows for the creation and sharing of simulation environments for intelligent agents and synthetic data generation.",
        "similar": "Simulation is a platform that facilitates the building and dissemination of simulation settings for artificial agents and artificial data production."
    },
    {
        "origin": "Amazon SageMaker is a platform that allows for the training and deployment of Transformer models with Amazon SageMaker and Hugging Face DLCs.",
        "similar": "Amazon SageMaker, in combination with Hugging Face DLCs, provides a platform for training and deploying Transformer models."
    },
    {
        "origin": "timm is a platform that provides state-of-the-art computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations, and training/evaluation scripts.",
        "similar": "Timm is a platform furnishing cutting-edge computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations, and training/evaluation scripts."
    },
    {
        "origin": "Safetensors are a simple, safe way to store and distribute tensors.",
        "similar": "Safetensors provide an uncomplicated and secure method of keeping and disseminating tensors."
    },
    {
        "origin": "LOAD_HU is a documentation page.",
        "similar": "LOAD_HU is a web page devoted to providing information."
    },
    {
        "origin": "No, LOAD_HU doesn't exist in v2.10.0.",
        "similar": "LOAD_HU is not a feature of v2.10.0."
    },
    {
        "origin": "You can find LOAD_HU documentation on the main version. Click [here](/docs/datasets/main/en/load_hu) to redirect to the main version of the documentation.",
        "similar": "The LOAD_HU documentation can be located on the main version. Click [here](/docs/datasets/main/en/load_hu) to be taken to the main version of the documentation."
    },
    {
        "origin": "The purpose of the Datasets documentation is to provide information on how to use the Datasets library.",
        "similar": "The objective of the Datasets library documentation is to furnish guidance on its utilization."
    },
    {
        "origin": "The different sections of the Datasets documentation are Get started, Tutorials, How-to guides, General usage, Audio, Vision, Text, Tabular, Dataset repository, Conceptual guides, and Reference.",
        "similar": "The various parts of the Datasets documentation include: Introduction, Tutorials, How-to guides, General usage, Audio, Vision, Text, Tabular, Dataset repository, Conceptual guides, and Reference."
    },
    {
        "origin": "To use Datasets with JAX, you need to install `jax` and `jaxlib` as `pip install datasets[jax]`. Then you can get JAX arrays (numpy-like) by setting the format of the dataset to `jax`.",
        "similar": "In order to utilize Datasets with JAX, you should install `jax` and `jaxlib` by running `pip install datasets[jax]`. Afterwards, you can obtain JAX arrays (similar to numpy) by setting the format of the dataset to `jax`."
    },
    {
        "origin": "A Dataset object is a wrapper of an Arrow table, which allows fast reads from arrays in the dataset to JAX arrays.",
        "similar": "A Dataset object serves as an interface to an Arrow table, enabling rapid conversion of arrays in the dataset to JAX arrays."
    },
    {
        "origin": "When setting the format of a DatasetDict to jax, all the Datasets there will be formatted as jax.",
        "similar": "When formatting a DatasetDict to jax, all the Datasets will be converted to the jax format."
    },
    {
        "origin": "To load the data in the device of your choice, you can specify the device argument, but note that jaxlib.xla_extension.Device is not supported as it\u2019s not serializable with neither pickle not dill, so you\u2019ll need to use its string identifier instead.",
        "similar": "You can specify the device argument to load the data in the device of your choice, however, jaxlib.xla_extension.Device cannot be used as it is not serializable with either pickle or dill, so you must use its string identifier."
    },
    {
        "origin": "If the device argument is not provided to with_format then it will use the default device which is jax.devices()[0].",
        "similar": "If the device argument is not specified for with_format, then the default device, jax.devices()[0], will be used."
    },
    {
        "origin": "By default, N-dimensional arrays are considered as nested lists.",
        "similar": "N-dimensional arrays are typically viewed as nested lists."
    },
    {
        "origin": "A DeviceArray object is a numpy-like array, which is the output of a JAX formatted dataset.",
        "similar": "A DeviceArray object is an array with a structure similar to numpy, produced as the result of a JAX formatted dataset."
    },
    {
        "origin": "ClassLabel data is properly converted to arrays.",
        "similar": "The conversion of ClassLabel data to arrays is done correctly."
    },
    {
        "origin": "The Image and Audio feature types are also supported.",
        "similar": "The Image and Audio types are also accommodated."
    },
    {
        "origin": "To use the Image feature type, you\u2019ll need to install the vision extra as pip install datasets[vision].",
        "similar": "You'll have to pip install datasets[vision] to be able to utilize the Image feature type."
    },
    {
        "origin": "No, it doesn't exist in version 2.10.0.",
        "similar": "Version 2.10.0 does not have it."
    },
    {
        "origin": "You can find it on the main version of the documentation by clicking on the provided link.",
        "similar": "You can access the main version of the documentation by clicking on the link given."
    },
    {
        "origin": "No, there is no alternative mentioned in the given document.",
        "similar": "No other option is specified in the given document."
    },
    {
        "origin": "No, it doesn't exist in version 2.10.0.",
        "similar": "Version 2.10.0 does not include it."
    },
    {
        "origin": "You can find it on the main version of the documentation by clicking on the provided link.",
        "similar": "You can access the main version of the documentation by clicking on the link given."
    },
    {
        "origin": "The document doesn't mention any alternative to the UPLOAD_DATASE documentation page in version 2.10.0.",
        "similar": "No alternative to the UPLOAD_DATASE documentation page in version 2.10.0 is mentioned in the document."
    },
    {
        "origin": "No, the documentation page STREA doesn't exist in version 2.10.0.",
        "similar": "Version 2.10.0 does not contain the documentation page STREA."
    },
    {
        "origin": "You can find the documentation page STREA on the main version. Click on the provided link to redirect to the main version of the documentation.",
        "similar": "The documentation page for STREA can be accessed by clicking on the link which will take you to the main version."
    },
    {
        "origin": "The Datasets documentation provides information on how to use and work with datasets in the Hugging Face library.",
        "similar": "The Hugging Face library's Datasets documentation offers guidance on utilizing and manipulating datasets."
    },
    {
        "origin": "The Datasets documentation is divided into different sections such as Get started, Tutorials, How-to guides, Audio, Vision, Text, Tabular, Dataset repository, Conceptual guides, and Reference.",
        "similar": "The Datasets documentation is broken down into various categories including Get going, Tutorials, How-to guides, Audio, Vision, Text, Tabular, Dataset library, Conceptual guides, and Reference."
    },
    {
        "origin": "Yes, Datasets supports access to cloud storage providers through a `fsspec` FileSystem implementations.",
        "similar": "Datasets can be accessed from cloud storage providers using a `fsspec` FileSystem implementation."
    },
    {
        "origin": "Some examples of supported cloud storage providers in Datasets are Amazon S3, Google Cloud Storage, Azure Blob/DataLake, Dropbox, and Google Drive.",
        "similar": "Examples of cloud storage providers that are compatible with Datasets include Amazon S3, Google Cloud Storage, Azure Blob/DataLake, Dropbox, and Google Drive."
    },
    {
        "origin": "You can load and save datasets from cloud storage in Datasets using the `fsspec` FileSystem implementations.",
        "similar": "Datasets allows you to upload and store data sets in the cloud with the help of `fsspec` FileSystem implementations."
    },
    {
        "origin": "This guide is about how to save and load datasets with any cloud storage.",
        "similar": "This guide provides instructions on how to store and retrieve datasets using any cloud storage."
    },
    {
        "origin": "The examples of cloud storage mentioned in this guide are S3, Google Cloud Storage, and Azure Blob Storage.",
        "similar": "This guide mentions S3, Google Cloud Storage, and Azure Blob Storage as examples of cloud storage."
    },
    {
        "origin": "You can install the S3 FileSystem implementation by running the command \"pip install s3fs\".",
        "similar": "You can get the S3 FileSystem implementation up and running by executing the command \"pip install s3fs\"."
    },
    {
        "origin": "To use an anonymous connection, use \"anon=True\". Otherwise, include your \"aws_access_key_id\" and \"aws_secret_access_key\" whenever you are interacting with a private S3 bucket.",
        "similar": "If you wish to keep your connection anonymous, set \"anon=True\". Otherwise, make sure to provide your \"aws_access_key_id\" and \"aws_secret_access_key\" when accessing a private S3 bucket."
    },
    {
        "origin": "You can create your FileSystem instance for S3 by importing s3fs and running \"fs = s3fs.S3FileSystem(**storage_options)\".",
        "similar": "By importing s3fs and executing \"fs = s3fs.S3FileSystem(**storage_options)\", you can generate a FileSystem instance for S3."
    },
    {
        "origin": "You can install the Google Cloud Storage implementation by running the command \"conda install -c conda-forge gcsfs\" or \"pip install gcsfs\".",
        "similar": "To install the Google Cloud Storage implementation, you can execute either \"conda install -c conda-forge gcsfs\" or \"pip install gcsfs\" command."
    },
    {
        "origin": "You can define your credentials for Google Cloud Storage by specifying \"token\": \"anon\" for an anonymous connection, or \"project\": \"my-google-project\" for using your default gcloud credentials or from the google metadata service.",
        "similar": "You can set your credentials for Google Cloud Storage by indicating \"token\": \"anon\" for an anonymous connection, or \"project\": \"my-google-project\" to use your default gcloud credentials or from the google metadata service."
    },
    {
        "origin": "You can create your FileSystem instance for Google Cloud Storage by importing gcsfs and running \"fs = gcsfs.GCSFileSystem(**storage_options)\".",
        "similar": "By importing gcsfs and executing \"fs = gcsfs.GCSFileSystem(**storage_options)\", you can generate a FileSystem instance for Google Cloud Storage."
    },
    {
        "origin": "You can install the Azure Blob Storage implementation by running the command \"conda install -c conda-forge adlfs\" or \"pip install adlfs\".",
        "similar": "You can get the Azure Blob Storage implementation up and running by executing the command \"conda install -c conda-forge adlfs\" or \"pip install adlfs\"."
    },
    {
        "origin": "You can define your credentials for Azure Blob Storage by specifying \"anon\": True for an anonymous connection, or \"account_name\": ACCOUNT_NAME and \"account_key\": ACCOUNT_KEY for the gen 2 filesystem, or \"tenant_id\": TENANT_ID, \"client_id\": CLIENT_ID, and \"client_secret\": CLIENT_SECRET for the gen 1 filesystem.",
        "similar": "To set up your credentials for Azure Blob Storage, you can use \"anon\": True for an anonymous connection, or \"account_name\": ACCOUNT_NAME and \"account_key\": ACCOUNT_KEY for the gen 2 filesystem, or \"tenant_id\": TENANT_ID, \"client_id\": CLIENT_ID, and \"client_secret\": CLIENT_SECRET for the gen 1 filesystem."
    },
    {
        "origin": "You can create your FileSystem instance for Azure Blob Storage by importing adlfs and running \"fs = adlfs.AzureBlobFileSystem(**storage_options)\".",
        "similar": "By importing adlfs and executing \"fs = adlfs.AzureBlobFileSystem(**storage_options)\", you can generate your own FileSystem instance for Azure Blob Storage."
    },
    {
        "origin": "You can download and prepare a dataset into a cloud storage by specifying a remote \"output_dir\" in \"download_and_prepare\". Don\u2019t forget to use the previously defined \"storage_options\" containing your credentials to write into a private cloud storage.",
        "similar": "By specifying a remote \"output_dir\" in \"download_and_prepare\", you can download and store a dataset into the cloud storage. Remember to include the \"storage_options\" with your credentials to enable writing into a private cloud storage."
    },
    {
        "origin": "The \"download_and_prepare\" method works in two steps: 1) it first downloads the raw data files (if any) in your local cache, and 2) then it generates the dataset in Arrow or Parquet format in your cloud storage by iterating over the raw data files.",
        "similar": "The \"download_and_prepare\" method is a two-step process: it first stores the raw data files (if any) in the local cache, and then it iterates over these files to create the dataset in Arrow or Parquet format in the cloud storage."
    },
    {
        "origin": "You can load a dataset builder from the Hugging Face Hub by running \"builder = load_dataset_builder(\"imdb\")\" and then running \"builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\")\".",
        "similar": "To access a dataset builder from the Hugging Face Hub, execute \"builder = load_dataset_builder(\"imdb\")\" and then \"builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\")\"."
    },
    {
        "origin": "You can load a dataset builder using a loading script by running \"builder = load_dataset_builder(\"path/to/local/loading_script/loading_script.py\")\" and then running \"builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\")\".",
        "similar": "To load a dataset builder using a loading script, execute \"builder = load_dataset_builder(\"path/to/local/loading_script/loading_script.py\")\" and then \"builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\")\"."
    },
    {
        "origin": "You can use your own data files by following the instructions in the \"how to load local and remote files\" section of the guide.",
        "similar": "By adhering to the directions in the \"how to load local and remote files\" section of the guide, you can employ your own data files."
    },
    {
        "origin": "It is recommended to save the files as compressed Parquet files to optimize I/O.",
        "similar": "It is suggested to store the files as compressed Parquet files for optimized I/O."
    },
    {
        "origin": "Yes, the size of the shards can be specified using `max_shard_size`.",
        "similar": "It is possible to determine the size of the shards by using `max_shard_size`."
    },
    {
        "origin": "Dask is a parallel computing library and it has a pandas-like API for working with larger than memory Parquet datasets in parallel. Dask can use multiple threads or processes on a single machine, or a cluster of machines to process data in parallel. Dask supports local data but also data from a cloud storage. It can be used to load a dataset saved as sharded Parquet files.",
        "similar": "Dask is a parallel computing library that offers a pandas-like API for processing Parquet datasets that exceed memory capacity. It can be employed to utilize multiple threads or processes on a single machine, or a cluster of machines, and it is compatible with both local and cloud-based data. Furthermore, it is capable of loading datasets stored as sharded Parquet files."
    },
    {
        "origin": "Serialized datasets can be saved to cloud storage using `Dataset.save_to_disk()`.",
        "similar": "`Dataset.save_to_disk()` can be used to store serialized datasets in cloud storage."
    },
    {
        "origin": "Files can be listed from a cloud storage using `fs.ls` with the FileSystem instance `fs`.",
        "similar": "Using the FileSystem instance `fs`, `fs.ls` can be used to list files from a cloud storage."
    },
    {
        "origin": "Serialized datasets can be loaded from cloud storage using `Dataset.load_from_disk()`.",
        "similar": "`Dataset.load_from_disk()` can be used to retrieve serialized datasets from cloud storage."
    },
    {
        "origin": "This document is the documentation for the Datasets library, providing information on how to use and process various types of datasets.",
        "similar": "This document serves as a guide to the Datasets library, offering instructions on how to utilize and manipulate different types of datasets."
    },
    {
        "origin": "The different sections in this document include getting started, tutorials, how-to guides, general usage, audio, vision, text, tabular, dataset repository, conceptual guides, and reference.",
        "similar": "This document is divided into sections such as initiation, tutorials, instructions, general utilization, sound, sight, written material, tabular data, dataset depository, conceptual instructions, and reference."
    },
    {
        "origin": "The audio section of the document covers how to load, process, and create audio datasets, including specific methods for resampling the sampling rate and using map() with audio datasets.",
        "similar": "This document provides information on how to load, process, and generate audio datasets, with particular focus on techniques such as resampling the sampling rate and the utilization of map() with audio datasets."
    },
    {
        "origin": "The cast_column() function is used to cast a column to another feature to be decoded, and when used with the Audio feature, it can be used to resample the sampling rate.",
        "similar": "The cast_column() function can be employed to transform a column into a different feature to be decoded, and when combined with the Audio feature, it can be used to alter the sampling rate."
    },
    {
        "origin": "Audio files are decoded and resampled on-the-fly to 16kHz.",
        "similar": "The decoding and resampling of audio files is done in real time to 16kHz."
    },
    {
        "origin": "The map() function helps preprocess the entire dataset at once.",
        "similar": "The map() function assists in preprocessing the whole dataset in one go."
    },
    {
        "origin": "For pretrained speech recognition models, you need to load a feature extractor and tokenizer and combine them in a processor.",
        "similar": "You must combine a feature extractor, tokenizer, and processor to utilize pretrained speech recognition models."
    },
    {
        "origin": "For fine-tuned speech recognition models, you only need to load a processor.",
        "similar": "A processor is all that is required to utilize a fine-tuned speech recognition model."
    },
    {
        "origin": "Include the audio column in the preprocessing function.",
        "similar": "Incorporate the audio feature into the preprocessing routine."
    },
    {
        "origin": "No, the documentation page SHAR doesn't exist in version 2.10.0.",
        "similar": "Version 2.10.0 does not have the SHAR documentation page."
    },
    {
        "origin": "You can find the documentation page SHAR on the main version. Click [here](/docs/datasets/main/en/shar) to redirect to the main version of the documentation.",
        "similar": "The SHAR documentation page can be accessed from the main version. To go to the main version of the documentation, click [here](/docs/datasets/main/en/shar)."
    },
    {
        "origin": "No, it doesn't exist in version 2.10.0.",
        "similar": "Version 2.10.0 does not contain it."
    },
    {
        "origin": "It exists on the main version of the documentation. You can click on the provided link to redirect to the main version of the documentation.",
        "similar": "The main version of the documentation can be accessed by clicking on the link."
    },
    {
        "origin": "A fingerprint in \ud83e\udd17 Datasets is a unique identifier for a dataset that is updated every time a transform is applied to it. It is computed by combining the fingerprint of the previous state and a hash of the latest transform applied.",
        "similar": "A fingerprint in Datasets is a distinctive marker for a dataset that is modified each time a transformation is executed on it. It is generated by combining the fingerprint of the prior state and a hash of the most recent transformation carried out."
    },
    {
        "origin": "Fingerprints in \ud83e\udd17 Datasets are computed by hashing the function passed to `map` as well as the `map` parameters (`batch_size`, `remove_columns`, etc.).",
        "similar": "The `map` parameters (`batch_size`, `remove_columns`, etc.) and the function passed to `map` are used to calculate Fingerprints in \ud83e\udd17 Datasets through hashing."
    },
    {
        "origin": "When a non-hashable transform is used in \ud83e\udd17 Datasets, a random fingerprint is assigned instead, and a warning is raised. The non-hashable transform is considered different from the previous transforms, and as a result, \ud83e\udd17 Datasets will recompute all the transforms.",
        "similar": "When a non-hashable transform is used in \ud83e\udd17 Datasets, a unique identifier is assigned to it and a warning is issued. This transform is seen as distinct from the prior ones, thus \ud83e\udd17 Datasets will recalculate all the transforms."
    },
    {
        "origin": "One can check the hash of any Python object in \ud83e\udd17 Datasets using the `fingerprint.Hasher` module.",
        "similar": "The `fingerprint.Hasher` module can be used to generate the hash of any Python object in \ud83e\udd17 Datasets."
    },
    {
        "origin": "The hash in \ud83e\udd17 Datasets is computed by dumping the object using a `dill` pickler and hashing the dumped bytes. The pickler recursively dumps all the variables used in the function, so any change made to an object used in the function will cause the hash to change.",
        "similar": "The \ud83e\udd17 Datasets hash is generated by taking the object and serializing it with a `dill` pickler, then hashing the resulting bytes. As the pickler recursively dumps all the variables used in the function, any alteration to an object used in the function will cause the hash to be altered."
    },
    {
        "origin": "To avoid recomputing all the transforms in \ud83e\udd17 Datasets, one should ensure that their transforms are serializable with pickle or dill. Additionally, when caching is disabled, one should use `Dataset.save_to_disk()` to save their transformed dataset, or it will be deleted once the session ends.",
        "similar": "In order to prevent having to recalculate all the transformations in \ud83e\udd17 Datasets, it is necessary to make sure that the transformations are serializable with pickle or dill. Furthermore, when caching is disabled, `Dataset.save_to_disk()` should be used to save the transformed dataset, or else it will be lost when the session ends."
    },
    {
        "origin": "There are several methods for creating and sharing an audio dataset, including creating it from local files in python using Dataset.push_to_hub().",
        "similar": "Using python, one can create an audio dataset from local files and share it with Dataset.push_to_hub(), among other methods."
    },
    {
        "origin": "Yes, you can share your audio dataset with your team or anyone in the community by creating a dataset repository on the Hugging Face Hub.",
        "similar": "It is possible to make your audio dataset available to your team or anyone in the community by setting up a dataset repository on the Hugging Face Hub."
    },
    {
        "origin": "The `AudioFolder` builder is a no-code solution for quickly creating an audio dataset with several thousand audio files.",
        "similar": "The `AudioFolder` builder is a fast way to generate an audio dataset with thousands of audio files without any coding."
    },
    {
        "origin": "The alternative method for creating an audio dataset is by writing a loading script, which is for advanced users and requires more effort and coding.",
        "similar": "For those who are more experienced and willing to put in extra effort, writing a loading script is another way to create an audio dataset."
    },
    {
        "origin": "You can control access to your dataset by requiring users to share their contact information first, using the Gated datasets feature.",
        "similar": "Requiring users to provide their contact information before accessing your dataset can be done through the Gated datasets feature."
    },
    {
        "origin": "You can load your own dataset using the paths to your audio files and the `cast_column()` function to take a column of audio file paths and cast it to the `Audio` feature.",
        "similar": "You can use the `cast_column()` function to take a column of audio file paths and cast it to the `Audio` feature, thereby enabling you to load your own dataset with the paths to your audio files."
    },
    {
        "origin": "You can upload your dataset to the Hugging Face Hub using `Dataset.push_to_hub()`.",
        "similar": "You can push your dataset to the Hugging Face Hub by utilizing `Dataset.push_to_hub()`."
    },
    {
        "origin": "The metadata file for the `AudioFolder` builder should include a `file_name` column to link an audio file to its metadata.",
        "similar": "A `file_name` column should be included in the metadata file for the `AudioFolder` builder to link an audio file to its corresponding metadata."
    },
    {
        "origin": "The directory should have a `data` folder with subfolders for each split (`train`, `test`, etc.), and each split folder should contain the audio files and a metadata file with a `file_name` column specifying the relative path to each audio file.",
        "similar": "A `data` folder should be present in the directory, with subfolders for each split (e.g. `train`, `test`) containing the audio files and a metadata file with a `file_name` column that indicates the relative path of each audio file."
    },
    {
        "origin": "If the audio dataset doesn't have any associated metadata, `AudioFolder` will create a `label` column based on the directory name (language id).",
        "similar": "`AudioFolder` will generate a `label` column based on the directory name (language id) in the absence of any associated metadata in the audio dataset."
    },
    {
        "origin": "Yes, in that case the `file_name` column in the metadata file should be a full relative path to the audio file, not just its filename.",
        "similar": "In that situation, the `file_name` column in the metadata file should contain the full relative path to the audio file, not just its name."
    },
    {
        "origin": "The script should define the dataset's splits and configurations, handle downloading and generating the dataset examples, and support streaming mode. The script should be named after the dataset folder and located in the same directory as the `data` folder.",
        "similar": "The script, named after the dataset folder and located in the same directory as the `data` folder, should be responsible for defining the dataset's splits and configurations, downloading and generating the dataset examples, and providing streaming mode."
    },
    {
        "origin": "The purpose of the my_dataset.py file is not specified in the given document.",
        "similar": "The given document does not provide any information about the purpose of the my_dataset.py file."
    },
    {
        "origin": "The data folder includes train.tar.gz, test.tar.gz, and metadata.csv.",
        "similar": "The data folder contains train.tar.gz, test.tar.gz, and metadata.csv as its contents."
    },
    {
        "origin": "You will learn how to create a streamable dataset, create a dataset builder class, create dataset configurations, add dataset metadata, download and define the dataset splits, generate the dataset, and upload the dataset to the Hub.",
        "similar": "You will be taught how to make a streamable collection of data, devise a dataset constructor class, devise dataset setups, include dataset metadata, download and specify the dataset divisions, generate the dataset, and post the dataset to the Hub."
    },
    {
        "origin": "The base class for datasets generated from a dictionary generator is GeneratorBasedBuilder.",
        "similar": "GeneratorBasedBuilder serves as the basis for datasets created from a dictionary generator."
    },
    {
        "origin": "The three methods to help create a dataset within the GeneratorBasedBuilder class are _info, _split_generators, and _generate_examples.",
        "similar": "The GeneratorBasedBuilder class provides three approaches for constructing a dataset, namely _info, _split_generators, and _generate_examples."
    },
    {
        "origin": "To create different configurations for a dataset, use the BuilderConfig class to create a subclass of your dataset.",
        "similar": "By subclassing your dataset, you can use the BuilderConfig class to generate various configurations for the dataset."
    },
    {
        "origin": "You can define your configurations in the `BUILDER_CONFIGS` class variable inside the GeneratorBasedBuilder class.",
        "similar": "You can specify your configurations within the `BUILDER_CONFIGS` class variable of the GeneratorBasedBuilder class."
    },
    {
        "origin": "You can load a specific configuration using load_dataset() by specifying the dataset name, configuration name, and split.",
        "similar": "By providing the dataset name, configuration name, and split, you can employ load_dataset() to load a particular configuration."
    },
    {
        "origin": "You can add metadata to your dataset by defining a DatasetInfo class with information such as description, features, homepage, license, and citation.",
        "similar": "By creating a DatasetInfo class containing details such as description, features, homepage, license, and citation, you can add metadata to your dataset."
    },
    {
        "origin": "Some important features to include in the DatasetInfo class for an audio loading script are the Audio feature and the sampling rate of the dataset.",
        "similar": "Including the Audio feature and the sampling rate of the dataset are two essential elements to be included in the DatasetInfo class for an audio loading script."
    },
    {
        "origin": "The purpose of the `_generate_examples` method is to yield examples as (key, example) tuples.",
        "similar": "The `_generate_examples` method is designed to produce (key, example) pairs as output."
    },
    {
        "origin": "The `load_dataset` function loads a dataset from the Hub.",
        "similar": "The `load_dataset` function fetches a dataset from the Hub."
    },
    {
        "origin": "TAR archives can be extracted locally using the `extract` method in non-streaming mode and passing the local path to the extracted archive directory to the next step in `gen_kwargs`.",
        "similar": "The `extract` method in non-streaming mode can be used to extract TAR archives locally, with the local path to the extracted archive directory passed to the next step in `gen_kwargs`."
    },
    {
        "origin": "The DownloadManager class is used to download and extract TAR archives in non-streaming mode.",
        "similar": "The DownloadManager class facilitates the downloading and unpacking of TAR archives without streaming."
    },
    {
        "origin": "The `download_and_extract()` method should be used to download the metadata file specified in `_METADATA_URL`.",
        "similar": "The `_METADATA_URL` should be used with the `download_and_extract()` method to download the metadata file."
    },
    {
        "origin": "The SplitGenerator class is used to organize the audio files and metadata in each split.",
        "similar": "The SplitGenerator class is employed to arrange the audio files and metadata for each split."
    },
    {
        "origin": "The standard names for the splits are `Split.TRAIN`, `Split.TEST`, and `SPLIT.Validation`.",
        "similar": "The designations for the splits are usually `Split.TRAIN`, `Split.TEST`, and `SPLIT.Validation`."
    },
    {
        "origin": "The `_generate_examples` method is used to access and yield TAR files sequentially, and to associate the metadata in `metadata_path` with the audio files in the TAR file.",
        "similar": "The `_generate_examples` method is employed to sequentially access and yield TAR files, and to link the metadata from `metadata_path` with the audio files in the TAR file."
    },
    {
        "origin": "The files yielded by iter_archive() are in the form of a tuple of (path, f) where path is a relative path to a file inside the archive, and f is the file object itself.",
        "similar": "Iter_archive() produces a tuple of (path, f) as output, where path is a relative path to a file within the archive and f is the file object."
    },
    {
        "origin": "To get the full path to the locally extracted file, you need to join the path of the directory where the archive is extracted to and the relative audio file path. This can be done using the os.path.join() function.",
        "similar": "To obtain the complete route to the locally extracted file, you must combine the directory path where the archive is extracted and the relative audio file path by using the os.path.join() function."
    },
    {
        "origin": "The _generate_examples() method yields examples by iterating over the audio files and metadata, setting the audio feature and the path to the extracted file, and then yielding the result.",
        "similar": "By looping through the audio files and metadata, the _generate_examples() method produces examples by assigning the audio feature and the path to the extracted file, and then outputting the result."
    },
    {
        "origin": "Dataset streaming allows working with a dataset without downloading it. The data is streamed as you iterate over the dataset.",
        "similar": "Streaming datasets enable the ability to work with the data without needing to download it, as the iteration over the dataset is done in real-time."
    },
    {
        "origin": "Dataset streaming is helpful when you don't want to wait for an extremely large dataset to download, the dataset size exceeds the amount of available disk space on your computer, or you want to quickly explore just a few samples of a dataset.",
        "similar": "Streaming datasets is beneficial when you don't want to wait for a huge dataset to download, the size of the dataset surpasses the disk space available on your computer, or you need to quickly analyze a few samples of a dataset."
    },
    {
        "origin": "The benefits of using dataset streaming include faster exploration of datasets, the ability to work with larger datasets without needing to download them, and the ability to work with datasets even if you don't have enough disk space to store them.",
        "similar": "Dataset streaming offers a range of advantages, such as expedited investigation of datasets, the capacity to handle larger datasets without downloading them, and the possibility of working with datasets even if you don't possess enough disk storage."
    },
    {
        "origin": "To use dataset streaming, you can iterate over the dataset and the data will be streamed as you go. This is especially useful for exploring a dataset or working with a large dataset that you don't want to download.",
        "similar": "By utilizing dataset streaming, you can traverse through the dataset and the data will be streamed as you progress. This is especially advantageous when investigating a dataset or managing a large dataset that you don't wish to download."
    },
    {
        "origin": "Dataset streaming is available for some datasets, but not all. You should check the documentation for the specific dataset you are interested in to see if streaming is available.",
        "similar": "It is not guaranteed that streaming is available for all datasets, so you should consult the documentation of the particular dataset you are interested in to find out if streaming is an option."
    },
    {
        "origin": "The dataset is 1.2 terabytes.",
        "similar": "The dataset is of 1.2 terabytes in size."
    },
    {
        "origin": "You can stream a dataset by setting `streaming=True` in `load_dataset()` function.",
        "similar": "By setting `streaming=True` in the `load_dataset()` function, streaming of a dataset can be enabled."
    },
    {
        "origin": "Yes, you can use dataset streaming to work with a local dataset without doing any conversion.",
        "similar": "It is possible to work with a local dataset without needing to convert it, by using dataset streaming."
    },
    {
        "origin": "Dataset streaming is especially helpful when you don\u2019t want to wait for an extremely large local dataset to be converted to Arrow, the converted files size would exceed the amount of available disk space on your computer, or you want to quickly explore just a few samples of a dataset.",
        "similar": "Streaming datasets can be particularly useful when you don't want to wait for a huge local dataset to be converted to Arrow, as the resulting file size may exceed the disk capacity of your computer, or you just want to take a quick look at a few samples of the dataset."
    },
    {
        "origin": "An IterableDataset is a special type of dataset created when loading a dataset in streaming mode.",
        "similar": "A IterableDataset is a specific dataset generated when loading a dataset in streaming mode."
    },
    {
        "origin": "An IterableDataset is useful for iterative jobs like training a model.",
        "similar": "A IterableDataset is advantageous for iterative tasks such as training a model."
    },
    {
        "origin": "Yes, you can shuffle an IterableDataset with `IterableDataset.shuffle()`.",
        "similar": "It is possible to randomize the order of an IterableDataset using the `IterableDataset.shuffle()` method."
    },
    {
        "origin": "You can use `IterableDataset.set_epoch()` in between epochs to tell the dataset what epoch you\u2019re on.",
        "similar": "You can call `IterableDataset.set_epoch()` to indicate the current epoch when switching between epochs."
    },
    {
        "origin": "You can split your dataset using `IterableDataset.take()` or `IterableDataset.skip()` methods.",
        "similar": "You can divide your dataset by employing the `IterableDataset.take()` and `IterableDataset.skip()` methods."
    },
    {
        "origin": "Yes, you can use `interleave_datasets()` method to combine an `IterableDataset` with other datasets.",
        "similar": "It is possible to merge an `IterableDataset` with other datasets by using the `interleave_datasets()` method."
    },
    {
        "origin": "You can use methods like `IterableDataset.rename_column()`, `IterableDataset.remove_columns()`, and `IterableDataset.cast()` to modify the columns of a dataset.",
        "similar": "Methods such as `IterableDataset.rename_column()`, `IterableDataset.remove_columns()`, and `IterableDataset.cast()` can be employed to alter the columns of a dataset."
    },
    {
        "origin": "Use `IterableDataset.rename_column()` with the name of the original column and the new column name.",
        "similar": "Rename the original column to a new one using `IterableDataset.rename_column()`."
    },
    {
        "origin": "Use `IterableDataset.remove_columns()` with the name of the column(s) to remove.",
        "similar": "You can use `IterableDataset.remove_columns()` to eliminate the column(s) by specifying its name."
    },
    {
        "origin": "Use `IterableDataset.cast()` with your new `Features` as its argument. Use `IterableDataset.cast_column()` to change the feature type of just one column.",
        "similar": "The `IterableDataset.cast()` should be used with the new `Features` as its argument, while `IterableDataset.cast_column()` is to be used for altering the feature type of a single column."
    },
    {
        "origin": "Use `IterableDataset.map()` to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns.",
        "similar": "`IterableDataset.map()` can be used to apply a processing function to each example in a dataset, either individually or in batches. This function can even generate new columns and rows."
    },
    {
        "origin": "IterableDataset can be integrated into a training loop by first shuffling the dataset.",
        "similar": "The IterableDataset can be incorporated into a training loop by first randomly rearranging the dataset."
    },
    {
        "origin": "The code to shuffle the dataset in Pytorch is:\n```\nseed, buffer_size = 42, 10_000\ndataset = dataset.shuffle(seed, buffer_size=buffer_size)\n```",
        "similar": "To randomize the dataset in Pytorch, the code is:\nseed, buffer_size = 42, 10_000\ndataset = dataset.randomize(seed, buffer_size=buffer_size)"
    },
    {
        "origin": "The code to create a simple training loop and start training in Pytorch is:\n```\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\ndataset = dataset.with_format(\"torch\")\ndataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu' \nmodel = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\nmodel.train().to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\nfor epoch in range(3):\n    dataset.set_epoch(epoch)\n    for i, batch in enumerate(tqdm(dataloader, total=5)):\n        if i == 5:\n            break\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs[0]\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        if i % 10 == 0:\n            print(f\"loss: {loss}\")\n```",
        "similar": "To create and initiate a training loop in Pytorch, the following code can be used:\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling\nfrom tqdm import tqdm\ndataset = dataset.with_format(\"torch\")\ndataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu' \nmodel = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\nmodel.train().to(device)\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n\nfor epoch in range(3):\n    dataset.set_epoch(epoch)\n    for i, batch in enumerate(tqdm(dataloader, total=5)):\n        if i == 5:\n            break\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs[0]\n        loss.backward()\n        optimizer.step()\n        optimizer"
    },
    {
        "origin": "The Datasets documentation provides information on how to use the Datasets library, including tutorials, how-to guides, and reference materials.",
        "similar": "The Datasets library is explained in the documentation, which includes tutorials, how-to guides, and reference materials for utilization."
    },
    {
        "origin": "The Datasets documentation covers topics such as audio, vision, text, and tabular data, as well as dataset creation and sharing.",
        "similar": "The Datasets manual covers topics like audio, vision, text, tabular data, and how to create and share datasets."
    },
    {
        "origin": "The \"All about metrics\" section provides information on how to use NLP metrics in the Datasets library, including how to load and compute metrics for evaluating model performance.",
        "similar": "The \"All about metrics\" section gives instructions on how to utilize NLP metrics in the Datasets library, such as loading and calculating metrics to assess model effectiveness."
    },
    {
        "origin": "No, the \"Metrics\" section is deprecated in the Datasets library. Users should refer to the library \"Evaluate\" for information on using metrics.",
        "similar": "The \"Metrics\" section of the Datasets library is no longer available; users should look to the \"Evaluate\" library for guidance on metrics."
    },
    {
        "origin": "The load_metric() function is used to download and import the metric loading script from GitHub, which contains information about the metric such as its citation, homepage, and description.",
        "similar": "The load_metric() function is employed to obtain and incorporate the metric loading script from GitHub, which holds data about the metric including its citation, homepage, and explanation."
    },
    {
        "origin": "The Metric object stores the predictions and references, which are needed to compute the metric values. It is stored as an Apache Arrow table, allowing for lazy computation of the metric and making it easier to gather all the predictions in a distributed setting.",
        "similar": "The Metric object is stored as an Apache Arrow table, which holds the predictions and references required to calculate the metric values. This setup allows for the metric to be computed lazily, making it simpler to accumulate all the predictions in a distributed environment."
    },
    {
        "origin": "\ud83e\udd17 Datasets only computes the final metric on the first node, while the predictions and references are computed and provided to the metric separately for each node. These are temporarily stored in an Apache Arrow table, avoiding cluttering the GPU or CPU memory. Once it has gathered all the predictions and references, Metric.compute() will perform the final metric evaluation.",
        "similar": "The final metric is only computed on the first node, while the predictions and references are computed and stored in an Apache Arrow table, avoiding the usage of GPU or CPU memory. Then, Metric.compute() will be used to perform the evaluation when all the predictions and references have been gathered."
    },
    {
        "origin": "No, it doesn't exist in v2.10.0.",
        "similar": "It is not available in version 2.10.0."
    },
    {
        "origin": "It exists on the main version and can be accessed by clicking on the provided link (/docs/datasets/main/en/how_to_metric).",
        "similar": "The main version has it and it can be reached by tapping the link (/docs/datasets/main/en/how_to_metric) given."
    },
    {
        "origin": "LOAD_HU is a documentation page.",
        "similar": "LOAD_HU is a web page for providing information."
    },
    {
        "origin": "No, LOAD_HU doesn't exist in version 2.10.0.",
        "similar": "LOAD_HU is not available in version 2.10.0."
    },
    {
        "origin": "You can find LOAD_HU documentation on the main version. Click on the provided link to redirect to the main version of the documentation.",
        "similar": "By following the link, you can access the LOAD_HU documentation on the main version."
    },
    {
        "origin": "The Datasets documentation provides information on how to use the Datasets library.",
        "similar": "The documentation for the Datasets library outlines how to utilize it."
    },
    {
        "origin": "The Datasets library can be used with TensorFlow, PyTorch, and JAX.",
        "similar": "TensorFlow, PyTorch, and JAX are compatible with the Datasets library."
    },
    {
        "origin": "The \"Use with JAX\" section provides information on how to use the Datasets library with the JAX library, with a focus on training JAX models.",
        "similar": "This section outlines the usage of the Datasets library with JAX, particularly for training JAX models."
    },
    {
        "origin": "To use the code examples in the \"Use with JAX\" section, the user must have the jax and jaxlib libraries installed.",
        "similar": "In order to utilize the code examples in the \"Use with JAX\" section, the user must have the jax and jaxlib libraries installed."
    },
    {
        "origin": "By default, datasets return regular Python objects: integers, floats, strings, lists, etc., and string and binary objects are unchanged.",
        "similar": "Datasets usually return regular Python objects such as integers, floats, strings, and lists, while string and binary objects remain unchanged by default."
    },
    {
        "origin": "To get JAX arrays (numpy-like) instead, you can set the format of the dataset to `jax`.",
        "similar": "To obtain JAX arrays (similar to numpy), you can set the format of the dataset to `jax`."
    },
    {
        "origin": "A Dataset object is a wrapper of an Arrow table, which allows fast reads from arrays in the dataset to JAX arrays.",
        "similar": "A Dataset object acts as a container for an Arrow table, enabling quick conversion of arrays in the dataset to JAX arrays."
    },
    {
        "origin": "When setting the format of a `DatasetDict` to `jax`, all the `Dataset`s there will be formatted as `jax`.",
        "similar": "By setting the `DatasetDict` to `jax`, all the `Dataset`s within it will be formatted in `jax` style."
    },
    {
        "origin": "The formatting is not applied until you actually access the data. So if you want to get a JAX array out of a dataset, you\u2019ll need to access the data first, otherwise the format will remain the same.",
        "similar": "In order to get a JAX array out of a dataset, you must access the data first, as the formatting will not be applied until then. Otherwise, the format will stay the same."
    },
    {
        "origin": "To load the data in the device of your choice, you can specify the `device` argument.",
        "similar": "You can specify the `device` argument to upload the data to the device of your choice."
    },
    {
        "origin": "If the `device` argument is not provided to `with_format` then it will use the default device which is `jax.devices()[0]`.",
        "similar": "If `device` argument is not specified for `with_format`, it will resort to the default device, which is `jax.devices()[0]`."
    },
    {
        "origin": "By default, N-dimensional arrays are considered as nested lists.",
        "similar": "N-dimensional arrays are usually thought of as nested lists."
    },
    {
        "origin": "ClassLabel data is properly converted to arrays.",
        "similar": "The data of ClassLabel is effectively transformed into arrays."
    },
    {
        "origin": "String and binary objects are unchanged, while the Image and Audio feature types are also supported.",
        "similar": "The Image and Audio feature types are supported, and String and binary objects remain the same."
    },
    {
        "origin": "No, the INSTALLATIO page doesn't exist in version 2.10.0.",
        "similar": "The INSTALLATIO page is not available in version 2.10.0."
    },
    {
        "origin": "You can find the INSTALLATIO page on the main version of the documentation. Click on the provided link to redirect to the main version.",
        "similar": "The main version of the documentation contains the INSTALLATION page. Click the link to be directed there."
    },
    {
        "origin": "No, there is no alternative to access the INSTALLATIO page in version 2.10.0. You need to redirect to the main version of the documentation.",
        "similar": "You cannot access the INSTALLATION page in version 2.10.0, so you must refer to the main version of the documentation."
    },
    {
        "origin": "UPLOAD_DATASE is a documentation page.",
        "similar": "UPLOAD_DATASE is a page containing documentation."
    },
    {
        "origin": "No, UPLOAD_DATASE doesn't exist in v2.10.0.",
        "similar": "UPLOAD_DATASE is not available in version 2.10.0."
    },
    {
        "origin": "You can find UPLOAD_DATASE documentation on the main version. Click [here](/docs/datasets/main/en/upload_datase) to redirect to the main version of the documentation.",
        "similar": "The UPLOAD_DATASE documentation can be located on the main version. Click [here](/docs/datasets/main/en/upload_datase) to be taken to the main version of the documentation."
    },
    {
        "origin": "Yes, Datasets supports access to cloud storage providers through a `fsspec` FileSystem implementations.",
        "similar": "Datasets provides access to cloud storage services via `fsspec` FileSystem implementations."
    },
    {
        "origin": "Yes, you can save and load datasets from any cloud storage in a Pythonic way.",
        "similar": "It is possible to store and retrieve datasets from any cloud storage using Python."
    },
    {
        "origin": "Some examples of supported cloud storage providers are listed in the table provided in the documentation.",
        "similar": "Examples of cloud storage providers that are compatible with the documentation are shown in the table."
    },
    {
        "origin": "You can use the `load_dataset_builder` function with the `data_files` parameter and specify the path to your data files. Then, you can call the `download_and_prepare` method on the returned builder object, passing in the output directory and storage options.",
        "similar": "The `load_dataset_builder` function can be used with the `data_files` parameter to indicate the location of the data files. Subsequently, the `download_and_prepare` method can be called on the returned builder object, with the output directory and storage options being specified."
    },
    {
        "origin": "It is recommended to save datasets as compressed Parquet files to optimize I/O. You can specify this format by setting `file_format=\"parquet\"` when calling the `download_and_prepare` method.",
        "similar": "It is suggested to save datasets in compressed Parquet format to maximize I/O. You can select this format by setting `file_format=\"parquet\"` when using the `download_and_prepare` method."
    },
    {
        "origin": "You can specify the maximum shard size by setting the `max_shard_size` parameter when calling the `download_and_prepare` method. The default value is 500MB.",
        "similar": "By calling the `download_and_prepare` method, you can set the `max_shard_size` parameter to specify the maximum shard size, which is 500MB by default."
    },
    {
        "origin": "You can use the `dask.dataframe.read_parquet` function to load a dataset saved as sharded Parquet files in Dask. You can specify the path to the files and storage options as parameters.",
        "similar": "Dask's `dask.dataframe.read_parquet` function allows you to load a dataset saved as sharded Parquet files, providing the path to the files and storage options as parameters."
    },
    {
        "origin": "You can use the `save_to_disk` method on a `Dataset` object to save it to cloud storage. You need to specify the path to the output directory and storage options.",
        "similar": "The `Dataset` object can be saved to cloud storage by utilizing the `save_to_disk` method. It requires the output directory path and storage options to be specified."
    },
    {
        "origin": "You can use the `ls` method on a FileSystem instance to list files from a cloud storage. You need to specify the path to the directory as a parameter.",
        "similar": "The `ls` method of a FileSystem instance can be employed to list files from a cloud storage, with the path to the directory needing to be specified as a parameter."
    },
    {
        "origin": "You can use the `load_from_disk` function from the `datasets` module to load a serialized dataset from cloud storage. You need to specify the path to the directory and storage options as parameters.",
        "similar": "The `datasets` module provides the `load_from_disk` function, which can be used to retrieve a serialized dataset from cloud storage. All you need to do is to pass the directory path and storage options as parameters."
    },
    {
        "origin": "The purpose of this document is to provide documentation for the Datasets library.",
        "similar": "This document is intended to supply information about the Datasets library."
    },
    {
        "origin": "The different sections in this document include Get started, Tutorials, How-to guides, General usage, Audio, Vision, Text, Tabular, Dataset repository, Conceptual guides, and Reference.",
        "similar": "This document contains sections such as Introduction, Tutorials, Step-by-step instructions, General information, Audio, Visual, Textual, Tabular, Dataset collection, Conceptual instructions, and Documentation."
    },
    {
        "origin": "You can process audio data using this library by following the specific methods mentioned in the guide, such as resampling the sampling rate and using map() with audio datasets.",
        "similar": "By following the instructions in the guide, such as resampling the sampling rate and utilizing map() with audio datasets, you can manipulate audio data with this library."
    },
    {
        "origin": "It is a guide on how to process any type of dataset.",
        "similar": "This guide provides instructions on how to handle any kind of dataset."
    },
    {
        "origin": "The function is used to cast a column to another feature to be decoded.",
        "similar": "This function is employed to transform a column into another feature type for decoding."
    },
    {
        "origin": "When you use this function with the [Audio](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Audio) feature, you can resample the sampling rate.",
        "similar": "By utilizing the [Audio](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Audio) feature with this function, you can change the sampling rate."
    },
    {
        "origin": "Audio files are decoded and resampled on-the-fly, so the next time you access an example, the audio file is resampled to 16kHz.",
        "similar": "The audio files are decoded and re-rendered in real-time, thus the next time you access an example, it will be resampled to 16kHz."
    },
    {
        "origin": "The function helps preprocess your entire dataset at once.",
        "similar": "The function assists in the preprocessing of the whole dataset in one go."
    },
    {
        "origin": "You need to load a feature extractor and tokenizer and combine them in a `processor`.",
        "similar": "It is essential to obtain a feature extractor and tokenizer and join them in a `processor`."
    },
    {
        "origin": "You only need to load a `processor`.",
        "similar": "It is only necessary to incorporate a `processor`."
    },
    {
        "origin": "Include the `audio` column to ensure you\u2019re actually resampling the audio data.",
        "similar": "Ensure that the `audio` column is included in order to actually resample the audio data."
    },
    {
        "origin": "No, it doesn't exist in version 2.10.0.",
        "similar": "Version 2.10.0 does not include it."
    },
    {
        "origin": "You can find it on the main version of the documentation. Click on the provided link to redirect to the main version.",
        "similar": "The main version of the documentation can be accessed by clicking on the link provided."
    },
    {
        "origin": "The cache in Datasets is a storage system that stores previously downloaded and processed datasets, allowing for faster access to the data without the need to download or process it again.",
        "similar": "Datasets' cache is a storage system that keeps previously obtained and processed datasets, thus making it possible to access the data quickly without having to download or process it again."
    },
    {
        "origin": "The cache in Datasets improves efficiency by storing previously downloaded and processed datasets, allowing for faster access to the data without the need to download or process it again. This saves time and resources when working with large datasets.",
        "similar": "By keeping previously downloaded and processed datasets in the Datasets cache, it is possible to access the data quickly without having to download or process it again, thus saving time and resources when dealing with large datasets."
    },
    {
        "origin": "\ud83e\udd17 Datasets assigns a fingerprint to the cache file, which keeps track of the current state of a dataset. The initial fingerprint is computed using a hash from the Arrow table, or a hash of the Arrow files if the dataset is on disk. Subsequent fingerprints are computed by combining the fingerprint of the previous state, and a hash of the latest transform applied.",
        "similar": "A fingerprint is assigned to the cache file of the dataset by Datasets, which monitors the current state of the dataset. The initial fingerprint is calculated through a hash of the Arrow table or a hash of the Arrow files if the dataset is stored on disk. Subsequent fingerprints are generated by combining the fingerprint of the prior state and a hash of the most recent transformation applied."
    },
    {
        "origin": "Transforms are any of the processing methods from the How-to Process guides such as Dataset.map() or Dataset.shuffle().",
        "similar": "Any of the processing techniques from the How-to Process guides, such as Dataset.map() or Dataset.shuffle(), can be referred to as Transforms."
    },
    {
        "origin": "The fingerprint of a dataset is updated by hashing the function passed to map as well as the map parameters (batch_size, remove_columns, etc.). The hash is computed by dumping the object using a dill pickler and hashing the dumped bytes.",
        "similar": "The hash of a dataset is recalculated by hashing the map function and its parameters (batch_size, remove_columns, etc.) with the help of a dill pickler which dumps the object into bytes."
    },
    {
        "origin": "When a non-hashable transform is used, \ud83e\udd17 Datasets uses a random fingerprint instead and raises a warning. The non-hashable transform is considered different from the previous transforms, and \ud83e\udd17 Datasets will recompute all the transforms.",
        "similar": "If a non-hashable transform is applied, \ud83e\udd17 Datasets will substitute it with a random fingerprint and give a warning. This transform is distinct from the ones used before, and \ud83e\udd17 Datasets will recalculate all the transforms."
    },
    {
        "origin": "One can check the hash of any Python object using the fingerprint.Hasher.",
        "similar": "The fingerprint.Hasher can be used to generate the hash of any Python object."
    },
    {
        "origin": "Transforms should be serializable with pickle or dill to avoid recomputing all the transforms in \ud83e\udd17 Datasets.",
        "similar": "Serializing the transforms with pickle or dill can help to prevent the need for recalculating all the transforms in \ud83e\udd17 Datasets."
    },
    {
        "origin": "You can create an audio dataset by following the instructions provided in the \"Create an audio dataset\" section of the documentation.",
        "similar": "By adhering to the directions in the \"Create an audio dataset\" part of the documentation, you can assemble an audio dataset."
    },
    {
        "origin": "Yes, you can share your dataset with your team or anyone in the community by creating a dataset repository on the Hugging Face Hub.",
        "similar": "You can make a dataset repository on the Hugging Face Hub to share your dataset with your team or anyone in the community."
    },
    {
        "origin": "You can load a dataset using the `load_dataset` function provided by the `datasets` module.",
        "similar": "The `datasets` module offers a `load_dataset` function which can be utilized to import a dataset."
    },
    {
        "origin": "There are three methods for creating and sharing an audio dataset: \n   1. Create an audio dataset from local files in python with Dataset.push_to_hub(). \n   2. Create an audio dataset repository with the AudioFolder builder. \n   3. Create an audio dataset by writing a loading script.",
        "similar": "1. Utilizing Dataset.push_to_hub() in python, one can generate an audio dataset from local files. \n2. The AudioFolder builder can be used to construct an audio dataset repository. \n3. A loading script can be written to produce an audio dataset."
    },
    {
        "origin": "You can control access to your dataset by requiring users to share their contact information first. You can enable this feature on the Hub by following the Gated datasets guide.",
        "similar": "Requiring users to provide their contact information before they can access your dataset can be enabled on the Hub by following the Gated datasets guide."
    },
    {
        "origin": "You can load your own dataset using the paths to your audio files. Use the cast_column() function to take a column of audio file paths, and cast it to the Audio feature. Then upload the dataset to the Hugging Face Hub using Dataset.push_to_hub().",
        "similar": "You can upload your own dataset to the Hugging Face Hub using the Dataset.push_to_hub() function by taking a column of audio file paths and casting it to the Audio feature with the cast_column() method."
    },
    {
        "origin": "AudioFolder is a dataset builder designed to quickly load an audio dataset with several thousand audio files without requiring you to write any code. It automatically loads any additional information about your dataset, such as transcription, speaker accent, or speaker intent, as long as you include this information in a metadata file (metadata.csv/metadata.jsonl).",
        "similar": "AudioFolder is a dataset builder that eliminates the need for coding to quickly load a dataset with thousands of audio files. It will automatically incorporate any extra data such as transcription, accent, or intent, provided that it is included in a metadata file (metadata.csv/metadata.jsonl)."
    },
    {
        "origin": "It can be helpful to store your metadata as a jsonl file if the data columns contain a more complex format (like a list of floats) to avoid parsing errors or reading complex values as strings. The metadata file should include a file_name column to link an audio file to its metadata.",
        "similar": "Storing your metadata as a jsonl file may be beneficial if the data columns have a more intricate format (e.g. a list of floats) in order to prevent any parsing mistakes or misinterpreting complex values as strings. The metadata file should include a file_name column to associate an audio file with its metadata."
    },
    {
        "origin": "`audiofolder` is a loading method that can be used to load audio datasets involving multiple splits.",
        "similar": "`audiofolder` is a technique for loading audio datasets that involve multiple splits."
    },
    {
        "origin": "You can load a dataset using `audiofolder` by specifying the data directory in `data_dir` parameter while calling `load_dataset()`.",
        "similar": "By providing the data directory in `data_dir` parameter when calling `load_dataset()`, you can load a dataset using `audiofolder`."
    },
    {
        "origin": "The dataset directory for audio datasets involving multiple splits should have the following structure:\n```\ndata/train/first_train_audio_file.mp3\ndata/train/second_train_audio_file.mp3\ndata/test/first_test_audio_file.mp3\ndata/test/second_test_audio_file.mp3\n```",
        "similar": "The directory structure for audio datasets with multiple splits should be as follows:\ndata/train/first_train_audio_file.mp3\ndata/train/second_train_audio_file.mp3\ndata/test/first_test_audio_file.mp3\ndata/test/second_test_audio_file.mp3"
    },
    {
        "origin": "If audio files are not located right next to a metadata file, the `file_name` column should be a full relative path to an audio file, not just its filename.",
        "similar": "If the audio files are not situated in the same directory as the metadata file, the `file_name` column should contain the full relative path to the audio file, not just its name."
    },
    {
        "origin": "`AudioFolder` automatically infers the class labels of the dataset based on the directory name.",
        "similar": "`AudioFolder` can deduce the class labels of the dataset from the directory name automatically."
    },
    {
        "origin": "You can load a dataset using `AudioFolder` by specifying the data directory in `data_dir` parameter while calling `load_dataset()`.",
        "similar": "By providing the data directory in `data_dir` parameter when calling `load_dataset()`, you can load a dataset using `AudioFolder`."
    },
    {
        "origin": "If all audio files are contained in a single directory or if they are not on the same level of directory structure, the `label` column won\u2019t be added automatically. If you need it, set `drop_labels=False` explicitly.",
        "similar": "If the audio files are not all located in the same directory or are not at the same level of the directory structure, the `label` column will not be added automatically. To include it, you must explicitly set `drop_labels=False`."
    },
    {
        "origin": "Yes, `audiofolder` can be used to load all splits of audio datasets found in Kaggle competitions if the metadata features are the same for each split.",
        "similar": "It is possible to utilize `audiofolder` to load all the divisions of audio datasets from Kaggle competitions if the metadata features remain consistent for each split."
    },
    {
        "origin": "The directory structure for creating a dataset loading script should have a `my_dataset.py` file, a `data` folder (optional), and a `README.md` file.",
        "similar": "The directory for creating a dataset loading script should feature a `my_dataset.py` file, an optional `data` folder, and a `README.md` file."
    },
    {
        "origin": "Users without a lot of disk space can use the dataset without downloading it, and users can preview a dataset in the dataset viewer.",
        "similar": "Those with limited storage capacity can access the dataset without downloading it, and they can view a preview of the dataset in the dataset viewer."
    },
    {
        "origin": "In addition to learning how to create a streamable dataset, you\u2019ll also learn how to create a dataset builder class, create dataset configurations, add dataset metadata, download and define the dataset splits, generate the dataset, and upload the dataset to the Hub.",
        "similar": "Apart from understanding how to form a streamable dataset, you will be taught to construct a dataset builder class, arrange dataset configurations, attach dataset metadata, download and determine the dataset divisions, fabricate the dataset, and upload the dataset to the Hub."
    },
    {
        "origin": "The base class for datasets generated from a dictionary generator is GeneratorBasedBuilder.",
        "similar": "The GeneratorBasedBuilder serves as the foundation for datasets created by a dictionary generator."
    },
    {
        "origin": "The three methods to help create a dataset within the GeneratorBasedBuilder class are _info, _split_generators, and _generate_examples.",
        "similar": "Three methods to build a dataset using the GeneratorBasedBuilder class are _info, _split_generators, and _generate_examples."
    },
    {
        "origin": "To create different configurations for a dataset, use the BuilderConfig class to create a subclass of your dataset.",
        "similar": "Subclass your dataset by using the BuilderConfig class to generate various configurations."
    },
    {
        "origin": "The dataset comprises a certain number of hours of transcribed speech data.",
        "similar": "The dataset consists of a certain number of hours of transcribed speech recordings."
    },
    {
        "origin": "Users can specify a configuration to load in `load_dataset()` by setting the configuration name.",
        "similar": "`load_dataset()` allows users to select a configuration by specifying its name."
    },
    {
        "origin": "Information that can be included in the DatasetInfo class includes a description of the dataset, features specifying the dataset column types, a link to the dataset homepage, the license type, and a BibTeX citation of the dataset.",
        "similar": "The DatasetInfo class can comprise a description of the dataset, features indicating the dataset column types, a link to the dataset homepage, the license type, and a BibTeX citation of the dataset."
    },
    {
        "origin": "The next step is to download the dataset and define the splits.",
        "similar": "The next move is to acquire the dataset and delineate the divisions."
    },
    {
        "origin": "Use the download() method.",
        "similar": "Employ the download() technique."
    },
    {
        "origin": "The download() method returns the path to the local file/archive.",
        "similar": "The download() method yields the location of the local file/archive."
    },
    {
        "origin": "The download() method accepts a relative path to a file inside a Hub dataset repository, a URL to a file hosted somewhere else, or a (nested) list or dictionary of file names or URLs.",
        "similar": "The download() method can take a path to a file within a Hub dataset repository, a URL to a file located elsewhere, or a (nested) list or dictionary of filenames or URLs as argument."
    },
    {
        "origin": "Use the SplitGenerator to organize the audio files and sentence prompts in each split, and name each split with a standard name like: Split.TRAIN, Split.TEST, and SPLIT.Validation.",
        "similar": "Organize the audio files and sentence prompts in each split with the SplitGenerator, and label each split with a standard title such as Split.TRAIN, Split.TEST, and SPLIT.Validation."
    },
    {
        "origin": "In the gen_kwargs parameter, specify the file path to the prompts_path and path_to_clips. For audio_files, use iter_archive() to iterate over the audio files in the TAR archive.",
        "similar": "In the gen_kwargs parameter, provide the file path for prompts_path and path_to_clips. To iterate over the audio files in the TAR archive, employ iter_archive() for audio_files."
    },
    {
        "origin": "The generate_examples method actually generates the samples in the dataset.",
        "similar": "The method of generate_examples actually produces the samples in the dataset."
    },
    {
        "origin": "The generate_examples method accepts the prompts_path, path_to_clips, and audio_files from the previous method as arguments.",
        "similar": "The generate_examples method takes in the prompts_path, path_to_clips, and audio_files from the preceding method as parameters."
    },
    {
        "origin": "Files inside TAR archives are accessed and yielded sequentially using iter_archive().",
        "similar": "Iter_archive() is employed to sequentially access and yield files inside TAR archives."
    },
    {
        "origin": "The purpose of the `_generate_examples` method is to yield examples as (key, example) tuples.",
        "similar": "The `_generate_examples` method yields (key, example) tuples with the intent of providing examples."
    },
    {
        "origin": "The `load_dataset` function loads a dataset from the Hub.",
        "similar": "The `load_dataset` function retrieves a dataset from the Hub."
    },
    {
        "origin": "TAR archives can be extracted locally using the `extract()` method, but only in non-streaming mode. The `iter_archive()` method can be used to iterate over the files within the archive.",
        "similar": "The `extract()` method can be used to locally unpack TAR archives, however, it only works in non-streaming mode. Alternatively, `iter_archive()` can be used to iterate through the files within the archive."
    },
    {
        "origin": "The `download_and_extract()` method is used to download a metadata file specified in `_METADATA_URL` and extract it in non-streaming mode.",
        "similar": "The `download_and_extract()` method is employed to acquire the metadata file indicated in `_METADATA_URL` and unpack it without streaming."
    },
    {
        "origin": "The `SplitGenerator` is used to organize the audio files and metadata in each split and name each split with a standard name like: `Split.TRAIN`, `Split.TEST`, and `SPLIT.Validation`.",
        "similar": "The `SplitGenerator` is employed to arrange the audio files and metadata of each split and label each split with a standard nomenclature such as: `Split.TRAIN`, `Split.TEST`, and `SPLIT.Validation`."
    },
    {
        "origin": "The `iter_archive()` method is used to iterate over the audio files in the TAR archives and enables streaming for the dataset.",
        "similar": "The `iter_archive()` method allows for iteration over the audio files in the TAR archives and provides streaming capabilities for the dataset."
    },
    {
        "origin": "The `_generate_examples` method accepts `local_extracted_archive`, `audio_files`, `metadata_path`, and `path_to_clips` as arguments and yields the metadata associated with the audio files in the TAR file.",
        "similar": "The `_generate_examples` method takes `local_extracted_archive`, `audio_files`, `metadata_path`, and `path_to_clips` as inputs and produces the metadata related to the audio files in the TAR file."
    },
    {
        "origin": "The yielded tuple in iter_archive() method is of the format (`path`, `f`), where `path` is a relative path to a file inside the archive, and `f` is the file object itself.",
        "similar": "A tuple is yielded by the iter_archive() method which has the form (`path`, `f`), with `path` being a relative path to a file within the archive, and `f` representing the file object."
    },
    {
        "origin": "To get the full path to the locally extracted file, we need to join the path of the directory where the archive is extracted to and the relative audio file path.",
        "similar": "In order to obtain the complete route to the locally extracted file, we must combine the directory path where the archive is decompressed and the relative path of the audio file."
    },
    {
        "origin": "A tabular dataset is a generic dataset used to describe any data stored in rows and columns, where the rows represent an example and the columns represent a feature (can be continuous or categorical).",
        "similar": "A dataset in tabular form is a generic representation of any data that is structured into rows and columns, with each row representing an instance and each column representing a feature (which can be either continuous or categorical)."
    },
    {
        "origin": "To load tabular data, you can refer to the Load tabular data section in the Datasets documentation.",
        "similar": "For guidance on how to load tabular data, please refer to the Load tabular data section in the Datasets documentation."
    },
    {
        "origin": "Tabular datasets can be loaded and created from CSV files.",
        "similar": "CSV files can be used to generate and load tabular datasets."
    },
    {
        "origin": "Multiple CSV files can be loaded using Datasets by passing them as a list to the `data_files` parameter.",
        "similar": "Datasets can load multiple CSV files by providing them as a list to the `data_files` parameter."
    },
    {
        "origin": "Yes, specific CSV files can be mapped to train and test splits.",
        "similar": "It is possible to assign particular CSV files to the training and testing divisions."
    },
    {
        "origin": "Remote CSV files can be loaded using Datasets by passing the URLs instead.",
        "similar": "Datasets can be used to load remote CSV files by providing the URLs."
    },
    {
        "origin": "Yes, zipped CSV files can be loaded using Datasets.",
        "similar": "Datasets can be used to load zipped CSV files."
    },
    {
        "origin": "The `from_pandas()` method can be used to load datasets from Pandas DataFrames.",
        "similar": "The `from_pandas()` method facilitates loading datasets from Pandas DataFrames."
    },
    {
        "origin": "The `splits` parameter can be used to specify the name of the dataset split when loading datasets from Pandas DataFrames.",
        "similar": "The `splits` argument can be employed to indicate the name of the dataset division when obtaining datasets from Pandas DataFrames."
    },
    {
        "origin": "If the dataset doesn't look as expected when loading from Pandas DataFrames, the dataset features should be explicitly specified.",
        "similar": "In case the dataset does not appear as anticipated when being imported from Pandas DataFrames, its attributes should be explicitly defined."
    },
    {
        "origin": "Datasets are typically stored in databases accessed with SQL queries.",
        "similar": "Databases are usually the place where datasets are kept and can be accessed using SQL queries."
    },
    {
        "origin": "A dataset can be created from a SQLite database by connecting to the database, querying for the data needed, and creating a dataset out of it.",
        "similar": "A dataset can be constructed by establishing a connection to a SQLite database, extracting the desired data, and forming it into a dataset."
    },
    {
        "origin": "You can load a table from a SQL database using \ud83e\udd17 Datasets by passing the table name and URI to the `from_sql()` method.",
        "similar": "\ud83e\udd17 Datasets allows you to load a table from a SQL database by providing the table name and URI to the `from_sql()` method."
    },
    {
        "origin": "Yes, you can use \ud83e\udd17 Datasets to filter data from a loaded SQL table using the `filter()` method.",
        "similar": "You can use \ud83e\udd17 Datasets to sift through data from a SQL table that has been loaded by employing the `filter()` method."
    },
    {
        "origin": "Yes, it is possible to load a dataset from a SQL query instead of an entire table using \ud83e\udd17 Datasets by passing your query and URI to the `from_sql()` method.",
        "similar": "It is feasible to load data from a SQL query rather than a complete table into \ud83e\udd17 Datasets by providing the `from_sql()` method with your query and URI."
    },
    {
        "origin": "Yes, you can connect and load a dataset from a PostgreSQL database using \ud83e\udd17 Datasets by setting up your PostgreSQL database and using the `from_sql()` method to load a dataset from a table or query.",
        "similar": "You can link and import a dataset from a PostgreSQL database with \ud83e\udd17 Datasets by configuring your PostgreSQL database and using the `from_sql()` method to bring in a dataset from a table or query."
    },
    {
        "origin": "The quickstart is intended for developers who are ready to dive into the code and see an example of how to integrate \ud83e\udd17 Datasets into their model training workflow.",
        "similar": "Developers who are eager to explore the code and gain insight into incorporating \ud83e\udd17 Datasets into their model training process can take advantage of the quickstart."
    },
    {
        "origin": "For beginners, it is recommended to start with the tutorials where they can get a more thorough introduction.",
        "similar": "It is suggested that those just starting out begin with the tutorials to gain a more comprehensive understanding."
    },
    {
        "origin": "After signing up, users can collaborate on models, datasets, and Spaces, get faster examples with accelerated inference, and switch between documentation themes.",
        "similar": "Once registered, users can work together on models, datasets and Spaces, get quicker results with accelerated inference, and alternate between different documentation themes."
    },
    {
        "origin": "The sampling rate used in the audio preprocessing function is 16000.",
        "similar": "The audio preprocessing function utilizes a sampling rate of 16000."
    },
    {
        "origin": "The preprocess_function() function is used to preprocess the audio array.",
        "similar": "The audio array is preprocessed by the preprocess_function() function."
    },
    {
        "origin": "You can use the rename_column() function to rename the intent_class column to labels.",
        "similar": "The rename_column() function can be employed to alter the intent_class column to labels."
    },
    {
        "origin": "You can use the set_format() function to set the dataset format to torch and specify the columns you want to format.",
        "similar": "The set_format() function can be employed to configure the dataset to torch format and indicate the columns desired to be formatted."
    },
    {
        "origin": "A feature extractor is needed to preprocess image datasets in computer vision.",
        "similar": "A tool for extracting features is necessary for preprocessing image datasets in computer vision."
    },
    {
        "origin": "Albumentations, imgaug, Kornia, and torchvision can be used for data augmentation.",
        "similar": "Data augmentation can be achieved by using Albumentations, imgaug, Kornia, and torchvision."
    },
    {
        "origin": "You can use the with_transform() function to apply data augmentations on-the-fly.",
        "similar": "The with_transform() function can be utilized to implement data augmentations in real-time."
    },
    {
        "origin": "You can wrap the dataset in DataLoader using torch.utils.data.DataLoader and create a collate function to collate the samples into batches.",
        "similar": "Using torch.utils.data.DataLoader, you can package the dataset and craft a collate function to group the samples into batches."
    },
    {
        "origin": "The Microsoft Research Paraphrase Corpus (MRPC) dataset is used for training a model to determine whether a pair of sentences mean the same thing.",
        "similar": "A model is trained using the Microsoft Research Paraphrase Corpus (MRPC) dataset to decide if two sentences have the same meaning."
    },
    {
        "origin": "The BERT model and its corresponding tokenizer from the \ud83e\udd17 Transformers library are used for training on the MRPC dataset.",
        "similar": "The MRPC dataset is trained with the BERT model and its tokenizer from the \ud83e\udd17 Transformers library."
    },
    {
        "origin": "The tokenizer generates three new columns in the dataset: input_ids, token_type_ids, and an attention_mask.",
        "similar": "The tokenizer creates three additional columns in the dataset: input_ids, token_type_ids, and attention_mask."
    },
    {
        "origin": "The encode function is used to tokenize batches of examples in a dataset to speed up processing.",
        "similar": "The encode function tokenizes batches of examples in a dataset to expedite processing."
    },
    {
        "origin": "The expected input name in BertForSequenceClassification is \"labels\".",
        "similar": "The anticipated label name for BertForSequenceClassification is \"labels\"."
    },
    {
        "origin": "For PyTorch, use the set_format() function to set the dataset format to \"torch\" and specify the columns to format. For TensorFlow, use the to_tf_dataset() function and a data collator to set the dataset format to be compatible with TensorFlow.",
        "similar": "To use PyTorch, the set_format() function should be employed to change the dataset format to \"torch\" and the columns to be formatted should be specified. For TensorFlow, the to_tf_dataset() function and a data collator should be used to make the dataset format compatible with TensorFlow."
    },
    {
        "origin": "You can find an end-to-end example of how to train a model on a text dataset in the \ud83e\udd17 Transformers text classification guide.",
        "similar": "A comprehensive illustration of how to train a model on a text dataset can be found in the \ud83e\udd17 Transformers text classification guide."
    },
    {
        "origin": "To learn more about \ud83e\udd17 Datasets core concepts, you can read our Conceptual Guides.",
        "similar": "You can gain a better understanding of \ud83e\udd17 Datasets core concepts by reading our Conceptual Guides."
    },
    {
        "origin": "You can find the Conceptual Guides for \ud83e\udd17 Datasets by clicking on the link provided in the document.",
        "similar": "By accessing the link in the document, you can locate the Conceptual Guides for \ud83e\udd17 Datasets."
    },
    {
        "origin": "The document does not provide information on the Quickstart for \ud83e\udd17 Datasets.",
        "similar": "No information regarding the Quickstart for \ud83e\udd17 Datasets is contained in the document."
    },
    {
        "origin": "Yes, according to the document, \ud83e\udd17 Datasets supports Audio, Vision, and NLP.",
        "similar": "As per the document, Audio, Vision, and NLP are all supported by Datasets."
    },
    {
        "origin": "The document does not provide information on what to do after reading the Conceptual Guides for \ud83e\udd17 Datasets.",
        "similar": "No instruction is given in the document on what action to take after going through the Conceptual Guides for \ud83e\udd17 Datasets."
    },
    {
        "origin": "You can find the installation guide for \ud83e\udd17 Datasets by clicking on the link provided in the document.",
        "similar": "By accessing the link given in the document, you can get the installation guide for \ud83e\udd17 Datasets."
    },
    {
        "origin": "The purpose of the Datasets documentation is to provide information on how to use and work with datasets.",
        "similar": "The goal of the Datasets documentation is to furnish guidance on how to employ and manipulate datasets."
    },
    {
        "origin": "Some of the topics covered in the Datasets documentation include getting started, tutorials, how-to guides, audio, vision, text, tabular data, and the dataset repository.",
        "similar": "The Datasets documentation covers a range of topics, such as introductory information, tutorials, how-to guides, audio, vision, text, tabular data, and the dataset repository."
    },
    {
        "origin": "The two types of dataset objects mentioned in the documentation are regular Dataset and IterableDataset.",
        "similar": "The documentation mentions two varieties of dataset objects, namely regular Dataset and IterableDataset."
    },
    {
        "origin": "A Dataset in Hugging Face provides fast random access to the rows and memory-mapping so that loading even large datasets only uses a relatively small amount of device memory.",
        "similar": "A Dataset from Hugging Face offers quick random access to the rows and memory-mapping, thus allowing even large datasets to be loaded with only a limited amount of device memory."
    },
    {
        "origin": "An IterableDataset in Hugging Face allows you to access and use the dataset without waiting for it to download completely, even for really, really big datasets that won\u2019t even fit on disk or in memory.",
        "similar": "Hugging Face's IterableDataset enables you to access and utilize the dataset without the need to wait for it to finish downloading, even for extremely large datasets that are too large to fit in memory or on disk."
    },
    {
        "origin": "You can load a Dataset in Hugging Face using the `load_dataset()` function and specifying the name of the dataset and the split you want to load.",
        "similar": "The `load_dataset()` function of Hugging Face can be used to load a Dataset by providing the name of the dataset and the split required."
    },
    {
        "origin": "You can manipulate and interact with the data stored inside a Dataset object in Hugging Face. It contains columns of data, and each column can be a different type of data. You can access examples from the dataset using indexing or slicing.",
        "similar": "You can interact with and manipulate the data stored in a Hugging Face Dataset object, which consists of columns of various data types. You can also retrieve examples from the dataset through indexing or slicing."
    },
    {
        "origin": "An IterableDataset is a type of dataset that progressively iterates over a dataset one example at a time, so you don\u2019t have to wait for the whole dataset to download before you can use it.",
        "similar": "A IterableDataset is a type of dataset that iterates over the data one example at a time, allowing you to start using the dataset without having to wait for the entire download."
    },
    {
        "origin": "The behavior of an IterableDataset is different from a regular Dataset. You don\u2019t get random access to examples in an IterableDataset. Instead, you should iterate over its elements, for example, by calling next(iter()) or with a for loop to return the next item from the IterableDataset.",
        "similar": "The way an IterableDataset behaves is unlike that of a regular Dataset; you cannot access its examples randomly. To get the next item from the IterableDataset, you must iterate over its elements, for instance, by calling next(iter()) or using a for loop."
    },
    {
        "origin": "You can return a subset of the dataset with a specific number of examples in it with IterableDataset.take().",
        "similar": "IterableDataset.take() allows you to retrieve a subset of the dataset with a specified number of examples."
    },
    {
        "origin": "This document is a guide to learn how to preprocess a Dataset or an IterableDataset.",
        "similar": "This document serves as a tutorial on preprocessing a Dataset or IterableDataset."
    },
    {
        "origin": "A Dataset is a class that is used to load and preprocess data.",
        "similar": "A Dataset class is employed to load and prepare data."
    },
    {
        "origin": "An IterableDataset is a subclass of Dataset that allows for iterating over the data.",
        "similar": "A Dataset that is Iterable is a type of Dataset that enables cycling through the data."
    },
    {
        "origin": "You can learn how to preprocess a Dataset by following the Process guide.",
        "similar": "By adhering to the Process guide, one can gain knowledge on how to preprocess a Dataset."
    },
    {
        "origin": "You can learn how to preprocess an IterableDataset by following the Stream guide.",
        "similar": "By following the Stream guide, one can acquire knowledge on how to preprocess an IterableDataset."
    },
    {
        "origin": "You can load a dataset from the Hub by following the Load a dataset from the Hub link.",
        "similar": "You can access a dataset from the Hub by clicking on the \"Load a dataset from the Hub\" link."
    },
    {
        "origin": "The next steps are not specified in the given document.",
        "similar": "The document does not provide any further instructions."
    },
    {
        "origin": "The purpose of the Datasets documentation is to provide information and guidance on how to use the Datasets library.",
        "similar": "The objective of the Datasets documentation is to furnish instructions and advice on the utilization of the Datasets library."
    },
    {
        "origin": "The different sections in the Datasets documentation include Get started, Tutorials, How-to guides, Audio, Vision, Text, Tabular, Dataset repository, Conceptual guides, and Reference.",
        "similar": "The Datasets documentation is divided into sections such as Introduction, Tutorials, How-to guides, Audio, Vision, Text, Tabular, Dataset repository, Conceptual guides, and Reference."
    },
    {
        "origin": "The Dataset loading script is an optional script that can be used to load datasets that are not in CSV, JSON, JSON lines, text, or Parquet formats.",
        "similar": "The Dataset loading script is an alternative script that can be employed to load datasets that are not in the formats of CSV, JSON, JSON lines, text, or Parquet."
    },
    {
        "origin": "load_dataset() is a function that can be used to automatically load datasets in CSV, JSON, JSON lines, text, or Parquet formats.",
        "similar": "The function load_dataset() can be employed to automatically import datasets in CSV, JSON, JSON lines, text, or Parquet formats."
    },
    {
        "origin": "A dataset loading script is a Python file that defines the different configurations and splits of a dataset, as well as how to download and process the data.",
        "similar": "A Python script for loading datasets defines the different configurations, divisions, downloading and processing of the data."
    },
    {
        "origin": "A dataset loading script should include information or attributes about the dataset such as a description, features, homepage, and citation. It should also define how to download and process the data.",
        "similar": "A dataset loading script should incorporate details about the dataset, including a description, features, homepage, and citation. It should also specify the procedure for downloading and manipulating the data."
    },
    {
        "origin": "A dataset loading script should have the same name as a dataset repository or directory and can be loaded using the `load_dataset` function from the `datasets` library.",
        "similar": "The `load_dataset` function from the `datasets` library can be used to load a dataset loading script which should have the same name as the dataset repository or directory."
    },
    {
        "origin": "A BuilderConfig in Datasets allows you to create different configurations for the user to select from, such as in the case of the SuperGLUE dataset.",
        "similar": "Datasets provide a BuilderConfig that enables the user to generate various configurations to choose from, such as with the SuperGLUE dataset."
    },
    {
        "origin": "The SuperGLUE loading script is a script that allows you to define several configurations for your dataset.",
        "similar": "A script called SuperGLUE is available for setting up multiple configurations for your dataset."
    },
    {
        "origin": "A BuilderConfig subclass is a class that contains attributes about your dataset, such as the features of your dataset, label classes, and a URL to the data files.",
        "similar": "A subclass of BuilderConfig holds information about your dataset, including the features, label classes, and the URL of the data files."
    },
    {
        "origin": "The arguments of the SuperGlueConfig class are features, data_url, citation, url, label_classes, and **kwargs.",
        "similar": "The SuperGlueConfig class has arguments such as features, data_url, citation, url, label_classes, and additional parameters in **kwargs."
    },
    {
        "origin": "The SuperGlue class is a class that represents the SuperGLUE benchmark.",
        "similar": "The SuperGLUE benchmark is embodied in the SuperGlue class."
    },
    {
        "origin": "Users can load a specific configuration of the dataset by specifying the configuration name when loading the dataset using the load_dataset function.",
        "similar": "The load_dataset function allows users to specify a configuration name in order to load a particular configuration of the dataset."
    },
    {
        "origin": "The DEFAULT_CONFIG_NAME attribute is used to set a default dataset configuration to avoid a ValueError prompt when loading a dataset with multiple configurations.",
        "similar": "The DEFAULT_CONFIG_NAME attribute is employed to establish a default dataset configuration in order to prevent a ValueError alert when loading a dataset with multiple configurations."
    },
    {
        "origin": "DEFAULT_CONFIG_NAME is a variable that holds the name of the default configuration.",
        "similar": "The variable DEFAULT_CONFIG_NAME stores the name of the default configuration."
    },
    {
        "origin": "A default configuration should only be used when it makes sense and not just for the convenience of the user.",
        "similar": "The user should not resort to a default configuration merely for convenience, but only when it is appropriate."
    },
    {
        "origin": "After defining the attributes of a dataset, the next step is to download the data files and organize them according to their splits.",
        "similar": "Once the characteristics of the dataset have been identified, the subsequent step is to acquire the data files and arrange them in accordance with their divisions."
    },
    {
        "origin": "Data files can be downloaded by creating a dictionary of URLs in the loading script that point to the original data files and using DownloadManager.download_and_extract() to download the files.",
        "similar": "A dictionary of URLs pointing to the original data files can be created in the loading script, and DownloadManager.download_and_extract() can be used to download the data files."
    },
    {
        "origin": "SplitGenerator is a simple class that contains the name of each split and gen_kwargs that provides the file paths to the data files to load for each split.",
        "similar": "SplitGenerator is a straightforward class that holds the name of each split and offers gen_kwargs which furnishes the paths of the data files to be loaded for each split."
    },
    {
        "origin": "The purpose of DatasetBuilder._generate_examples is to read and parse the data files and yield a tuple of an id and an example from the dataset.",
        "similar": "The aim of DatasetBuilder._generate_examples is to scan and interpret the data files, and then produce a pair of an id and an example from the dataset."
    },
    {
        "origin": "Dataset metadata is information about the dataset that is stored in the dataset card `README.md` in YAML. It includes information like the number of examples required to confirm the dataset was correctly generated, and information about the dataset like its `features`.",
        "similar": "The `README.md` in YAML of the dataset card contains the dataset metadata, which provides details such as the number of samples needed to ensure the dataset was generated correctly, and the `features` of the dataset."
    },
    {
        "origin": "You can generate dataset metadata by running the following command: `datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs`.",
        "similar": "You can create dataset metadata by executing the command `datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs`."
    },
    {
        "origin": "You can upload your dataset to the Hub by creating a dataset card and uploading it to the Hub.",
        "similar": "You can upload your dataset to the Hub by creating a dataset card and then submitting it."
    },
    {
        "origin": "You can load your dataset from the Hub using the following code: `from datasets import load_dataset; load_dataset(\"<username>/my_dataset\")`.",
        "similar": "The following code can be used to obtain your dataset from the Hub: `from datasets import load_dataset; load_dataset(\"<username>/my_dataset\")`."
    },
    {
        "origin": "Sharding is a feature in datasets that allows for running the dataset generation script in parallel to make it faster. It can help if you have hundreds or thousands of TAR archives, or JSONL files.",
        "similar": "Sharding can be used to speed up the dataset generation process by running the script in parallel. This is especially useful if you have a large number of TAR archives or JSONL files."
    },
    {
        "origin": "To use sharding in your dataset, you can define lists of files in `gen_kwargs` to be shards. Therefore, datasets can automatically spawn several workers to run `_generate_examples` in parallel, and each worker is given a subset of shards to process. Users can also specify `num_proc=` in `load_dataset()` to specify the number of processes to use as workers.",
        "similar": "By using sharding in your dataset, you can specify lists of files in `gen_kwargs` to be shards. This will enable datasets to spawn multiple workers to execute `_generate_examples` in parallel, with each worker being allocated a subset of shards to process. Additionally, users can also specify the number of processes to be used as workers by setting `num_proc=` in `load_dataset()`."
    },
    {
        "origin": "ArrowBasedBuilder is a dataset builder class in datasets that allows for yielding batches of data rather than examples one by one. It can speed up the dataset generation by yielding Arrow tables directly, instead of examples.",
        "similar": "The ArrowBasedBuilder class in datasets enables the generation of batches of data instead of individual examples, thus accelerating the dataset creation by outputting Arrow tables instead of examples."
    },
    {
        "origin": "The search index in Datasets enables searching for examples in a dataset, which can be useful when you want to retrieve specific examples from a dataset that are relevant to your NLP task.",
        "similar": "The indexing feature in Datasets facilitates searching for samples within a dataset, which can be beneficial when attempting to obtain relevant examples from a dataset for your NLP project."
    },
    {
        "origin": "The tools used for the search index in Datasets are FAISS and ElasticSearch.",
        "similar": "FAISS and ElasticSearch are the tools employed for indexing in Datasets."
    },
    {
        "origin": "The search index can be helpful for NLP tasks by allowing users to retrieve specific examples from a dataset that are relevant to their task.",
        "similar": "The search index can be advantageous for NLP projects by permitting users to access particular samples from a dataset that are pertinent to their project."
    },
    {
        "origin": "FAISS is a system that retrieves documents based on the similarity of their vector representations.",
        "similar": "FAISS is a system that finds documents by comparing their vector representations."
    },
    {
        "origin": "You can generate vector representations with the DPR model by downloading it from \ud83e\udd17 Transformers and using it to compute the vector representations of your dataset.",
        "similar": "You can obtain vector representations through the DPR model by getting it from \ud83e\udd17 Transformers and then utilizing it to calculate the vector representations of your data."
    },
    {
        "origin": "You can create an index for your dataset by using [Dataset.add_faiss_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.add_faiss_index) or [Dataset.add_elasticsearch_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.add_elasticsearch_index) depending on the system you want to use.",
        "similar": "You can decide which system you want to use to create an index for your dataset by using either [Dataset.add_faiss_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.add_faiss_index) or [Dataset.add_elasticsearch_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.add_elasticsearch_index)."
    },
    {
        "origin": "You can query your dataset with the embeddings index by loading the DPR Question Encoder, searching for a question with [Dataset.get_nearest_examples()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples), and accessing the retrieved examples.",
        "similar": "You can use the DPR Question Encoder to query your dataset with the embeddings index, [Dataset.get_nearest_examples()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.get_nearest_examples) to search for a question, and then access the examples that are retrieved."
    },
    {
        "origin": "You can save the index on disk with [Dataset.save_faiss_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.save_faiss_index) or [Dataset.save_elasticsearch_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.save_elasticsearch_index), and reload it at a later time with [Dataset.load_faiss_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.load_faiss_index) or [Dataset.load_elasticsearch_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.load_elasticsearch_index).",
        "similar": "You can store the index on disk with [Dataset.save_faiss_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.save_faiss_index) or [Dataset.save_elasticsearch_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.save_elasticsearch_index) and retrieve it later with [Dataset.load_faiss_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.load_faiss_index) or [Dataset.load_elasticsearch_index()](/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.load_elasticsearch_index)."
    },
    {
        "origin": "You can query the `context` index with `Dataset.get_nearest_examples()`.",
        "similar": "You can use `Dataset.get_nearest_examples()` to query the `context` index."
    },
    {
        "origin": "To reuse the index, define the `es_index_name` parameter when you build the index.",
        "similar": "When constructing the index, specify the `es_index_name` parameter to utilize the index again."
    },
    {
        "origin": "You can reload it later with the index name when you call `Dataset.load_elasticsearch_index()`.",
        "similar": "You can call `Dataset.load_elasticsearch_index()` later with the index name to reload it."
    },
    {
        "origin": "For more advanced ElasticSearch usage, you can specify your own configuration with custom settings.",
        "similar": "For more sophisticated ElasticSearch utilization, you can configure your own setup with personalized parameters."
    },
    {
        "origin": "Datasets is a tool that offers various features to manipulate and prepare datasets for training.",
        "similar": "Datasets provide a range of functions that enable users to manipulate and prepare data for training."
    },
    {
        "origin": "You can get started with Datasets by going through the Quickstart and Installation guides provided in the documentation.",
        "similar": "You can begin working with Datasets by consulting the Quickstart and Installation guides in the documentation."
    },
    {
        "origin": "The tutorials available for Datasets include Load a dataset from the Hub, Know your dataset, Preprocess, Evaluate predictions, Create a dataset, and Share a dataset to the Hub.",
        "similar": "The tutorials available for Datasets encompass Loading a dataset from the Hub, Becoming familiar with your dataset, Preprocessing, Assessing predictions, Constructing a dataset, and Distributing a dataset to the Hub."
    },
    {
        "origin": "Datasets can process audio, vision, text, and tabular data.",
        "similar": "Datasets are able to handle audio, visual, textual, and tabular information."
    },
    {
        "origin": "The how-to guides available for Datasets include Load, Process, Stream, Use with TensorFlow, Use with PyTorch, Cache management, Cloud storage, Search index, Metrics, and Beam Datasets.",
        "similar": "The how-to guides for Datasets comprise of steps such as Loading, Processing, Streaming, Utilizing TensorFlow, Utilizing PyTorch, Caching, Cloud storage, Indexing, Measuring, and Beam Datasets."
    },
    {
        "origin": "The Dataset repository in Datasets allows you to share, create a dataset loading script, create a dataset card, and structure your repository.",
        "similar": "In Datasets, the Dataset Repository provides you with the ability to share, generate a script for loading a dataset, make a dataset card, and organize your repository."
    },
    {
        "origin": "You can collaborate on models, datasets, and Spaces using Datasets by joining the Hugging Face community and getting access to the augmented documentation experience.",
        "similar": "By joining the Hugging Face community and gaining access to the enhanced documentation experience, you can work together on models, datasets, and Spaces through Datasets."
    },
    {
        "origin": "The guides assume that you are familiar and comfortable with the \ud83e\udd17 Datasets basics.",
        "similar": "It is assumed that you have a good understanding and are comfortable with the fundamentals of \ud83e\udd17 Datasets."
    },
    {
        "origin": "We recommend newer users check out our tutorials first.",
        "similar": "It is suggested that new users should begin by viewing our tutorials."
    },
    {
        "origin": "Interested users can take a look at Chapter 5 of the Hugging Face course.",
        "similar": "Those who are curious can examine Chapter 5 of the Hugging Face course."
    },
    {
        "origin": "The guides are organized into six sections: General usage, Audio, Vision, Text, Tabular, and Dataset repository.",
        "similar": "Guides have been divided into six parts: General usage, Audio, Vision, Text, Tabular, and Dataset repository."
    },
    {
        "origin": "If you have any questions about \ud83e\udd17 Datasets, you can join and ask the community on the forum.",
        "similar": "If you have queries regarding \ud83e\udd17 Datasets, you can join the forum and seek answers from the community."
    },
    {
        "origin": "Cache management in datasets refers to the process of storing processed data and scripts locally on a user's computer to avoid re-downloading or processing the entire dataset every time it is used.",
        "similar": "The practice of caching in datasets is the act of storing already processed data and scripts on a user's computer to prevent the need for re-downloading or reprocessing the entire dataset each time it is accessed."
    },
    {
        "origin": "Cache management is important in datasets because it helps to save time and resources by avoiding the need to re-download or re-process the entire dataset every time it is used.",
        "similar": "Cache management is essential in datasets as it helps to conserve time and resources by preventing the requirement to re-download or re-process the entire dataset each time it is utilized."
    },
    {
        "origin": "To change the cache directory in datasets, you can follow the guide provided in the documentation.",
        "similar": "You can refer to the documentation to alter the cache directory in datasets."
    },
    {
        "origin": "You can change the default cache directory for datasets by setting the shell environment variable `HF_DATASETS_CACHE` to another directory.",
        "similar": "By altering the shell environment variable `HF_DATASETS_CACHE` to a different directory, the default cache directory for datasets can be changed."
    },
    {
        "origin": "You can change the cache directory for a specific dataset or metric by setting the `cache_dir` parameter to the desired path when loading the dataset or metric.",
        "similar": "You can specify the directory for caching a certain dataset or metric by setting the `cache_dir` parameter to the desired path when loading the dataset or metric."
    },
    {
        "origin": "You can control how a dataset is loaded from the cache by using the `load_from_cache_file` argument in the `Dataset.map()` method. Setting it to `False` will execute the function over the entire dataset again instead of loading the dataset from its previous state.",
        "similar": "By using the `load_from_cache_file` argument in the `Dataset.map()` method, you can determine whether a dataset is loaded from the cache or not. If you set it to `False`, the function will be executed over the entire dataset instead of loading it from its previous state."
    },
    {
        "origin": "You can clean up cache files in the directory by using the `Dataset.cleanup_cache_files()` method.",
        "similar": "The `Dataset.cleanup_cache_files()` method can be used to clear out cache files in the directory."
    },
    {
        "origin": "You can enable or disable caching by setting the `load_from_cache_file` argument in the `Dataset.map()` method or by using the `disable_caching()` function. You can also keep the metric in CPU memory instead of caching it by setting the `keep_in_memory` parameter to `True` when loading the metric.",
        "similar": "You can control caching by adjusting the `load_from_cache_file` argument in the `Dataset.map()` method or by using the `disable_caching()` function. To keep the metric in CPU memory instead of caching it, set the `keep_in_memory` parameter to `True` when loading the metric."
    },
    {
        "origin": "You can improve dataset performance by disabling the cache and copying the dataset in-memory. This can be done by setting `datasets.config.IN_MEMORY_MAX_SIZE` or the `HF_DATASETS_IN_MEMORY_MAX_SIZE` environment variable to a nonzero value that fits in your RAM memory.",
        "similar": "By disabling the cache and copying the dataset in-memory, you can enhance the performance of the dataset. This can be achieved by setting `datasets.config.IN_MEMORY_MAX_SIZE` or the `HF_DATASETS_IN_MEMORY_MAX_SIZE` environment variable to a value that is compatible with your RAM memory."
    },
    {
        "origin": "Beam Datasets are datasets that are too large to be processed on a single machine, so they are processed with Apache Beam, a library for parallel data processing.",
        "similar": "Datasets that are too voluminous to be handled by one machine are processed using Apache Beam, a library for parallel data processing, which is known as Beam Datasets."
    },
    {
        "origin": "Apache Beam is a library for parallel data processing.",
        "similar": "Apache Beam provides a framework for parallel data processing."
    },
    {
        "origin": "Beam Datasets are processed using a processing pipeline that is executed on a distributed system.",
        "similar": "A distributed system is utilized to execute a processing pipeline for the processing of Beam Datasets."
    },
    {
        "origin": "Beam Datasets support processing backends such as Apache Flink, Apache Spark, or Google Cloud Dataflow.",
        "similar": "Beam Datasets can be processed with backends like Apache Flink, Apache Spark, or Google Cloud Dataflow."
    },
    {
        "origin": "Yes, Beam pipelines are available for some of the larger datasets like wikipedia and wiki40b.",
        "similar": "Beam pipelines can be used to process some of the larger datasets, such as Wikipedia and Wiki40b."
    },
    {
        "origin": "You can load pre-created Beam pipelines normally with load_dataset().",
        "similar": "It is possible to load pre-existing Beam pipelines using the load_dataset() function."
    },
    {
        "origin": "To run your own Beam pipeline with Dataflow in Beam Datasets, you need to specify the dataset and configuration you want to process, input your Google Cloud Platform information, specify your Python requirements, and run the pipeline.",
        "similar": "In order to execute a Beam pipeline with Dataflow in Beam Datasets, you must provide the dataset and configuration you wish to process, enter your Google Cloud Platform credentials, designate your Python requirements, and execute the pipeline."
    },
    {
        "origin": "Yes, you can adjust the parameters to change the runner (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers when running your pipeline in Beam Datasets.",
        "similar": "It is possible to modify the settings to switch between Flink and Spark, the output destination between S3 bucket and HDFS, and the number of workers when executing your pipeline with Beam Datasets."
    },
    {
        "origin": "No, it doesn't exist in version 2.10.0.",
        "similar": "Version 2.10.0 does not contain it."
    },
    {
        "origin": "You can find it on the main version of the documentation. Click on the provided link to redirect to the main version.",
        "similar": "The main version of the documentation can be accessed by clicking on the link."
    },
    {
        "origin": "You can load text data using Hugging Face's Datasets by following the guide provided in the \"Load text data\" section of the documentation.",
        "similar": "By consulting the \"Load text data\" section of the Hugging Face's Datasets documentation, one can load text data."
    },
    {
        "origin": "You can find tutorials for using Hugging Face's Datasets in the \"Tutorials\" section of the documentation.",
        "similar": "Tutorials on how to utilize Hugging Face's Datasets can be located in the \"Tutorials\" part of the documentation."
    },
    {
        "origin": "Yes, you can use Hugging Face's Datasets with TensorFlow or PyTorch. There are specific guides provided in the \"General usage\" section of the documentation for using Hugging Face's Datasets with these frameworks.",
        "similar": "You can employ Hugging Face's Datasets with either TensorFlow or PyTorch. The \"General usage\" section of the documentation offers specific instructions for using the Datasets with these frameworks."
    },
    {
        "origin": "You can share a dataset to the Hub using Hugging Face's Datasets by following the guide provided in the \"Share a dataset to the Hub\" tutorial in the \"Tutorials\" section of the documentation.",
        "similar": "By referring to the \"Share a dataset to the Hub\" tutorial in the \"Tutorials\" section of the documentation, you can learn how to share a dataset to the Hub with Hugging Face's Datasets."
    },
    {
        "origin": "You can load various types of data using Hugging Face's Datasets, including text, audio, vision, and tabular data. There are specific guides provided in the documentation for loading each type of data.",
        "similar": "Using Hugging Face's Datasets, you can load a range of data types, such as text, audio, vision, and tabular data. The documentation provides specific instructions for loading each type of data."
    },
    {
        "origin": "\ud83e\udd17 Datasets is a library for working with datasets in Python.",
        "similar": "Python's library for manipulating datasets is called Datasets."
    },
    {
        "origin": "\ud83e\udd17 Datasets samples a text file line by line to build the dataset.",
        "similar": "The dataset is constructed by sampling a text file line by line."
    },
    {
        "origin": "You can load a dataset from a directory using \ud83e\udd17 Datasets by specifying the data directory path.",
        "similar": "\ud83e\udd17 Datasets allows you to load a dataset from a directory by providing the path of the data directory."
    },
    {
        "origin": "You can sample a text file by paragraph or entire document using \ud83e\udd17 Datasets by specifying the `sample_by` parameter.",
        "similar": "You can select the sampling of a text file either by paragraph or the whole document by setting the `sample_by` parameter in \ud83e\udd17 Datasets."
    },
    {
        "origin": "Yes, you can use grep patterns to load specific files using \ud83e\udd17 Datasets.",
        "similar": "\ud83e\udd17 Datasets can be used to load specific files by utilizing grep patterns."
    },
    {
        "origin": "Yes, you can load remote text files via HTTP using \ud83e\udd17 Datasets by passing the URLs instead of local file paths.",
        "similar": "It is possible to obtain remote text files via HTTP using \ud83e\udd17 Datasets by providing the URLs instead of local file paths."
    },
    {
        "origin": "Albumentations is a Python library for performing data augmentation for computer vision.",
        "similar": "Albumentations is a Python package providing data augmentation for computer vision."
    },
    {
        "origin": "The NYU Depth V2 dataset is comprised of video sequences from various indoor scenes, recorded by RGB and depth cameras. The dataset consists of scenes from 3 cities and provides images along with their depth maps as labels.",
        "similar": "The NYU Depth V2 dataset is composed of video sequences from multiple indoor scenes, captured by RGB and depth cameras. It contains scenes from 3 cities and offers images with their corresponding depth maps as labels."
    },
    {
        "origin": "We can load the train split of the NYU Depth V2 dataset using the following code: \n```\nfrom datasets import load_dataset\ntrain_dataset = load_dataset(\"sayakpaul/nyu_depth_v2\", split=\"train\")\n```",
        "similar": "The following code can be used to get the train split of the NYU Depth V2 dataset: \n```\nfrom datasets import load_dataset\ntrain_dataset = load_dataset(\"sayakpaul/nyu_depth_v2\", split=\"train\")\n```"
    },
    {
        "origin": "The two fields in the NYU Depth V2 dataset are `image` and `depth_map`.",
        "similar": "The NYU Depth V2 dataset contains two fields, namely `image` and `depth_map`."
    },
    {
        "origin": "We need to first convert the data type of the depth map to `uint8` using `.convert('RGB')` as PIL can\u2019t display `float32` images. Then we can create a colored depth map using `plt.cm` and display it using `plt.imshow()` or the `show_depthmap()` function provided in the guide.",
        "similar": "Using `.convert('RGB')`, we must first convert the data type of the depth map to `uint8` as PIL cannot display `float32` images. Afterwards, we can use `plt.cm` to create a colored depth map and display it with either `plt.imshow()` or the `show_depthmap()` function given in the guide."
    },
    {
        "origin": "The guide provides instructions on how to apply transformations to a depth estimation dataset using the Albumentations library.",
        "similar": "The guide furnishes directions on how to execute transformations to a depth estimation dataset through the Albumentations library."
    },
    {
        "origin": "The purpose of the Datasets documentation is to provide tutorials, how-to guides, and conceptual guides for working with \ud83e\udd17 Datasets.",
        "similar": "The goal of the Datasets documentation is to offer tutorials, how-to instructions, and conceptual guidance for utilizing \ud83e\udd17 Datasets."
    },
    {
        "origin": "The Datasets documentation covers topics such as loading and processing data, using Datasets with TensorFlow and PyTorch, creating datasets, and working with audio, vision, text, and tabular data.",
        "similar": "The Datasets manual provides information on how to load and process data, integrate Datasets with TensorFlow and PyTorch, construct datasets, and handle audio, visual, textual, and tabular data."
    },
    {
        "origin": "You can get started with \ud83e\udd17 Datasets by following the Quickstart guide, which is available in the Datasets documentation.",
        "similar": "To begin using \ud83e\udd17 Datasets, refer to the Quickstart guide in the Datasets documentation for guidance."
    },
    {
        "origin": "By joining the Hugging Face community, you can collaborate on models, datasets, and Spaces, access faster examples with accelerated inference, and switch between documentation themes.",
        "similar": "By becoming a part of the Hugging Face community, you can work together on models, datasets, and Spaces, get quicker results with accelerated inference, and change between documentation themes."
    },
    {
        "origin": "You will learn how to load different dataset configurations and splits, interact with and see what\u2019s inside your dataset, preprocess, and share a dataset to the Hub.",
        "similar": "You will be taught how to upload various dataset configurations and divisions, interact with the data, analyze its content, pre-process it, and upload it to the Hub."
    },
    {
        "origin": "The tutorials assume some basic knowledge of Python and a machine learning framework like PyTorch or TensorFlow.",
        "similar": "The tutorials presuppose a rudimentary understanding of Python and a machine learning library such as PyTorch or TensorFlow."
    },
    {
        "origin": "You can check out the quickstart to see what you can do with \ud83e\udd17 Datasets.",
        "similar": "Have a look at the quickstart to find out what \ud83e\udd17 Datasets can offer."
    },
    {
        "origin": "No, the tutorials only cover the basic skills you need to use \ud83e\udd17 Datasets. There are many other useful functionalities and applications that aren\u2019t discussed here.",
        "similar": "The tutorials provided here only provide an introduction to the fundamentals of working with Datasets. There are many other features and applications that have not been addressed."
    },
    {
        "origin": "You can take a look at Chapter 5 of the Hugging Face course.",
        "similar": "You may want to peruse Chapter 5 of the Hugging Face course."
    },
    {
        "origin": "You can join and ask the community on the forum.",
        "similar": "You can become a member of the forum and pose questions to the community."
    },
    {
        "origin": "Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.",
        "similar": "Datasets is a repository which facilitates the access and exchange of datasets for Audio, Computer Vision, and Natural Language Processing (NLP) activities."
    },
    {
        "origin": "Datasets supports tasks related to Audio, Computer Vision, and Natural Language Processing (NLP).",
        "similar": "Datasets can be utilized for tasks related to Audio, Computer Vision, and Natural Language Processing (NLP)."
    },
    {
        "origin": "The tutorials available for Datasets are: Overview, Load a dataset from the Hub, Know your dataset, Preprocess, Evaluate predictions, Create a dataset, and Share a dataset to the Hub.",
        "similar": "The tutorials for Datasets include: An Overview, Loading a dataset from the Hub, Becoming familiar with the dataset, Preprocessing, Assessing predictions, Generating a dataset, and Uploading a dataset to the Hub."
    },
    {
        "origin": "Datasets can load audio, image, text, and tabular data.",
        "similar": "Datasets are able to accommodate audio, image, textual, and tabular information."
    },
    {
        "origin": "Yes, Datasets can be used with TensorFlow, PyTorch, and JAX.",
        "similar": "TensorFlow, PyTorch, and JAX are all compatible with Datasets."
    },
    {
        "origin": "The purpose of the Datasets repository is to share, create a dataset loading script, create a dataset card, and structure the repository.",
        "similar": "The objective of the Datasets repository is to provide a platform to share, generate a script for loading a dataset, generate a dataset card, and organize the repository."
    },
    {
        "origin": "The types of guides available for Datasets are: How-to guides, Conceptual guides, and Reference.",
        "similar": "The different types of guides for Datasets are: Tutorials, Explanations, and Documentation."
    },
    {
        "origin": "Yes, you can load a dataset in a single line of code and use powerful data processing methods to quickly get your dataset ready for training.",
        "similar": "It is possible to load a dataset in one line of code and then take advantage of powerful data manipulation techniques to quickly prepare the dataset for training."
    },
    {
        "origin": "Yes, you can process large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency.",
        "similar": "It is possible to process large datasets with zero-copy reads, allowing for maximum speed and efficiency without any memory limitations."
    },
    {
        "origin": "Yes, this tool is backed by the Apache Arrow format.",
        "similar": "This tool is supported by the Apache Arrow format."
    },
    {
        "origin": "Yes, this tool features a deep integration with the Hugging Face Hub, allowing you to easily load and share a dataset with the wider machine learning community.",
        "similar": "This tool has a strong connection with the Hugging Face Hub, making it easy to upload and share datasets with the machine learning community."
    },
    {
        "origin": "Yes, there are tutorials available to help you learn the basics and become familiar with loading, accessing, and processing a dataset.",
        "similar": "Tutorials are available to assist you in grasping the fundamentals and getting used to loading, accessing, and manipulating a dataset."
    },
    {
        "origin": "Yes, there are practical guides available to help you achieve a specific goal and learn how to use this tool to solve real-world problems.",
        "similar": "Yes, there are useful manuals available to assist you in reaching a particular objective and understand how to employ this tool to tackle real-world issues."
    },
    {
        "origin": "Yes, there are conceptual guides available to provide high-level explanations for building a better understanding about important topics such as the underlying data format, the cache, and how datasets are generated.",
        "similar": "Conceptual guides are available to help build a better understanding of significant topics like the data format, the cache, and how datasets are created."
    },
    {
        "origin": "Yes, there are technical descriptions available in the reference section for how this tool's classes and methods work.",
        "similar": "Technical descriptions of the classes and methods of this tool can be found in the reference section."
    },
    {
        "origin": "You can load an audio dataset using the Audio feature that automatically decodes and resamples the audio files when you access the examples.",
        "similar": "You can access the examples that are automatically decoded and resampled by the Audio feature to load an audio dataset."
    },
    {
        "origin": "The Audio feature automatically decodes and resamples the audio files when you access the examples.",
        "similar": "When you access the examples, the Audio feature will automatically decode and resample the audio files."
    },
    {
        "origin": "You can find information on loading text data under the Text section of the documentation.",
        "similar": "The Text section of the documentation has details on loading text data."
    },
    {
        "origin": "Yes, you can collaborate on models, datasets, and Spaces with the Hugging Face community by signing up for an account.",
        "similar": "By registering for an account, you can join the Hugging Face community and work together on models, datasets, and Spaces."
    },
    {
        "origin": "The `audio` dependencies need to be installed.",
        "similar": "The `audio` libraries must be installed."
    },
    {
        "origin": "Use the `cast_column()` function to take a column of audio file paths, and cast it to the `Audio` feature.",
        "similar": "The `cast_column()` function is to be employed to convert a column of audio file paths into an `Audio` feature."
    },
    {
        "origin": "`AudioFolder` is a dataset builder that allows for quickly creating and loading audio datasets with several thousand audio files.",
        "similar": "`AudioFolder` is a dataset builder that facilitates the rapid assembly and loading of audio datasets with thousands of audio files."
    },
    {
        "origin": "Make sure your dataset has a `metadata.csv` file with a `file_name` column which links audio files with their metadata.",
        "similar": "Ensure that your dataset has a `metadata.csv` file which contains a `file_name` column that connects audio files to their respective metadata."
    },
    {
        "origin": "Yes, remote datasets can be loaded from their URLs with the `data_files` parameter.",
        "similar": "It is possible to load remote datasets from their URLs by using the `data_files` parameter."
    },
    {
        "origin": "Set `drop_metadata=True` in `load_dataset()`.",
        "similar": "Include the option `drop_metadata=True` when calling `load_dataset()`."
    },
    {
        "origin": "`AudioFolder` automatically infers the label name from the directory name.",
        "similar": "`AudioFolder` can deduce the label name from the directory name without any manual input."
    },
    {
        "origin": "The `load_dataset` function is used to load an audio dataset.",
        "similar": "The audio dataset can be loaded by utilizing the `load_dataset` function."
    },
    {
        "origin": "The `data_dir` parameter is used to specify the path to the folder containing the audio data.",
        "similar": "The path to the folder with the audio data is specified by the `data_dir` parameter."
    },
    {
        "origin": "Yes, you can create your own `AudioFolder` dataset. For more information, you can refer to the \"Create an audio dataset\" guide.",
        "similar": "You can create your own `AudioFolder` dataset, as outlined in the \"Create an audio dataset\" guide, for more details."
    },
    {
        "origin": "You can find a guide on how to load any type of dataset in the \"general loading guide\".",
        "similar": "A guide on how to load datasets of any kind can be located in the \"general loading guide\"."
    },
    {
        "origin": "Yes, other types of audio datasets can be loaded using this code. For example, you can load an `AudioFolder` dataset with metadata.",
        "similar": "This code can be used to load not only one type of audio dataset, but also others. For instance, an `AudioFolder` dataset with metadata can be loaded."
    },
    {
        "origin": "Other options available for loading audio data using this code include local files and `AudioFolder` with metadata.",
        "similar": "This code can be used to load audio data from local files and `AudioFolder` with metadata, as well as other available options."
    },
    {
        "origin": "Datasets is a tool designed by Hugging Face that allows anyone to share a dataset with the greater Machine Learning community.",
        "similar": "Hugging Face has created a tool called Datasets, which enables people to make their datasets available to the Machine Learning world."
    },
    {
        "origin": "You can get started with Datasets by visiting the Quickstart and Installation pages in the documentation.",
        "similar": "To begin working with Datasets, you can refer to the Quickstart and Installation sections of the documentation."
    },
    {
        "origin": "Datasets supports audio, vision, text, and tabular data.",
        "similar": "Datasets accommodate audio, visual, textual, and tabular information."
    },
    {
        "origin": "Yes, Datasets can be used with TensorFlow, PyTorch, and JAX.",
        "similar": "TensorFlow, PyTorch, and JAX are all compatible with Datasets."
    },
    {
        "origin": "Yes, you can create your own dataset using Datasets.",
        "similar": "You have the ability to generate your own dataset with the help of Datasets."
    },
    {
        "origin": "You can share a dataset to the Hugging Face Hub by following the instructions on the Share page in the documentation.",
        "similar": "By adhering to the guidelines on the Share page of the documentation, you can submit a dataset to the Hugging Face Hub."
    },
    {
        "origin": "The Hugging Face community is a group of individuals who collaborate on models, datasets, and Spaces to democratize good Machine Learning.",
        "similar": "The Hugging Face collective is a collection of people who work together on models, datasets, and Spaces to make Machine Learning available to everyone."
    },
    {
        "origin": "There are currently thousands of datasets in over 100 languages in the Hugging Face Hub.",
        "similar": "At present, the Hugging Face Hub contains thousands of datasets in more than one hundred languages."
    },
    {
        "origin": "Dataset repositories offer features such as free dataset hosting, dataset versioning, commit history and diffs, metadata for discoverability, and dataset cards for documentation, licensing, limitations, etc.",
        "similar": "Dataset repositories provide amenities like free data hosting, version control, commit history and diffs, metadata to facilitate discovery, and dataset cards to document licensing, restrictions, and so on."
    },
    {
        "origin": "You can share your dataset with the community by creating a dataset repository on the Hugging Face Hub. It can also be a private dataset if you want to control who has access to it.",
        "similar": "By creating a dataset repository on the Hugging Face Hub, you can make your dataset available to the public or keep it private, depending on your preference."
    },
    {
        "origin": "A dataset script is optional if your dataset is in one of the supported formats such as CSV, JSON, JSON lines, text or Parquet. The script also supports many kinds of compressed file types such as GZ, BZ2, LZ4, LZMA or ZSTD. If your dataset is not in a supported format or if you want more control over how your dataset is loaded, you can write your own dataset script.",
        "similar": "If your dataset is not in a supported format like CSV, JSON, JSON lines, text or Parquet, or if you want more control over how your dataset is loaded, you can write your own dataset script. Otherwise, the script also supports many kinds of compressed file types such as GZ, BZ2, LZ4, LZMA or ZSTD, and the dataset script is optional."
    },
    {
        "origin": "If there\u2019s no dataset script, all the files in the supported formats are loaded. If there\u2019s a dataset script, it is downloaded and executed to download and prepare the dataset.",
        "similar": "In the absence of a dataset script, all files in the supported formats will be loaded. However, if a dataset script is present, it will be downloaded and executed in order to download and prepare the dataset."
    },
    {
        "origin": "You can create a new dataset repository from your account on the Hugging Face Hub or upload a dataset from the terminal using the huggingface-cli repo create command.",
        "similar": "You can either generate a new dataset repository from your Hugging Face Hub account or utilize the huggingface-cli repo create command to upload a dataset from the terminal."
    },
    {
        "origin": "You can clone the repository using Git LFS and the git clone command.",
        "similar": "Git LFS and the git clone command can be used to clone the repository."
    },
    {
        "origin": "You should upload the raw data files of the dataset (optional, if they are hosted elsewhere you can specify the URLs in the dataset script) and your dataset loading script (optional if your data files are already in the supported formats csv/jsonl/json/parquet/txt).",
        "similar": "It is optional to upload the raw data files of the dataset, but if they are hosted elsewhere, the URLs should be specified in the dataset script. Additionally, it is also optional to include the dataset loading script if the data files are already in the supported formats such as csv/jsonl/json/parquet/txt."
    },
    {
        "origin": "You can load your dataset in a single line of code using the following command: `dataset = load_dataset(\"namespace/your_dataset_name\")`.",
        "similar": "A single line of code is all it takes to load your dataset, just use the command `dataset = load_dataset(\"namespace/your_dataset_name\")`."
    },
    {
        "origin": "You can make your dataset discoverable by creating a dataset card to document it. You can learn more about creating a dataset card by checking out the \"Create a dataset card\" guide.",
        "similar": "By creating a dataset card, you can make your dataset discoverable. To learn more about this process, please refer to the \"Create a dataset card\" guide."
    },
    {
        "origin": "You can get help with your dataset script by checking the datasets forum. It is possible that someone had similar issues and shared how they managed to fix them.",
        "similar": "By consulting the datasets forum, you may be able to find assistance with your dataset script. It is likely that someone has encountered similar problems and revealed how they were able to resolve them."
    },
    {
        "origin": "If you wish your dataset script to be reviewed by the Hugging Face team, you can open a discussion in the Community tab of your dataset with the message: \"# Dataset review request for <Dataset name>\". Members of the Hugging Face team will be happy to review your dataset script and give you advice.",
        "similar": "If you would like the Hugging Face team to evaluate your dataset script, you can start a conversation in the Community section of your dataset with the statement: \"# Dataset review request for <Dataset name>\". The Hugging Face team will be glad to review your dataset script and provide feedback."
    },
    {
        "origin": "If you think a fix is needed for a legacy GitHub dataset, you can use their \"Community\" tab to open a discussion or create a Pull Request. The code of these datasets is reviewed by the Hugging Face team.",
        "similar": "If you believe a modification is necessary for an existing GitHub dataset, you can initiate a discussion or submit a Pull Request via the \"Community\" tab. The Hugging Face team will then inspect the code of the dataset."
    },
    {
        "origin": "The purpose of the Datasets documentation is to provide information and guidance on how to use and process datasets.",
        "similar": "The aim of the Datasets documentation is to furnish knowledge and advice on how to utilize and manipulate datasets."
    },
    {
        "origin": "Some of the tutorials available in the Datasets documentation include loading a dataset from the Hub, preprocessing, evaluating predictions, creating a dataset, and sharing a dataset to the Hub.",
        "similar": "In the Datasets documentation, tutorials are provided for tasks such as loading a dataset from the Hub, preprocessing, evaluating predictions, creating a dataset, and uploading a dataset to the Hub."
    },
    {
        "origin": "Some of the how-to guides available in the Datasets documentation include loading, processing, streaming, using with TensorFlow, using with PyTorch, cache management, cloud storage, search index, metrics, and Beam Datasets.",
        "similar": "The Datasets documentation provides a range of tutorials, such as loading, processing, streaming, integration with TensorFlow and PyTorch, cache management, cloud storage, search index, metrics, and Beam Datasets."
    },
    {
        "origin": "Some of the methods for processing image datasets include using map() with image dataset.",
        "similar": "Using map() is one of the techniques for manipulating image datasets."
    },
    {
        "origin": "The function to apply data augmentations to a dataset is `set_transform()`.",
        "similar": "The `set_transform()` is used to apply data augmentations to a dataset."
    },
    {
        "origin": "The `map()` function can apply transforms over an entire dataset.",
        "similar": "The `map()` function can be used to apply transformations to all elements of a dataset."
    },
    {
        "origin": "An example of a transform that can be applied to an image is `ColorJitter`.",
        "similar": "An illustration of a transformation that can be utilized on an image is `ColorJitter`."
    },
    {
        "origin": "`set_transform()` consumes less disk space compared to `map()`.",
        "similar": "`map()` requires more disk space than `set_transform()`."
    },
    {
        "origin": "Other data augmentation libraries that can be used besides `torchvision` include Albumentations, Kornia, and imgaug.",
        "similar": "Besides `torchvision`, Albumentations, Kornia, and imgaug are other data augmentation libraries that can be employed."
    },
    {
        "origin": "Image datasets can be loaded from the `image` column, which contains a PIL object.",
        "similar": "The `image` column holds a PIL object, from which image datasets can be sourced."
    },
    {
        "origin": "To work with image datasets, you need to have the `vision` dependency installed. Check out the installation guide to learn how to install it.",
        "similar": "In order to work with image datasets, it is necessary to have the `vision` dependency installed. Refer to the installation guide to find out how to set it up."
    },
    {
        "origin": "You can load an image dataset using the datasets library by calling the `load_dataset()` function and specifying the name of the dataset and the split you want to load.",
        "similar": "The datasets library provides the `load_dataset()` function to enable loading of an image dataset, where you need to specify the name of the dataset and the split you want to load."
    },
    {
        "origin": "You can index into an image dataset using the row index first and then the `image` column - `dataset[0][\"image\"]` - to avoid decoding and resampling all the image objects in the dataset.",
        "similar": "By using the row index first and then the `image` column - `dataset[0][\"image\"]` - one can access an image dataset without having to decode and resample all the image objects in the dataset."
    },
    {
        "origin": "You can load a dataset from the image path by using the `cast_column()` function to accept a column of image file paths, and decode it into a PIL image with the `Image` feature.",
        "similar": "The `cast_column()` function can be used to accept a column of image file paths, and the `Image` feature can be used to decode it into a PIL image, thus allowing the loading of a dataset from the image path."
    },
    {
        "origin": "You can load a dataset with an `ImageFolder` dataset builder by specifying `imagefolder` and the directory of your dataset in `data_dir`.",
        "similar": "An `ImageFolder` dataset builder can be used to load a dataset by indicating `imagefolder` and the directory of the dataset in `data_dir`."
    },
    {
        "origin": "You can load remote datasets from their URLs with the `data_files` parameter by specifying the URL of the dataset in the `data_files` parameter when calling the `load_dataset()` function.",
        "similar": "The `load_dataset()` function allows you to access remote datasets from their URLs by inputting the URL into the `data_files` parameter."
    },
    {
        "origin": "The guide is about creating an image dataset.",
        "similar": "This guide is devoted to assembling an image dataset."
    },
    {
        "origin": "You can find the guide at the link provided: ./image_dataset.",
        "similar": "The guide can be located at the provided link: ./image_dataset."
    },
    {
        "origin": "Yes, there is a guide for creating an audio dataset. You can find it at the link provided: /docs/datasets/v2.10.0/audio_dataset.",
        "similar": "A guide for constructing an audio dataset is available; the link is /docs/datasets/v2.10.0/audio_dataset."
    },
    {
        "origin": "Yes, there is a guide for processing image data. You can find it at the link provided: /docs/datasets/v2.10.0/image_process.",
        "similar": "html\n\nYou can find a guide on how to process image data at the link provided: /docs/datasets/v2.10.0/image_process.html"
    },
    {
        "origin": "The options for loading image data are local files and ImageFolder.",
        "similar": "The choices for loading image data include local files and ImageFolder."
    },
    {
        "origin": "Arrow is a data format that stores data in a columnar memory layout, enabling large amounts of data to be processed and moved quickly.",
        "similar": "Arrow is a data format that stores information in a columnar memory structure, thus allowing for rapid processing and transportation of large volumes of data."
    },
    {
        "origin": "Arrow provides several significant advantages, including the ability to process and move large amounts of data quickly due to its columnar memory layout.",
        "similar": "The columnar memory layout of Arrow facilitates the rapid processing and transfer of large amounts of data, making it a highly advantageous technology."
    },
    {
        "origin": "Arrow is a language-agnostic data interchange format that allows for zero-copy reads and supports many column types.",
        "similar": "Arrow is a data interchange format that is not specific to any language and enables zero-copy reads, as well as accommodating a variety of column types."
    },
    {
        "origin": "Arrow's format allows for zero-copy reads, which removes serialization overhead, and it is column-oriented, making it faster at querying and processing slices or columns of data. Arrow also supports many column types and allows for copy-free hand-offs to standard machine learning tools.",
        "similar": "Arrow's format is designed to enable zero-copy reads, thereby eliminating serialization overhead, and its column-oriented structure makes it highly efficient when it comes to querying and processing data slices or columns. Moreover, Arrow supports a wide range of column types and facilitates copy-free hand-offs to common machine learning tools."
    },
    {
        "origin": "Arrow's memory-mapping feature allows for datasets to be backed by an on-disk cache, which is memory-mapped for fast lookup. This architecture allows for large datasets to be used on machines with relatively small device memory.",
        "similar": "Arrow's memory-mapping capability permits datasets to be supported by an on-disk cache, which is mapped to memory for quick retrieval. This design makes it possible to use large datasets on machines with limited device memory."
    },
    {
        "origin": "Iterating over a memory-mapped dataset using Arrow is fast. For example, iterating over the full English Wikipedia dataset on a laptop gives speeds of 1-3 Gbit/s.",
        "similar": "Iterating through a memory-mapped dataset with Arrow is speedy. For instance, going through the full English Wikipedia dataset on a laptop yields speeds of 1-3 Gbit/s."
    },
    {
        "origin": "A dataset card is a tool to promote responsible usage and inform users of any potential biases within a dataset.",
        "similar": "A dataset card serves to encourage responsible usage and alert users to any potential biases that may exist in a dataset."
    },
    {
        "origin": "Dataset cards help users understand and evaluate the data they are using, including any potential biases or limitations.",
        "similar": "Cards for datasets enable users to comprehend and appraise the data they are utilizing, including any possible biases or restrictions."
    },
    {
        "origin": "To create a dataset card, you should provide information about the dataset, including its purpose, potential biases, and any relevant metadata.",
        "similar": "In order to construct a dataset card, you should furnish data about the dataset, including its intent, any potential prejudices, and any applicable metadata."
    },
    {
        "origin": "A dataset card is a `README.md` file in a dataset repository that provides information about the dataset's contents, context, creation, and other considerations for users.",
        "similar": "A `README.md` file in a dataset repository, known as a dataset card, is designed to give users information regarding the dataset's context, creation, and other related considerations."
    },
    {
        "origin": "You can create a dataset card by going to your dataset repository on the Hub and clicking on \"Create Dataset Card\". Then, use the Metadata UI to select relevant tags and fill out a template with information about the dataset.",
        "similar": "To make a dataset card, go to your dataset repository on the Hub and click \"Create Dataset Card\". Afterwards, use the Metadata UI to pick applicable tags and fill out the template with details about the dataset."
    },
    {
        "origin": "A dataset card should include information about the dataset's contents, context for using the dataset, how it was created, and any other considerations that users should be aware of. You can refer to the Dataset Card Creation Guide for more detailed information about what to include in each section of the card.",
        "similar": "A Dataset Card should provide information regarding the dataset's contents, context for utilization, how it was formulated, and any other relevant considerations that users should take into account. For more detailed information on what to include in each part of the card, please refer to the Dataset Card Creation Guide."
    },
    {
        "origin": "No, there are no required tags for a dataset card. However, selecting relevant tags can help users discover and find your dataset on the Hub.",
        "similar": "It is not necessary to add tags to a dataset card; nevertheless, it is advantageous to choose pertinent tags to facilitate users to locate your dataset on the Hub."
    },
    {
        "origin": "Yes, you can use existing dataset cards such as SNLI, CNN/DailyMail, and Allocin\u00e9 as examples to help you get started with creating your own dataset card.",
        "similar": "You can take advantage of existing dataset cards such as SNLI, CNN/DailyMail, and Allocin\u00e9 to get a head start on creating your own dataset card."
    },
    {
        "origin": "The Datasets documentation provides information and guides on how to use and process various types of datasets.",
        "similar": "The Datasets documentation furnishes directions and data on the utilization and manipulation of different types of datasets."
    },
    {
        "origin": "The documentation covers audio, vision, text, and tabular datasets.",
        "similar": "The documentation encompasses datasets related to audio, vision, text, and tabular."
    },
    {
        "origin": "The Datasets library provides specific methods for processing text datasets, including tokenization and label alignment for NLI datasets.",
        "similar": "The Datasets library furnishes particular techniques for dealing with text datasets, including tokenization and matching of labels for NLI datasets."
    },
    {
        "origin": "By joining the Hugging Face community, users can access an augmented documentation experience, collaborate on models and datasets, and get faster examples with accelerated inference.",
        "similar": "By becoming a member of the Hugging Face community, users can take advantage of an enhanced documentation experience, work together on models and datasets, and get quicker examples with improved inference."
    },
    {
        "origin": "The map() function in the datasets package supports processing batches of examples at once which speeds up tokenization.",
        "similar": "The datasets package's map() function facilitates the tokenization of multiple examples in a single batch, thereby increasing its speed."
    },
    {
        "origin": "You can load a tokenizer from the Transformers package by using the following code: from transformers import AutoTokenizer and then tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\").",
        "similar": "The following code can be used to get a tokenizer from the Transformers package: from transformers import AutoTokenizer and then tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")."
    },
    {
        "origin": "You can apply the tokenizer to batches of examples by setting the batched parameter to True in the map() function and passing the examples to the tokenizer.",
        "similar": "By setting the batched parameter to True in the map() function and passing the examples to the tokenizer, it is possible to apply the tokenizer to batches of examples."
    },
    {
        "origin": "You can align a dataset label id with the label name by using the align_labels_with_mapping() function and passing a dictionary of the label mappings and the column to align on.",
        "similar": "The align_labels_with_mapping() function can be used to match a dataset label id with its label name by providing a dictionary of label mappings and the column to align on."
    },
    {
        "origin": "This document is a documentation for datasets.",
        "similar": "This document serves as an overview of datasets."
    },
    {
        "origin": "There are two methods for creating and sharing an image dataset. One method is to use `ImageFolder` and add metadata.",
        "similar": "One way to create and distribute an image dataset is to utilize `ImageFolder` and incorporate metadata."
    },
    {
        "origin": "Yes, using `ImageFolder` is a no-code solution for quickly creating an image dataset with several thousand images.",
        "similar": "`ImageFolder` is a no-code approach that allows one to rapidly generate an image dataset with thousands of images."
    },
    {
        "origin": "The document provides information on creating audio, vision, text, and tabular datasets.",
        "similar": "This document furnishes data on how to assemble audio, visual, textual, and tabular datasets."
    },
    {
        "origin": "Yes, you can collaborate on models, datasets, and Spaces using the Hugging Face community.",
        "similar": "It is possible to work together on models, datasets, and Spaces through the Hugging Face community."
    },
    {
        "origin": "Yes, you can switch between documentation themes by signing up for the augmented documentation experience.",
        "similar": "By signing up for the augmented documentation experience, you can change between documentation themes."
    },
    {
        "origin": "It is a file that contains metadata information about objects in an image, including their bounding boxes and categories.",
        "similar": "This file holds metadata about objects in an image, including their boundaries and classifications."
    },
    {
        "origin": "You can load a dataset with `ImageFolder` by using the `load_dataset` function and specifying the `imagefolder` dataset type, along with the directory path and split.",
        "similar": "The `load_dataset` function can be used to load a dataset of type `ImageFolder` by providing the directory path and split."
    },
    {
        "origin": "You can share your dataset to the Hub by using the `push_to_hub()` method and specifying your Hugging Face account and the name of your dataset.",
        "similar": "You can upload your dataset to the Hub by calling the `push_to_hub()` method and providing your Hugging Face account details and the name of your dataset."
    },
    {
        "origin": "A dataset loading script is a Python script that defines a dataset's splits and configurations, and handles downloading and generating the dataset.",
        "similar": "A Python script which defines the splits and configurations of a dataset, as well as managing the downloading and generation of the dataset, is known as a dataset loading script."
    },
    {
        "origin": "The base class for datasets generated from a dictionary generator is `GeneratorBasedBuilder`.",
        "similar": "The `GeneratorBasedBuilder` is the foundation class for datasets that are generated from a dictionary generator."
    },
    {
        "origin": "The document mentions the Imagenette dataset and provides a link to it, but does not give a detailed description of it.",
        "similar": "The document makes reference to the Imagenette dataset and furnishes a link to it, yet does not supply a thorough explanation of it."
    },
    {
        "origin": "You can use the BuilderConfig class to create a subclass for your dataset and provide the links to download the images and labels in data_url and metadata_urls.",
        "similar": "A subclass of BuilderConfig can be created to generate the links to download the images and labels in data_url and metadata_urls."
    },
    {
        "origin": "You can define your subsets at the top of GeneratorBasedBuilder and provide a name, description, and where to download the images and labels from for each configuration.",
        "similar": "At the top of GeneratorBasedBuilder, you can specify the subsets you want to create with a name, description, and the source of images and labels for each configuration."
    },
    {
        "origin": "Users can load a specific configuration of the dataset by using the configuration name when calling the load_dataset function.",
        "similar": "The load_dataset function allows users to access a particular configuration of the dataset by specifying the configuration name."
    },
    {
        "origin": "Some important information to include in the DatasetInfo class are description, features, supervised_keys, homepage, citation, and license.",
        "similar": "In the DatasetInfo class, it is essential to include details such as description, features, supervised_keys, homepage, citation, and license."
    },
    {
        "origin": "The given code is a guide for generating datasets using the Hugging Face Datasets library.",
        "similar": "The Hugging Face Datasets library provides a code to create datasets."
    },
    {
        "origin": "The DownloadManager.download() method accepts a name to a file inside a Hub dataset repository, a URL to a file hosted somewhere else, or a list or dictionary of file names or URLs.",
        "similar": "The DownloadManager.download() method can take a file name from a Hub dataset repository, a URL pointing to a file hosted externally, or a list/dictionary containing file names/URLs."
    },
    {
        "origin": "The splits should be named with a standard name like: `Split.TRAIN`, `Split.TEST`, and `SPLIT.Validation`.",
        "similar": "Splits should be labeled using a standardized naming convention such as `Split.TRAIN`, `Split.TEST`, and `SPLIT.Validation`."
    },
    {
        "origin": "The _generate_examples() method generates images and labels for splits.",
        "similar": "The _generate_examples() function produces images and labels for divisions."
    },
    {
        "origin": "The purpose of the given code is to yield a dictionary containing image path, image bytes, and label for each image file in the specified directory.",
        "similar": "The given code is designed to generate a dictionary with image path, image bytes, and label for each image file located in the specified directory."
    },
    {
        "origin": "The command to generate dataset metadata and test the loading script is \"datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs\".",
        "similar": "To create metadata for the dataset and check the loading script, use the command \"datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs\"."
    },
    {
        "origin": "The dataset can be loaded from the Hub using the \"load_dataset\" function from the \"datasets\" module by specifying the username and dataset name.",
        "similar": "The \"load_dataset\" function from the \"datasets\" module enables one to access the dataset from the Hub by providing the username and dataset name."
    },
    {
        "origin": "The two types of dataset objects are Dataset and IterableDataset.",
        "similar": "Two varieties of dataset objects are Dataset and IterableDataset."
    },
    {
        "origin": "The difference between Dataset and IterableDataset is not specified in the given document.",
        "similar": "It is not stated in the given document what the distinction is between Dataset and IterableDataset."
    },
    {
        "origin": "The main difference between a Dataset and an IterableDataset is that a Dataset provides random access to the rows, while an IterableDataset loads the data progressively as you iterate over the dataset.",
        "similar": "A major distinction between a Dataset and an IterableDataset is that a Dataset allows for random access to the rows, while an IterableDataset loads the data gradually as it is traversed."
    },
    {
        "origin": "An IterableDataset is ideal for big datasets (think hundreds of GBs!) due to its lazy behavior and speed advantages. It is also great for streaming datasets made out of multiple shards, each of which is hundreds of gigabytes.",
        "similar": "The IterableDataset is perfect for large datasets (think hundreds of GBs!) because of its lazy loading and speed benefits. It is also great for streaming datasets composed of multiple shards, each of which is hundreds of gigabytes."
    },
    {
        "origin": "You can access a row in a Dataset using `my_dataset[0]`, which provides random access to the rows.",
        "similar": "Random access to the rows of a Dataset can be achieved by using `my_dataset[0]`."
    },
    {
        "origin": "You can access a row in an IterableDataset using a `for` loop to load the data progressively as you iterate over the dataset.",
        "similar": "A `for` loop can be used to load data progressively from an IterableDataset row by row."
    },
    {
        "origin": "You can create a Dataset using lists or dictionaries, and the data is entirely converted to Arrow so you can easily access any row.",
        "similar": "You can construct a Dataset from either lists or dictionaries, and all the data is converted to Arrow, thus allowing you to access any row with ease."
    },
    {
        "origin": "To create an IterableDataset, you must provide a \u201clazy\u201d way to load the data. In Python, we generally use generator functions. These functions `yield` one example at a time.",
        "similar": "In order to construct an IterableDataset, a \"lazy\" loading method must be supplied. For this purpose, generator functions are commonly used in Python, which `emit` a single example at a time."
    },
    {
        "origin": "The conversion step from CSV to Arrow format is required to load the dataset and it takes time and disk space if the dataset is big.",
        "similar": "The transformation from CSV to Arrow format is essential for loading the dataset and it can be time-consuming and space-consuming if the dataset is large."
    },
    {
        "origin": "You can define an IterableDataset by streaming from the local files directly to skip the conversion step and save disk space.",
        "similar": "You can stream local files directly to an IterableDataset, thereby avoiding the need to convert and thus conserving disk space."
    },
    {
        "origin": "Many file formats are supported, including CSV, JSONL, Parquet, image, and audio files.",
        "similar": "A variety of file formats are accepted, such as CSV, JSONL, Parquet, images, and audio."
    },
    {
        "origin": "Eager data processing processes the entire dataset immediately and returns it, while lazy data processing applies the processing steps on-the-fly when iterating over the dataset.",
        "similar": "Data processing that is done eagerly processes the entire dataset right away and provides the result, whereas lazy data processing performs the operations as it goes through the dataset."
    },
    {
        "origin": "Exact shuffling is used for Datasets and shuffles the dataset exactly, while fast approximate shuffling is used for IterableDatasets and uses a shuffle buffer to sample random examples iteratively from the dataset.",
        "similar": "Exact shuffling is employed for Datasets, which shuffles the dataset precisely, while fast approximate shuffling is utilized for IterableDatasets and utilizes a shuffle buffer to take random examples iteratively from the dataset."
    },
    {
        "origin": "An IterableDataset is a type of dataset in the Hugging Face Datasets library that provides fast data loading when iterating using a for loop.",
        "similar": "A for loop can be used to quickly access data from an IterableDataset, which is a kind of dataset in the Hugging Face Datasets library."
    },
    {
        "origin": "You can create an IterableDataset from a generator function by using the from_generator method of the IterableDataset class and passing in the generator function and its arguments as arguments to the method.",
        "similar": "The IterableDataset class' from_generator method can be used to construct an IterableDataset from a generator function, with the generator function and its parameters being passed in as arguments."
    },
    {
        "origin": "You can shuffle an IterableDataset by using the shuffle method of the IterableDataset class and passing in a seed and buffer size as arguments. The shuffle method only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal.",
        "similar": "The IterableDataset class provides a shuffle method which takes a seed and buffer size as arguments. This method rearranges the order of the shards and adds a buffer to the dataset, thus ensuring the speed of the dataset remains optimal."
    },
    {
        "origin": "If you want to switch from a map-style Dataset to an IterableDataset, you can simply create a new IterableDataset object from your existing data using the appropriate method for your data source.",
        "similar": "To transition from a map-style Dataset to an IterableDataset, you can easily generate a new IterableDataset object from your existing data by utilizing the suitable method for your data source."
    },
    {
        "origin": "You can shuffle your dataset by generating a shared IterableDataset using the to_iterable_dataset() method and then using it with a PyTorch DataLoader.",
        "similar": "You can mix up your dataset by creating a shared IterableDataset with the to_iterable_dataset() method and then utilizing it with a PyTorch DataLoader."
    },
    {
        "origin": "An IterableDataset is a type of dataset that allows for streaming and lazy data processing. It can be generated using the to_iterable_dataset() method.",
        "similar": "A streaming and lazy data processing can be achieved with an IterableDataset, which can be created by using the to_iterable_dataset() method."
    },
    {
        "origin": "You can create an IterableDataset with a specific number of shards by using the to_iterable_dataset() method with the num_shards parameter.",
        "similar": "By using the to_iterable_dataset() method with the num_shards parameter, it is possible to generate an IterableDataset with a specific number of shards."
    },
    {
        "origin": "Dataset is a map-style dataset that loads local files entirely, while IterableDataset is a streaming dataset that loads files progressively. Dataset uses eager data processing, while IterableDataset uses lazy data processing. Dataset shuffling is exact, while IterableDataset shuffling is fast and approximate.",
        "similar": "Dataset loads all the local files at once, while IterableDataset streams them in a progressive manner. Dataset performs data processing eagerly, while IterableDataset does it lazily. Dataset shuffling is precise, while IterableDataset shuffling is speedy but not as accurate."
    },
    {
        "origin": "You can switch from map-style to iterable dataset by using the to_iterable_dataset() method.",
        "similar": "The to_iterable_dataset() method can be used to transition from a map-style to an iterable dataset."
    },
    {
        "origin": "The Metrics section in the Datasets documentation is deprecated.",
        "similar": "The Metrics section in the Datasets documentation has been discontinued."
    },
    {
        "origin": "To learn more about how to use metrics, you can take a look at the library \ud83e\udd17 Evaluate.",
        "similar": "You can consult the library to gain insight into the application of metrics."
    },
    {
        "origin": "In addition to metrics, you can find more tools for evaluating models and datasets.",
        "similar": "Apart from metrics, there are other resources available for assessing models and datasets."
    },
    {
        "origin": "`MetricInfo.inputs_description` is used to describe the expected inputs and outputs of a metric. It may also provide an example usage of the metric.",
        "similar": "The `MetricInfo.inputs_description` is utilized to explain the inputs and outputs anticipated from a metric, and can also include an illustration of the metric's application."
    },
    {
        "origin": "`MetricInfo.features` defines the name and type of the predictions and references.",
        "similar": "`MetricInfo.features` specifies the names and types of the predictions and references."
    },
    {
        "origin": "You can download metric files by using the `Metric._download_and_prepare()` method and providing a dictionary of URLs that point to the metric files.",
        "similar": "The `Metric._download_and_prepare()` method can be used to acquire metric files by supplying a dictionary of URLs that link to them."
    },
    {
        "origin": "`DatasetBuilder._compute` provides the actual instructions for how to compute a metric given the predictions and references.",
        "similar": "`DatasetBuilder._compute` furnishes the directions on how to work out a metric based on the predictions and references."
    },
    {
        "origin": "The function `acc_and_f1` returns accuracy and F1 score.",
        "similar": "`acc_and_f1` gives back accuracy and F1 score."
    },
    {
        "origin": "The `DatasetBuilder._compute` function supports the following configurations: \"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", and \"hans\".",
        "similar": "The `DatasetBuilder._compute` function is compatible with the configurations of \"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", and \"hans\"."
    },
    {
        "origin": "The `load_metric` function is used to load a custom metric loading script.",
        "similar": "The `load_metric` function serves to bring in a custom metric loading script."
    },
    {
        "origin": "LOADIN is a feature that was previously available in a specific version of the documentation.",
        "similar": "LOADIN was a feature that was included in a certain version of the documentation."
    },
    {
        "origin": "No, LOADIN is not available in version 2.10.0 of the documentation.",
        "similar": "Version 2.10.0 of the documentation does not include LOADIN."
    },
    {
        "origin": "You can find LOADIN documentation in the main version of the documentation by clicking on the provided link.",
        "similar": "You can access the LOADIN documentation in the main version of the documentation by clicking on the link provided."
    },
    {
        "origin": "Semantic segmentation datasets are used to train a model to classify every pixel in an image.",
        "similar": "Datasets for semantic segmentation are employed to teach a model to assign a label to each pixel in an image."
    },
    {
        "origin": "Some applications enabled by semantic segmentation datasets include background removal from images, stylizing images, or scene segmentation.",
        "similar": "Applications that can be enabled with semantic segmentation datasets include the capability to remove backgrounds from images, stylize images, or divide scenes."
    },
    {
        "origin": "The purpose of the Datasets documentation is to provide information and guidance on how to use and work with datasets.",
        "similar": "The goal of the Datasets documentation is to give instruction and advice on how to manipulate and employ datasets."
    },
    {
        "origin": "The Datasets documentation provides guidance on how to load and work with audio, vision, text, and tabular data.",
        "similar": "The Datasets documentation offers advice on how to manage and manipulate audio, visual, textual, and tabular data."
    },
    {
        "origin": "Users can collaborate on models, datasets, and Spaces using the Hugging Face community.",
        "similar": "Members of the Hugging Face community can work together on models, datasets, and Spaces."
    },
    {
        "origin": "Albumentations is a Python library for performing data augmentation for computer vision.",
        "similar": "Albumentations is a Python library that provides data augmentation capabilities for computer vision."
    },
    {
        "origin": "The Scene Parsing dataset is used for segmenting and parsing an image into different image regions associated with semantic categories, such as sky, road, person, and bed.",
        "similar": "The Scene Parsing dataset is employed for dividing an image into distinct regions correlated with semantic classes, like sky, road, person, and bed."
    },
    {
        "origin": "The Scene Parsing dataset has three fields: `image`, `annotation`, and `scene_category`.",
        "similar": "The Scene Parsing dataset consists of three parts: `image`, `annotation`, and `scene_category`."
    },
    {
        "origin": "You can visualize the dataset by overlaying the segmentation mask on top of the original image using a color palette.",
        "similar": "A color palette can be employed to superimpose the segmentation mask on the original image, thus allowing for visualization of the dataset."
    },
    {
        "origin": "The purpose of applying transformations to the dataset is to augment the data and improve the performance of computer vision models.",
        "similar": "The goal of applying transformations to the dataset is to increase the data and enhance the accuracy of computer vision models."
    },
    {
        "origin": "You can use the set_transform() function to apply the transformation on-the-fly to batches of the dataset to consume less disk space.",
        "similar": "The set_transform() function can be employed to apply the transformation to batches of the dataset in real-time, thus conserving disk space."
    },
    {
        "origin": "You can verify the transformation worked by indexing into the pixel_values and label of an example.",
        "similar": "You can check if the transformation was successful by accessing the pixel_values and label of a sample."
    },
    {
        "origin": "Yes, it is possible to use torchvision to apply some similar transforms.",
        "similar": "It is feasible to employ torchvision to effect some analogous transformations."
    },
    {
        "origin": "You can process a dataset for semantic segmentation by following the steps mentioned in the given document.",
        "similar": "By adhering to the steps outlined in the document, one can process a dataset for semantic segmentation."
    },
    {
        "origin": "You can learn how to train a semantic segmentation model and use it for inference by following the link provided in the given document.",
        "similar": "By following the link provided in the document, one can gain knowledge on how to train a semantic segmentation model and apply it for inference."
    },
    {
        "origin": "The purpose of the datasets.AutomaticSpeechRecognition class is to perform automatic speech recognition.",
        "similar": "The AutomaticSpeechRecognition class is designed to carry out automatic speech recognition as its purpose."
    },
    {
        "origin": "The parameters of the datasets.AudioClassification class are task, audio_column, and label_column.",
        "similar": "The parameters of the AudioClassification class of datasets are task, audio_column, and label_column."
    },
    {
        "origin": "The purpose of the align_with_features method in the datasets.ImageClassification and datasets.TextClassification classes is not specified in the given document.",
        "similar": "The given document does not state the purpose of the align_with_features method in the datasets.ImageClassification and datasets.TextClassification classes."
    },
    {
        "origin": "The purpose of the datasets.LanguageModeling class is to perform language modeling.",
        "similar": "The datasets.LanguageModeling class has been designed to carry out language modeling."
    },
    {
        "origin": "The parameters of the datasets.QuestionAnsweringExtractive class are task, question_column, context_column, and answers_column.",
        "similar": "The parameters of the datasets.QuestionAnsweringExtractive class consist of task, question_column, context_column, and answers_column."
    },
    {
        "origin": "The purpose of the datasets.Summarization class is to perform summarization.",
        "similar": "The aim of the datasets.Summarization class is to carry out summarization."
    },
    {
        "origin": "The parameters of the datasets.TextClassification class are task, text_column, and label_column.",
        "similar": "The parameters of the TextClassification class from the datasets are designated as task, text_column, and label_column."
    },
    {
        "origin": "The PACKAGE_REFERENCE/MAIN_CLASSE documentation page is a page that provides information about a specific package reference.",
        "similar": "The documentation page for PACKAGE_REFERENCE/MAIN_CLASSE offers details about the particular package reference."
    },
    {
        "origin": "No, the PACKAGE_REFERENCE/MAIN_CLASSE documentation page does not exist in v2.10.0.",
        "similar": "The PACKAGE_REFERENCE/MAIN_CLASSE documentation page is not available in v2.10.0."
    },
    {
        "origin": "Yes, you can access the PACKAGE_REFERENCE/MAIN_CLASSE documentation page on the main version of the documentation by clicking on the provided link.",
        "similar": "You can access the PACKAGE_REFERENCE/MAIN_CLASSE documentation page on the main version of the documentation by following the link provided."
    },
    {
        "origin": "A Dataset object is backed by a PyArrow Table.",
        "similar": "A PyArrow Table serves as the foundation for a Dataset object."
    },
    {
        "origin": "Yes, a Table can be loaded from either the disk (memory mapped) or in memory.",
        "similar": "It is possible to either load a Table from the disk (memory mapped) or keep it in memory."
    },
    {
        "origin": "Several Table types are available, and they all inherit from table.Table.",
        "similar": "There are multiple Table types offered, all of which are derived from table.Table."
    },
    {
        "origin": "The Table class in datasets is a class that wraps a pyarrow Table by using composition.",
        "similar": "The Table class in datasets is composed of a pyarrow Table."
    },
    {
        "origin": "The Table class implements all the basic attributes/methods of the pyarrow Table class except the Table transforms: `slice, filter, flatten, combine_chunks, cast, add_column, append_column, remove_column, set_column, rename_columns` and `drop`.",
        "similar": "The Table class does not include the Table transforms: `slice, filter, flatten, combine_chunks, cast, add_column, append_column, remove_column, set_column, rename_columns` and `drop`, which are all basic attributes/methods of the pyarrow Table class."
    },
    {
        "origin": "The `validate` method in the Table class is used to perform validation checks. An exception is raised if validation fails. By default only cheap validation checks are run. Pass `full=True` for thorough validation checks (potentially `O(n)`).",
        "similar": "The Table class's `validate` method is employed to execute validation tests. If validation is unsuccessful, an exception will be raised. By default, only inexpensive validation tests are run. To execute a more thorough validation test (which may be `O(n)`), pass `full=True`."
    },
    {
        "origin": "The `equals` method in the Table class is used to check if contents of two tables are equal.",
        "similar": "The Table class's `equals` method is employed to verify if the contents of two tables are the same."
    },
    {
        "origin": "The `to_batches` method in the Table class is used to convert Table to list of (contiguous) `RecordBatch` objects.",
        "similar": "The `Table` class has a `to_batches` method which is used to transform it into a list of `RecordBatch` objects that are contiguous."
    },
    {
        "origin": "The `to_pydict` method in the Table class is used to convert the Table to a `dict` or `OrderedDict`.",
        "similar": "The Table class provides a `to_pydict` method to transform it into either a `dict` or an `OrderedDict`."
    },
    {
        "origin": "The `to_pandas` method in the Table class is used to convert the Table to a `pandas.Series` or `pandas.DataFrame`.",
        "similar": "The Table class's `to_pandas` method is utilized to transform the Table into either a `pandas.Series` or `pandas.DataFrame`."
    },
    {
        "origin": "The `safe` option in the `to_pandas` method controls whether a safe cast or not is needed for certain data types to store the data in a pandas DataFrame or Series.",
        "similar": "The `safe` parameter in the `to_pandas` method determines if a secure conversion is necessary for certain data types to be stored in a pandas DataFrame or Series."
    },
    {
        "origin": "The `split_blocks` option in the `to_pandas` method generates one internal \"block\" for each column when creating a pandas DataFrame from a RecordBatch or Table.",
        "similar": "When using the `to_pandas` method, the `split_blocks` option will create a single \"block\" for each column in the resulting pandas DataFrame from a RecordBatch or Table."
    },
    {
        "origin": "The `self_destruct` option in the `to_pandas` method attempts to deallocate the originating Arrow memory while converting the Arrow object to pandas. However, if the object is used after calling `to_pandas` with this option, it will crash the program.",
        "similar": "The `to_pandas` method's `self_destruct` option attempts to free up the Arrow memory when converting the Arrow object to pandas. However, if the object is used after this option is enabled, it will cause the program to crash."
    },
    {
        "origin": "The `types_mapper` option in the `to_pandas` method is a function that maps a pyarrow DataType to a pandas `ExtensionDtype`. This can be used to override the default pandas type for conversion of built-in pyarrow types or in absence of `pandas_metadata` in the Table schema.",
        "similar": "The `to_pandas` method's `types_mapper` option is a function that assigns a pyarrow DataType to a pandas `ExtensionDtype`. This can be utilized to supersede the default pandas type for the conversion of built-in pyarrow types or when `pandas_metadata` is not present in the Table schema."
    },
    {
        "origin": "The `to_string` method converts the table to a string representation.",
        "similar": "The `to_string` method renders the table as a string."
    },
    {
        "origin": "The `field` method selects a schema field by its column name or numeric index.",
        "similar": "A column name or numeric index can be used to select a schema field with the `field` method."
    },
    {
        "origin": "The `column` method selects a column by its column name or numeric index.",
        "similar": "A column can be chosen by its name or numerical index using the `column` method."
    },
    {
        "origin": "The `itercolumns` method iterates over all columns in their numerical order.",
        "similar": "The numerical order of all columns is traversed by the `itercolumns` method."
    },
    {
        "origin": "The `schema` method returns the schema of the table and its columns.",
        "similar": "The `schema` method gives out the structure of the table and the columns it contains."
    },
    {
        "origin": "The `columns` method returns a list of all columns in numerical order.",
        "similar": "The `columns` method produces a list of all columns in numerical sequence."
    },
    {
        "origin": "The `num_columns` method returns the number of columns in the table.",
        "similar": "The `num_columns` method gives back the amount of columns in the table."
    },
    {
        "origin": "The `num_rows` method returns the number of rows in the table.",
        "similar": "The `num_rows` method gives the quantity of rows in the table."
    },
    {
        "origin": "The `shape` method returns the dimensions of the table: (#rows, #columns).",
        "similar": "The `shape` method provides the number of rows and columns in the table."
    },
    {
        "origin": "The `nbytes` method returns the total number of bytes consumed by the elements of the table.",
        "similar": "The `nbytes` method gives the size in bytes of all the elements in the table."
    },
    {
        "origin": "The `InMemoryTable` class is used to represent a table that is loaded into the user's RAM.",
        "similar": "A `InMemoryTable` class is employed to signify a table that has been loaded into the user's Random Access Memory."
    },
    {
        "origin": "The `self_destruct` option in `to_pandas` is an experimental option that, if set to `True`, attempts to deallocate the originating Arrow memory while converting the Arrow object to pandas. However, if the object is used after calling `to_pandas` with this option, it will crash the program.",
        "similar": "If `to_pandas` is called with the experimental `self_destruct` option set to `True`, it will attempt to free up the Arrow memory. However, if the object is used afterwards, it will result in a program crash."
    },
    {
        "origin": "The `types_mapper` option in `to_pandas` is a function that maps a pyarrow DataType to a pandas `ExtensionDtype`. This can be used to override the default pandas type for conversion of built-in pyarrow types or in absence of `pandas_metadata` in the Table schema.",
        "similar": "The `types_mapper` option in `to_pandas` is a function that allows one to map a pyarrow DataType to a pandas `ExtensionDtype` for overriding the default pandas type for conversion of built-in pyarrow types or when `pandas_metadata` is not present in the Table schema."
    },
    {
        "origin": "The `field` method selects a schema field by its column name or numeric index.",
        "similar": "A column name or numeric index can be used to select a schema field through the `field` method."
    },
    {
        "origin": "The `column` method selects a column by its column name or numeric index.",
        "similar": "The `column` function allows the selection of a column by its name or numerical index."
    },
    {
        "origin": "The `itercolumns` method is an iterator over all columns in their numerical order.",
        "similar": "An iterator over all columns in numerical order can be obtained using the `itercolumns` method."
    },
    {
        "origin": "The `schema` method returns the schema of the table and its columns.",
        "similar": "The `schema` method provides the structure of the table and its columns."
    },
    {
        "origin": "The `columns` method returns a list of all columns in numerical order.",
        "similar": "The `columns` method produces a list of all columns in numerical sequence."
    },
    {
        "origin": "The `num_columns` method returns the number of columns in the table.",
        "similar": "The `num_columns` method provides the count of columns in the table."
    },
    {
        "origin": "The `num_rows` method returns the number of rows in the table.",
        "similar": "The `num_rows` method gives the quantity of rows in the table."
    },
    {
        "origin": "The `shape` method returns the dimensions of the table: (#rows, #columns).",
        "similar": "The `shape` method gives the size of the table, which is (#rows, #columns)."
    },
    {
        "origin": "The `nbytes` method returns the total number of bytes consumed by the elements of the table.",
        "similar": "The `nbytes` method gives the sum of bytes taken up by the elements in the table."
    },
    {
        "origin": "The `column_names` method returns the names of the table's columns.",
        "similar": "The `column_names` method furnishes the titles of the table's columns."
    },
    {
        "origin": "The `slice` method computes a zero-copy slice of the table.",
        "similar": "The `slice` method produces a slice of the table without any copying."
    },
    {
        "origin": "The `filter` method selects records from a table.",
        "similar": "The `filter` method retrieves records from a table."
    },
    {
        "origin": "The `flatten` method flattens a table. Each column with a struct type is flattened into one column per struct field. Other columns are left unchanged.",
        "similar": "The `flatten` method restructures a table, converting each column with a struct type into multiple columns, one for each struct field, while leaving other columns unaltered."
    },
    {
        "origin": "The `combine_chunks` method combines chunks of a table.",
        "similar": "The `combine_chunks` method merges segments of a table."
    },
    {
        "origin": "The `memory_pool` parameter is used for memory allocations, if required, otherwise the default pool is used.",
        "similar": "The `memory_pool` parameter is employed for memory allocations, if necessary, otherwise the default pool will be adopted."
    },
    {
        "origin": "The `cast` function casts table values to another schema.",
        "similar": "The `cast` function transforms table values to another schema."
    },
    {
        "origin": "The `replace_schema_metadata` function creates a shallow copy of the table by replacing schema key-value metadata with the indicated new metadata.",
        "similar": "A shallow copy of the table is created by the `replace_schema_metadata` function, which replaces the existing schema key-value metadata with the specified new metadata."
    },
    {
        "origin": "The `add_column` function adds a column to the table at a specified position.",
        "similar": "A specified position can be used to insert a column into the table with the `add_column` function."
    },
    {
        "origin": "The `append_column` function appends a column at the end of the columns in the table.",
        "similar": "The `append_column` function adds a column to the end of the columns in the table."
    },
    {
        "origin": "The `remove_column` function creates a new table with the indicated column removed.",
        "similar": "A new table is generated by the `remove_column` function with the specified column taken out."
    },
    {
        "origin": "The `set_column` function replaces a column in the table at a specified position.",
        "similar": "The `set_column` function puts a column in the table at a specified position, replacing any existing column."
    },
    {
        "origin": "The `rename_columns` function creates a new table with columns renamed to provided names.",
        "similar": "A new table is generated by the `rename_columns` function with the columns having been renamed to the specified names."
    },
    {
        "origin": "The `select` function selects columns of the table and returns a new table with the specified columns, and metadata preserved.",
        "similar": "A new table with the specified columns and metadata is produced by the `select` function, which selects columns from the original table."
    },
    {
        "origin": "The `drop` function is not described in the given document.",
        "similar": "No information about the `drop` function is provided in the given document."
    },
    {
        "origin": "The purpose of the function `datasets.table.Table` is to drop one or more columns and return a new table.",
        "similar": "The `datasets.table.Table` function is used to eliminate one or more columns and generate a fresh table."
    },
    {
        "origin": "The `datasets.table.Table` function takes the following parameters:\n- `columns` (List[str]): List of field names referencing existing columns.",
        "similar": "The `datasets.table.Table` function requires the following parameters:\n- `columns` (List[str]): A list of field names that correspond to existing columns."
    },
    {
        "origin": "The `from_file` function is not described in the given document.",
        "similar": "The given document does not provide an explanation of the `from_file` function."
    },
    {
        "origin": "The `from_buffer` function is not described in the given document.",
        "similar": "No information about the `from_buffer` function is provided in the given document."
    },
    {
        "origin": "The purpose of the `from_pandas` function is to convert a pandas DataFrame to an Arrow Table.",
        "similar": "The `from_pandas` function is intended to transform a pandas DataFrame into an Arrow Table."
    },
    {
        "origin": "The `from_pandas` function takes the following parameters:\n- `df` (pandas.DataFrame): The pandas DataFrame to be converted.\n- `schema` (pyarrow.Schema, optional): The expected schema of the Arrow Table.\n- `preserve_index` (bool, optional): Whether to store the index as an additional column in the resulting Table.\n- `nthreads` (int, optional): If greater than 1, convert columns to Arrow in parallel using indicated number of threads.\n- `columns` (List[str], optional): List of column to be converted.\n- `safe` (bool, optional): Check for overflows or other unsafe conversions.",
        "similar": "The `from_pandas` function has the following parameters:\n- `df` (pandas.DataFrame): The pandas DataFrame to be transformed.\n- `schema` (pyarrow.Schema, optional): The expected schema of the Arrow Table.\n- `preserve_index` (bool, optional): Whether to keep the index as an extra column in the resulting Table.\n- `nthreads` (int, optional): If greater than 1, convert columns to Arrow in parallel using the specified number of threads.\n- `columns` (List[str], optional): List of columns to be converted.\n- `safe` (bool, optional): Check for overflows or other unsafe transformations."
    },
    {
        "origin": "The purpose of the `from_arrays` function is to construct a Table from Arrow arrays.",
        "similar": "The `from_arrays` function serves to create a Table out of Arrow arrays."
    },
    {
        "origin": "The `from_arrays` function takes the following parameters:\n- `arrays` (List[Union[pyarrow.Array, pyarrow.ChunkedArray]]): Equal-length arrays that should form the table.\n- `names` (List[str], optional): Names for the table columns. If not passed, schema must be passed.\n- `schema` (Schema, optional): Schema for the created table. If not passed, names must be passed.\n- `metadata` (Union[dict, Mapping], optional): Optional metadata for the schema (if inferred).",
        "similar": "The `from_arrays` function requires the following: \n- `arrays` (List[Union[pyarrow.Array, pyarrow.ChunkedArray]]): A list of arrays with equal lengths that will form the table.\n- `names` (List[str], optional): Names for the columns of the table. If not given, a schema must be provided.\n- `schema` (Schema, optional): Schema for the created table. If not specified, names must be supplied.\n- `metadata` (Union[dict, Mapping], optional): Optional metadata for the schema (if inferred)."
    },
    {
        "origin": "The purpose of the `from_pydict` function is to construct a Table from Arrow arrays or columns.",
        "similar": "The `from_pydict` function is intended to build a Table out of Arrow arrays or columns."
    },
    {
        "origin": "The `from_pydict` function takes the following parameters:\n- `mapping` (Union[dict, Mapping]): A mapping of strings to Arrays or Python lists.\n- `schema` (Schema, optional): If not passed, will be inferred from the Mapping values.\n- `metadata` (Union[dict, Mapping], optional): Optional metadata for the schema (if inferred).",
        "similar": "The `from_pydict` function requires the following:\n- `mapping` (Union[dict, Mapping]): A collection of strings linked to Arrays or Python lists.\n- `schema` (Schema, optional): If not given, it will be determined from the Mapping values.\n- `metadata` (Union[dict, Mapping], optional): Optional metadata for the schema (if inferred)."
    },
    {
        "origin": "The purpose of the `from_batches` function is not described in the given document.",
        "similar": "The given document does not provide an explanation of the `from_batches` function's purpose."
    },
    {
        "origin": "The `date_as_object` parameter is used to cast dates to objects.",
        "similar": "The `date_as_object` parameter serves to convert dates into objects."
    },
    {
        "origin": "The `timestamp_as_object` parameter is used to cast non-nanosecond timestamps to objects.",
        "similar": "The `timestamp_as_object` parameter is employed to convert non-nanosecond timestamps into objects."
    },
    {
        "origin": "The `use_threads` parameter is used to parallelize the conversion using multiple threads.",
        "similar": "The `use_threads` parameter is employed to speed up the transformation by utilizing multiple threads."
    },
    {
        "origin": "The `deduplicate_objects` parameter is used to not create multiple copies Python objects when created, to save on memory use.",
        "similar": "The `deduplicate_objects` parameter is employed to avoid creating multiple instances of Python objects, thereby conserving memory."
    },
    {
        "origin": "The `ignore_metadata` parameter is used to not use the \u2018pandas\u2019 metadata to reconstruct the DataFrame index, if present.",
        "similar": "The `ignore_metadata` parameter is employed to prevent the use of 'pandas' metadata to reconstruct the DataFrame index, if it exists."
    },
    {
        "origin": "The `safe` parameter is used to control whether a cast is needed in order to store the data in a pandas DataFrame or Series.",
        "similar": "The `safe` parameter is employed to determine whether a conversion is necessary to store the data in a pandas DataFrame or Series."
    },
    {
        "origin": "The `split_blocks` parameter is used to generate one internal \u201cblock\u201d for each column when creating a pandas.DataFrame from a RecordBatch or Table.",
        "similar": "The `split_blocks` parameter is employed to produce a single internal \u201cblock\u201d for each column when forming a pandas.DataFrame from a RecordBatch or Table."
    },
    {
        "origin": "The `self_destruct` parameter is an experimental feature that attempts to deallocate the originating Arrow memory while converting the Arrow object to pandas.",
        "similar": "The `self_destruct` parameter is an experimental function which tries to free up the memory used by the Arrow object while converting it to pandas."
    },
    {
        "origin": "The return type of the `to_string` method is not specified in the given document.",
        "similar": "The document does not specify what type of value is returned by the `to_string` method."
    },
    {
        "origin": "The `field` method is used to select a schema field by its column name or numeric index.",
        "similar": "The `field` method is employed to pick out a schema field either by its column name or numerical index."
    },
    {
        "origin": "The `column` method is used to select a column by its column name, or numeric index.",
        "similar": "The `column` function can be employed to select a column either by its name or its numerical index."
    },
    {
        "origin": "The `itercolumns` method is used to iterate over all columns in their numerical order.",
        "similar": "The `itercolumns` method is employed to traverse all columns in numerical sequence."
    },
    {
        "origin": "The `schema` method is used to retrieve the schema of the table and its columns.",
        "similar": "The `schema` procedure is employed to obtain the structure of the table and its components."
    },
    {
        "origin": "The `columns` method is used to retrieve a list of all columns in numerical order.",
        "similar": "The `columns` method is employed to acquire a list of all columns in numerical sequence."
    },
    {
        "origin": "The `num_columns` method is used to retrieve the number of columns in this table.",
        "similar": "The `num_columns` method can be employed to find out how many columns this table has."
    },
    {
        "origin": "The `num_rows` method is used to retrieve the number of rows in this table.",
        "similar": "The `num_rows` method can be employed to obtain the quantity of rows in this table."
    },
    {
        "origin": "The `shape` method is used to retrieve the shape of the table, which is a tuple of the number of rows and columns.",
        "similar": "The `shape` method is employed to obtain the dimensions of the table, represented as a tuple of the number of rows and columns."
    },
    {
        "origin": "\"nbytes\" refers to the total number of bytes consumed by the elements of the table.",
        "similar": "The size of the table is measured in terms of the total number of bytes taken up by its elements."
    },
    {
        "origin": "The \"slice\" function computes a zero-copy slice of the table, with the option to specify an offset and length.",
        "similar": "The \"slice\" function creates a portion of the table without making a copy, with the possibility to indicate an offset and size."
    },
    {
        "origin": "The \"filter\" function selects records from the table based on certain criteria, using the `pyarrow.compute.filter` method.",
        "similar": "The `pyarrow.compute.filter` method is employed to select records from the table based on certain criteria through the \"filter\" function."
    },
    {
        "origin": "The \"flatten\" function flattens the table by creating one column per struct field for each column with a struct type.",
        "similar": "The \"flatten\" function converts the table into one column for each struct field, for each column with a struct type."
    },
    {
        "origin": "The \"cast\" function casts the table values to another schema, with the option to check for overflows or other unsafe conversions.",
        "similar": "The \"cast\" function transforms the table values to another schema, with the capability to detect overflows or any other unsafe conversions."
    },
    {
        "origin": "The \"add_column\" function adds a new column to the table at a specified index, with the option to pass in the column data and field name.",
        "similar": "The \"add_column\" function inserts a column into the table at a designated index, with the option to provide the column data and column name."
    },
    {
        "origin": "The \"remove_column\" function creates a new table with the specified column removed.",
        "similar": "A new table is generated by the \"remove_column\" function without the designated column."
    },
    {
        "origin": "The purpose of the \"set_column\" function is not given in the provided document.",
        "similar": "The objective of the \"set_column\" function is not specified in the given document."
    },
    {
        "origin": "The `datasets.table.Table` class is used to create a new table with the passed column set.",
        "similar": "A new table can be created with the `datasets.table.Table` class by providing the set of columns."
    },
    {
        "origin": "The `rename_columns` method is used to create a new table with columns renamed to provided names.",
        "similar": "The `rename_columns` method is employed to generate a new table with columns having been renamed to the specified names."
    },
    {
        "origin": "The `select` method is used to select columns of the table and return a new table with the specified columns, and metadata preserved.",
        "similar": "A new table with the specified columns and metadata is generated by using the `select` method to pick columns from the table."
    },
    {
        "origin": "The `drop` method is used to drop one or more columns and return a new table.",
        "similar": "The `drop` function is employed to eliminate one or more columns and generate a fresh table."
    },
    {
        "origin": "The `from_file` method is used to create a new table from a file.",
        "similar": "A file can be used to generate a new table via the `from_file` method."
    },
    {
        "origin": "The `datasets.table.ConcatenationTable` class is used to enable concatenation on both axis 0 (append rows) and axis 1 (append columns) of several tables called blocks.",
        "similar": "The `datasets.table.ConcatenationTable` class allows the joining of multiple tables, referred to as blocks, along both axis 0 (for appending rows) and axis 1 (for appending columns)."
    },
    {
        "origin": "The `validate` method is used to perform validation checks on the table.",
        "similar": "The `validate` method is employed to execute validation tests on the table."
    },
    {
        "origin": "The `equals` method is used to check if two tables are equal.",
        "similar": "The `equals` method can be employed to determine if two tables are identical."
    },
    {
        "origin": "The purpose of the `table.py` module is to provide functionality for working with tables in the `datasets` package.",
        "similar": "The `table.py` module serves to furnish the `datasets` package with the capacity to manage tables."
    },
    {
        "origin": "The `to_batches` method in the `Table` class is used to convert a table to a list of contiguous `RecordBatch` objects.",
        "similar": "The `Table` class has a `to_batches` method which is utilized to transform a table into a list of `RecordBatch` objects that are contiguous."
    },
    {
        "origin": "The `to_pydict` method in the `Table` class is used to convert a table to a Python dictionary or `OrderedDict`.",
        "similar": "The `Table` class has a `to_pydict` method for transforming a table into a Python dictionary or `OrderedDict`."
    },
    {
        "origin": "The `to_pandas` method in the `Table` class is used to convert a table to a `pandas.Series` or `pandas.DataFrame`. It has many optional parameters to control the conversion process, such as specifying the memory pool to use, encoding string and binary types as `pandas.Categorical`, and casting integers with nulls to objects.",
        "similar": "The `Table` class provides a `to_pandas` method for transforming a table into either a `pandas.Series` or `pandas.DataFrame`. It offers a variety of optional parameters to customize the conversion, such as allocating memory, encoding strings and binaries as `pandas.Categorical`, and changing integers with nulls to objects."
    },
    {
        "origin": "It depends on the type of object.",
        "similar": "It is contingent on the kind of object."
    },
    {
        "origin": "It converts the object to a string.",
        "similar": "It transforms the object into a string."
    },
    {
        "origin": "It selects a schema field by its column name or numeric index.",
        "similar": "It chooses a schema field based on its column name or numerical index."
    },
    {
        "origin": "It selects a column by its column name or numeric index.",
        "similar": "It chooses a column based on either its column name or numerical index."
    },
    {
        "origin": "It iterates over all columns in their numerical order.",
        "similar": "It cycles through all columns in numerical sequence."
    },
    {
        "origin": "It returns the schema of the table and its columns.",
        "similar": "It yields the layout of the table and its fields."
    },
    {
        "origin": "It returns a list of all columns in numerical order.",
        "similar": "It produces a list of all columns in numerical sequence."
    },
    {
        "origin": "It returns the number of columns in the table.",
        "similar": "It yields the amount of columns in the table."
    },
    {
        "origin": "It returns the number of rows in the table.",
        "similar": "It yields the amount of rows in the table."
    },
    {
        "origin": "It returns the number of rows and number of columns.",
        "similar": "It yields the amount of rows and columns."
    },
    {
        "origin": "It returns the total number of bytes consumed by the elements of the table.",
        "similar": "It computes the aggregate number of bytes taken up by the elements of the table."
    },
    {
        "origin": "It returns the names of the table's columns.",
        "similar": "It yields the titles of the table's columns."
    },
    {
        "origin": "It computes a zero-copy slice of the table.",
        "similar": "It performs a zero-copy division of the table."
    },
    {
        "origin": "It selects records from a table.",
        "similar": "It retrieves records from a table."
    },
    {
        "origin": "It flattens the table, with each column with a struct type being flattened into one column per struct field.",
        "similar": "The table is flattened, with each column of struct type being converted into a single column for each struct field."
    },
    {
        "origin": "It makes a new table by combining the chunks that the table has.",
        "similar": "It creates a new table by amalgamating the chunks that the table contains."
    },
    {
        "origin": "It casts table values to another schema.",
        "similar": "It transfers the values of the table to a different schema."
    },
    {
        "origin": "It replaces the schema metadata.",
        "similar": "It substitutes the metadata of the schema."
    },
    {
        "origin": "The `datasets.table.Table` class is used to represent tabular data.",
        "similar": "The `datasets.table.Table` class serves as a representation of tabular data."
    },
    {
        "origin": "The `metadata` parameter is a dictionary that contains metadata about the table. It is an optional parameter that defaults to `None`.",
        "similar": "The `metadata` parameter, an optional one with `None` as its default value, is a dictionary that holds information about the table."
    },
    {
        "origin": "The `shallow_copy` method creates a shallow copy of the table by replacing the schema key-value metadata with the indicated new metadata.",
        "similar": "The `shallow_copy` method generates a shallow copy of the table, replacing the existing schema key-value metadata with the newly specified one."
    },
    {
        "origin": "The `add_column` method adds a new column to the table at a specified index position.",
        "similar": "A specified index position is used by the `add_column` method to incorporate a new column into the table."
    },
    {
        "origin": "The `append_column` method appends a new column to the end of the columns in the table.",
        "similar": "A new column is added to the table by the `append_column` method at the end of the existing columns."
    },
    {
        "origin": "The `remove_column` method creates a new table without the indicated column.",
        "similar": "A new table is generated by the `remove_column` method, omitting the specified column."
    },
    {
        "origin": "The `set_column` method replaces a column in the table at a specified index position.",
        "similar": "A specified index position is used to substitute a column in the table by the `set_column` method."
    },
    {
        "origin": "The `rename_columns` method creates a new table with columns renamed to provided names.",
        "similar": "A new table is generated by the `rename_columns` method, with the columns renamed to the specified names."
    },
    {
        "origin": "The `select` method creates a new table with the specified columns and metadata preserved.",
        "similar": "A new table is generated by the `select` method, which retains the specified columns and associated metadata."
    },
    {
        "origin": "The `drop` method drops one or more columns and returns a new table.",
        "similar": "The `drop` method eliminates one or more columns and produces a fresh table."
    },
    {
        "origin": "The `from_blocks` method creates a new table from a `TableBlockContainer`.",
        "similar": "A `TableBlockContainer` is used to generate a new table via the `from_blocks` method."
    },
    {
        "origin": "The `from_tables` method creates a new table from a list of tables.",
        "similar": "A new table is generated by the `from_tables` method based on a collection of tables."
    },
    {
        "origin": "The `axis` parameter specifies the axis to concatenate over, where `0` means over rows (vertically) and `1` means over columns (horizontally).",
        "similar": "The `axis` parameter determines the direction of concatenation, with `0` representing a vertical merge and `1` representing a horizontal one."
    },
    {
        "origin": "The `ConcatenationTable` class is a table class that is returned by the `concat_tables` function when the number of input tables is greater than one.",
        "similar": "When the `concat_tables` function has more than one input table, the `ConcatenationTable` class is the table class that is returned."
    },
    {
        "origin": "The `list_table_cache_files` function is used to get the cache files that are loaded by the table.",
        "similar": "The `list_table_cache_files` function retrieves the cache files loaded by the table."
    },
    {
        "origin": "The purpose of the Datasets documentation is to provide information on how to use the Datasets library.",
        "similar": "The aim of the Datasets library documentation is to furnish guidance on how to employ it."
    },
    {
        "origin": "Some of the tutorials available in the Datasets documentation include loading a dataset from the Hub, preprocessing data, and creating a dataset.",
        "similar": "The Datasets documentation offers tutorials on a variety of topics, such as loading a dataset from the Hub, preprocessing data, and constructing a dataset."
    },
    {
        "origin": "Some of the logging methods available in the Datasets library include adjusting the level of verbosity of the entire library.",
        "similar": "The Datasets library offers a variety of logging techniques, such as altering the verbosity level of the entire library."
    },
    {
        "origin": "The default verbosity level of the library is set to `WARNING`.",
        "similar": "The library has `WARNING` as its default verbosity level."
    },
    {
        "origin": "You can change the verbosity level by using one of the direct setters or by using the environment variable `DATASETS_VERBOSITY`.",
        "similar": "You can adjust the verbosity level either by using one of the direct setters or by setting the environment variable `DATASETS_VERBOSITY`."
    },
    {
        "origin": "The different levels of verbosity, in order from the least to the most verbose, are `CRITICAL` or `FATAL`, `ERROR`, `WARNING` or `WARN`, `INFO`, and `DEBUG`.",
        "similar": "The verbosity levels, from least to most, are `CRITICAL` or `FATAL`, `ERROR`, `WARNING` or `WARN`, `INFO`, and `DEBUG`."
    },
    {
        "origin": "You can use the method `logging.get_verbosity()` to get the current level of verbosity in the logger.",
        "similar": "The logger's current level of verbosity can be obtained by using the `logging.get_verbosity()` method."
    },
    {
        "origin": "You can use the method `logging.set_verbosity()` to set the verbosity to the level of your choice.",
        "similar": "The `logging.set_verbosity()` method can be used to adjust the verbosity to the desired level."
    },
    {
        "origin": "You can use the methods `logging.disable_progress_bar()` and `logging.enable_progress_bar()` to suppress or unsuppress the `tqdm` progress bars.",
        "similar": "The `logging.disable_progress_bar()` and `logging.enable_progress_bar()` methods can be used to turn off and on the `tqdm` progress bars, respectively."
    },
    {
        "origin": "The purpose of `datasets.logging.set_verbosity(datasets.logging.ERROR)` is to set the logging level to ERROR.",
        "similar": "`datasets.logging.set_verbosity(datasets.logging.ERROR)` is used to set the logging level to ERROR."
    },
    {
        "origin": "Log propagation can be disabled in the Hugging Face datasets library using the `datasets.utils.logging.disable_propagation()` function.",
        "similar": "The Hugging Face datasets library allows the disabling of log propagation through the `datasets.utils.logging.disable_propagation()` function."
    },
    {
        "origin": "The Hugging Face datasets library's default handler can be disabled to prevent double logging by calling the `datasets.utils.logging.enable_propagation()` function.",
        "similar": "By calling the `datasets.utils.logging.enable_propagation()` function, the Hugging Face datasets library's default handler can be disabled to avoid double logging."
    },
    {
        "origin": "The purpose of the `datasets.utils.logging.get_logger` function is to return a logger with the specified name, which can be used in dataset scripts.",
        "similar": "The `datasets.utils.logging.get_logger` function is intended to provide a logger with the designated name for use in dataset scripts."
    },
    {
        "origin": "The tqdm progress bar can be enabled in the Hugging Face datasets library by calling the `datasets.enable_progress_bar()` function.",
        "similar": "The Hugging Face datasets library offers the ability to enable the tqdm progress bar by invoking the `datasets.enable_progress_bar()` function."
    },
    {
        "origin": "The tqdm progress bar can be disabled in the Hugging Face datasets library by calling the `datasets.disable_progress_bar()` function.",
        "similar": "The Hugging Face datasets library allows users to disable the tqdm progress bar by using the `datasets.disable_progress_bar()` function."
    },
    {
        "origin": "It can be determined whether tqdm progress bars are enabled in the Hugging Face datasets library by calling the `datasets.is_progress_bar_enabled()` function, which returns a boolean value.",
        "similar": "To find out if tqdm progress bars are enabled in the Hugging Face datasets library, one can call the `datasets.is_progress_bar_enabled()` function, which will return a boolean value."
    },
    {
        "origin": "Builder classes are two main classes used during the dataset building process in Datasets documentation: DatasetBuilder and BuilderConfig.",
        "similar": "Two main classes utilized in the Datasets documentation for the dataset building process are DatasetBuilder and BuilderConfig."
    },
    {
        "origin": "The purpose of Datasets documentation is to provide information and guidance on how to use the Datasets library, including tutorials, how-to guides, and reference materials.",
        "similar": "The aim of the Datasets library documentation is to give users a comprehensive overview of how to use the library, including tutorials, step-by-step instructions, and reference materials."
    },
    {
        "origin": "Datasets documentation covers a wide range of topics, including audio, vision, text, and tabular data, as well as cache management, cloud storage, and metrics.",
        "similar": "The documentation of datasets encompasses a broad variety of subjects, such as audio, vision, text, tabular data, cache administration, cloud storage, and metrics."
    },
    {
        "origin": "To join the Hugging Face community, you can sign up on their website and gain access to augmented documentation, collaborate on models and datasets, and switch between documentation themes.",
        "similar": "By signing up on the Hugging Face website, you can become a part of their community and have access to enhanced documentation, work on models and datasets together, and choose from different documentation themes."
    },
    {
        "origin": "The `datasets.DatasetBuilder` class is an abstract base class for all datasets.",
        "similar": "The `datasets.DatasetBuilder` class serves as the foundation for all datasets."
    },
    {
        "origin": "The `datasets.DatasetBuilder` class has two key methods: `DatasetBuilder.info` and `DatasetBuilder.download_and_prepare()`.",
        "similar": "The `datasets.DatasetBuilder` class features two essential methods: `DatasetBuilder.info` and `DatasetBuilder.download_and_prepare()`."
    },
    {
        "origin": "The `cache_dir` parameter is used to specify the directory to cache data.",
        "similar": "The parameter `cache_dir` is employed to designate the directory to store cached data."
    },
    {
        "origin": "The `config_name` parameter is used to specify the name of the dataset configuration. It affects the data generated on disk.",
        "similar": "The `config_name` parameter affects the data generated on disk and is used to determine the name of the dataset configuration."
    },
    {
        "origin": "The `hash` parameter is used to specify a hash specific to the dataset code. It is used to update the caching directory when the dataset loading script code is updated.",
        "similar": "The `hash` parameter is employed to indicate a particular hash that is related to the dataset code. It is utilized to refresh the caching directory when the dataset loading script code is revised."
    },
    {
        "origin": "The `features` parameter is used to specify the features types to use with this dataset. It can be used to change the `Features` types of a dataset.",
        "similar": "The `features` parameter can be employed to designate the features types to be utilized with this dataset. It can also be used to modify the `Features` types of a dataset."
    },
    {
        "origin": "The `use_auth_token` parameter is used to specify a string or boolean to use as Bearer token for remote files on the Datasets Hub.",
        "similar": "The `use_auth_token` parameter is employed to indicate a string or boolean to serve as a Bearer token for external files on the Datasets Hub."
    },
    {
        "origin": "The `data_files` parameter is used to specify the path(s) to source data file(s).",
        "similar": "The path(s) to source data file(s) can be specified by the `data_files` parameter."
    },
    {
        "origin": "The `data_dir` parameter is used to specify the path to the directory containing source data file(s).",
        "similar": "The path to the directory containing source data file(s) is specified by the `data_dir` parameter."
    },
    {
        "origin": "The `name` parameter is used to specify the configuration name for the dataset. It is now deprecated and should be replaced with `config_name`.",
        "similar": "The `config_name` parameter should be used instead of the deprecated `name` parameter to specify the configuration name for the dataset."
    },
    {
        "origin": "It generates a `Dataset`.",
        "similar": "It produces a `Dataset`."
    },
    {
        "origin": "Yes, some `DatasetBuilder`s expose multiple variants of the dataset by defining a `BuilderConfig` subclass and accepting a config object (or name) on construction.",
        "similar": "A `BuilderConfig` subclass can be defined to expose multiple variants of the dataset, and a config object (or name) can be accepted on construction."
    },
    {
        "origin": "Configurable datasets are datasets that expose a pre-defined set of configurations in `DatasetBuilder.builder_configs()`.",
        "similar": "Datasets with configurable options can be accessed through `DatasetBuilder.builder_configs()`."
    },
    {
        "origin": "The parameters of `as_dataset()` are `split`, `run_post_process`, `verification_mode`, `ignore_verifications`, and `in_memory`.",
        "similar": "The `as_dataset()` function has five parameters: `split`, `run_post_process`, `verification_mode`, `ignore_verifications`, and `in_memory`."
    },
    {
        "origin": "It returns a `Dataset` for the specified split.",
        "similar": "It yields a `Dataset` for the designated split."
    },
    {
        "origin": "It downloads and prepares the dataset.",
        "similar": "It fetches and readies the dataset for downloading."
    },
    {
        "origin": "The parameters of `download_and_prepare()` are `output_dir`, `download_config`, `download_mode`, `verification_mode`, `ignore_verifications`, `try_from_hf_gcs`, `dl_manager`, `base_path`, `use_auth_token`, `file_format`, `max_shard_size`, `num_proc`, `storage_options`, and `download_and_prepare_kwargs`.",
        "similar": "The `download_and_prepare()` function has the following parameters: `output_dir`, `download_config`, `download_mode`, `verification_mode`, `ignore_verifications`, `try_from_hf_gcs`, `dl_manager`, `base_path`, `use_auth_token`, `file_format`, `max_shard_size`, `num_proc`, `storage_options`, and `download_and_prepare_kwargs`."
    },
    {
        "origin": "`ignore_verifications` is not described in the given document.",
        "similar": "No mention of `ignore_verifications` is made in the given document."
    },
    {
        "origin": "`verification_mode` should be used instead of `ignore_verifications`.",
        "similar": "`ignore_verifications` should be replaced with `verification_mode`."
    },
    {
        "origin": "The `try_from_hf_gcs` parameter is used to try to download the already prepared dataset from the HF Google cloud storage if set to `True`.",
        "similar": "The `try_from_hf_gcs` parameter can be set to `True` in order to attempt to retrieve the pre-prepared dataset from the HF Google cloud storage."
    },
    {
        "origin": "The `dl_manager` parameter is used to specify a specific `DownloadManager` to use.",
        "similar": "A particular `DownloadManager` can be specified by the `dl_manager` parameter."
    },
    {
        "origin": "The `base_path` parameter is used as a base path for relative paths that are used to download files. It can be a remote URL.",
        "similar": "The `base_path` parameter serves as a basis for relative paths to download files, which can be a remote URL."
    },
    {
        "origin": "The `use_auth_token` parameter is used as an optional string or boolean to use as Bearer token for remote files on the Datasets Hub. If True, or not specified, will get token from ~/.huggingface.",
        "similar": "The `use_auth_token` parameter can be used as an optional string or boolean to use as a Bearer token for remote files on the Datasets Hub. If it is set to True or not specified, the token will be obtained from ~/.huggingface."
    },
    {
        "origin": "Pass `use_auth_token` to the initializer/`load_dataset_builder` instead.",
        "similar": "Instead of passing `use_auth_token` to the initializer, use it for `load_dataset_builder`."
    },
    {
        "origin": "The `file_format` parameter is used to specify the format of the data files in which the dataset will be written.",
        "similar": "The dataset will be written in a data file whose format is determined by the `file_format` parameter."
    },
    {
        "origin": "The default value of the `file_format` parameter is \"arrow\" format.",
        "similar": "The \"arrow\" format is the pre-set value of the `file_format` parameter."
    },
    {
        "origin": "The supported formats for the `file_format` parameter are \"arrow\" and \"parquet\".",
        "similar": "The `file_format` parameter accepts \"arrow\" and \"parquet\" as its supported formats."
    },
    {
        "origin": "The `max_shard_size` parameter is used to specify the maximum number of bytes written per shard.",
        "similar": "The `max_shard_size` parameter determines the upper limit of bytes written per shard."
    },
    {
        "origin": "The default value of the `max_shard_size` parameter is \"500MB\".",
        "similar": "The `max_shard_size` parameter has a default setting of \"500MB\"."
    },
    {
        "origin": "The `num_proc` parameter is used to specify the number of processes when downloading and generating the dataset locally.",
        "similar": "The `num_proc` parameter is employed to indicate the quantity of processes when obtaining and creating the dataset locally."
    },
    {
        "origin": "The default value of the `num_proc` parameter is `None`.",
        "similar": "The `num_proc` parameter has `None` as its default setting."
    },
    {
        "origin": "The `storage_options` parameter is used to specify key/value pairs to be passed on to the caching file-system backend, if any.",
        "similar": "The `storage_options` parameter is employed to indicate key/value pairs to be provided to the caching file-system backend, when applicable."
    },
    {
        "origin": "The `download_and_prepare_kwargs` parameter is used to pass additional keyword arguments to the `download_and_prepare` method.",
        "similar": "The `download_and_prepare` method is supplied with extra keyword arguments through the `download_and_prepare_kwargs` parameter."
    },
    {
        "origin": "The `get_all_exported_dataset_infos` method is used to get all exported dataset infos.",
        "similar": "The `get_all_exported_dataset_infos` method serves to retrieve all exported dataset information."
    },
    {
        "origin": "The purpose of *args and **kwargs in the Beam-based Builder is not mentioned in the given document.",
        "similar": "The document does not specify the utilization of *args and **kwargs in the Beam-based Builder."
    },
    {
        "origin": "The ArrowBasedBuilder class is used as a base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet).",
        "similar": "The ArrowBasedBuilder class serves as a foundation for datasets whose data is generated through Arrow loading functions (CSV/JSON/Parquet)."
    },
    {
        "origin": "The purpose of the BuilderConfig class is to provide a base class for DatasetBuilder data configuration.",
        "similar": "The BuilderConfig class serves as a foundation for configuring DatasetBuilder data."
    },
    {
        "origin": "The create_config_id method is used to build the cache directory and create a unique identifier for the dataset being generated.",
        "similar": "The create_config_id method is employed to construct the cache directory and fabricate a distinctive identifier for the dataset being produced."
    },
    {
        "origin": "The DownloadManager class is used to manage the downloading and extraction of datasets.",
        "similar": "The DownloadManager class is employed to oversee the downloading and unpacking of datasets."
    },
    {
        "origin": "The download method of the DownloadManager class is used to download a given URL or list/dict of URLs.",
        "similar": "The DownloadManager class utilizes its download method to obtain a URL or a list/dict of URLs."
    },
    {
        "origin": "The download_and_extract method of the DownloadManager class is used to download and extract a given URL or list/dict of URLs.",
        "similar": "The DownloadManager class has a download_and_extract method which is used to obtain and unpack a URL or a list/dict of URLs."
    },
    {
        "origin": "The download_custom method of the DownloadManager class is not mentioned in the given document.",
        "similar": "The DownloadManager class's download_custom method is not referenced in the given document."
    },
    {
        "origin": "The `download_manager` module provides functions for downloading and extracting files from URLs.",
        "similar": "The `download_manager` module furnishes functions for obtaining and unpacking files from URLs."
    },
    {
        "origin": "The `url_or_urls` argument of the `download` function can take a string URL or a list or dictionary of URLs to download and extract.",
        "similar": "The `download` function's `url_or_urls` argument can accept either a single URL in string format, or a list/dictionary of URLs to be downloaded and extracted."
    },
    {
        "origin": "The `extract` function extracts given path(s).",
        "similar": "The `extract` function takes out the path(s) specified."
    },
    {
        "origin": "The `iter_archive` function iterates over files within an archive.",
        "similar": "The `iter_archive` function cycles through files inside an archive."
    },
    {
        "origin": "The `iter_files` function iterates over file paths.",
        "similar": "The `iter_files` function cycles through file paths."
    },
    {
        "origin": "The `StreamingDownloadManager` class is a download manager that uses the \u201d::\u201d separator to navigate through (possibly remote) compressed archives.",
        "similar": "The `StreamingDownloadManager` class is a download manager that employs the \"::\" separator to traverse (possibly remote) compressed files."
    },
    {
        "origin": "The `download` method of the `StreamingDownloadManager` class returns the path or url that could be opened using the `xopen` function which extends the built-in `open` function to stream data from remote files.",
        "similar": "The `StreamingDownloadManager` class's `download` method returns a path or url that can be opened with the `xopen` function, which is an extension of the native `open` function for streaming data from remote sources."
    },
    {
        "origin": "The `DownloadManager` class is used to download and extract files from URLs.",
        "similar": "The `DownloadManager` class is employed to obtain and unpack files from URLs."
    },
    {
        "origin": "The `download` method downloads the file from the given URL(s), while the `download_and_extract` method prepares the URL(s) for streaming and adds an extraction protocol.",
        "similar": "The `download` function fetches the file from the specified URL(s), while the `download_and_extract` function prepares the URL(s) for streaming and incorporates an extraction protocol."
    },
    {
        "origin": "The `extract` method adds an extraction protocol for given URL(s) for streaming.",
        "similar": "A protocol for streaming is added to the `extract` method for specified URL(s)."
    },
    {
        "origin": "The `iter_archive` method iterates over files within an archive.",
        "similar": "The `iter_archive` function cycles through the files contained in an archive."
    },
    {
        "origin": "The `iter_files` method iterates over files.",
        "similar": "The `iter_files` method cycles through files."
    },
    {
        "origin": "The `DownloadConfig` class is used to configure the download process, such as specifying a cache directory, number of retries, and whether to use an authentication token.",
        "similar": "The `DownloadConfig` class is employed to set up the download procedure, including designating a cache folder, the amount of retries, and if an authentication token is to be utilized."
    },
    {
        "origin": "The `resume_download` parameter is a boolean parameter that, when set to `True`, allows the download to be resumed if an incompletely received file is found.",
        "similar": "When the `resume_download` parameter is set to `True`, it enables the download to be resumed from where it left off if an incomplete file is detected."
    },
    {
        "origin": "The `proxies` parameter is an optional dictionary parameter that can be used to specify proxy settings for the download.",
        "similar": "The `proxies` dictionary can be used as an optional parameter to specify proxy settings for the download."
    },
    {
        "origin": "The `user_agent` parameter is an optional string or dictionary parameter that can be appended to the user-agent on remote requests.",
        "similar": "An optional string or dictionary parameter, `user_agent`, can be added to the user-agent for remote requests."
    },
    {
        "origin": "The `extract_compressed_file` parameter is a boolean parameter that, when set to `True` and the path points to a zip or tar file, extracts the compressed file in a folder along the archive.",
        "similar": "When the `extract_compressed_file` parameter is set to `True` and the path is a zip or tar file, the compressed file will be extracted in a folder beside the archive."
    },
    {
        "origin": "The `force_extract` parameter is a boolean parameter that, when set to `True` and `extract_compressed_file` is also set to `True`, re-extracts the archive and overrides the folder where it was extracted.",
        "similar": "When `force_extract` is set to `True` and `extract_compressed_file` is also enabled, the archive will be re-extracted and the existing extracted folder will be replaced."
    },
    {
        "origin": "The `delete_extracted` parameter is a boolean parameter that, when set to `True`, deletes the extracted files.",
        "similar": "When the `delete_extracted` parameter is set to `True`, the extracted files will be deleted."
    },
    {
        "origin": "The `use_etag` parameter is a boolean parameter that, when set to `True`, uses the ETag HTTP response header to validate the cached files.",
        "similar": "When the `use_etag` parameter is set to `True`, the ETag HTTP response header is used to validate the cached files."
    },
    {
        "origin": "The `num_proc` parameter is an optional integer parameter that specifies the number of processes to launch to download the files in parallel.",
        "similar": "An optional integer parameter `num_proc` can be specified to indicate the number of processes to be initiated for downloading the files in parallel."
    },
    {
        "origin": "The `max_retries` parameter is an integer parameter that specifies the number of times to retry an HTTP request if it fails.",
        "similar": "The `max_retries` parameter is an integer that defines how many times an HTTP request should be attempted if it is unsuccessful."
    },
    {
        "origin": "The `use_auth_token` parameter is an optional string or boolean parameter that can be used as a Bearer token for remote files on the Datasets Hub.",
        "similar": "The `use_auth_token` parameter is an optional string or boolean argument that can be employed as a Bearer token for accessing remote files on the Datasets Hub."
    },
    {
        "origin": "The `ignore_url_params` parameter is a boolean parameter that, when set to `True`, strips all query parameters and fragments from the download URL before using it for caching the file.",
        "similar": "When `ignore_url_params` is set to `True`, all query parameters and fragments are removed from the download URL prior to caching the file."
    },
    {
        "origin": "The `download_desc` parameter is an optional string parameter that provides a description to be displayed alongside the progress bar while downloading the files.",
        "similar": "An optional string parameter `download_desc` can be provided to display a description alongside the progress bar during the downloading of files."
    },
    {
        "origin": "The different stages of training and evaluation are TRAIN, VALIDATION, TEST, and ALL.",
        "similar": "The various phases of training and assessment are TRAINING, VALIDATION, TESTING, and COMPLETE."
    },
    {
        "origin": "The validation data is typically used as evaluation data while iterating on a model (e.g. changing hyperparameters, model architecture, etc.).",
        "similar": "Validation data is usually employed as a means of assessing the model while making alterations (e.g. adjusting hyperparameters, model structure, etc.)."
    },
    {
        "origin": "The testing data is the data to report metrics on. Typically you do not want to use this during model iteration as you may overfit to it.",
        "similar": "This data is used to measure performance metrics and should not be used for model development as it may lead to overfitting."
    },
    {
        "origin": "The NamedSplit class is a descriptor corresponding to a named split (train, test, \u2026).",
        "similar": "The NamedSplit class is a descriptor that is associated with a named split such as train, test, etc."
    },
    {
        "origin": "Splits can be composed using addition or slice by using the subsplit method and adding or slicing the resulting split.",
        "similar": "Splitting can be achieved by either adding or slicing, and the subsplit technique can be employed to further divide the resulting split."
    },
    {
        "origin": "The NamedSplitAll class is a split corresponding to the union of all defined dataset splits.",
        "similar": "The NamedSplitAll class is a division that is the combination of all the specified dataset splits."
    },
    {
        "origin": "The ReadInstruction class is a reading instruction for a dataset, which can be used to specify a split and its parameters.",
        "similar": "The ReadInstruction class is a way of reading a dataset, allowing for the specification of a split and its parameters."
    },
    {
        "origin": "The `from_spec` function creates a `ReadInstruction` instance out of a string spec, which specifies the split(s) and optional slice(s) to read, as well as optional rounding if percents are used as the slicing unit.",
        "similar": "A `ReadInstruction` instance is generated by the `from_spec` function from a string spec, which details the split(s) and slice(s) to be read, as well as any rounding that may be needed if percentages are employed as the slicing unit."
    },
    {
        "origin": "The `to_absolute` function translates a `ReadInstruction` instance into a list of absolute instructions, which are then added together.",
        "similar": "The `ReadInstruction` instance is converted into a list of absolute instructions by the `to_absolute` function, and these instructions are then combined."
    },
    {
        "origin": "The `datasets.Version` class represents a dataset version, with a `MAJOR.MINOR.PATCH` format, and can include a description of what is new in that version.",
        "similar": "The `datasets.Version` class is a representation of a dataset version in `MAJOR.MINOR.PATCH` format, with an option to include a description of the changes in that version."
    },
    {
        "origin": "It is a documentation page that doesn't exist in version v2.10.0.",
        "similar": "There is no such documentation page in version v2.10.0."
    },
    {
        "origin": "It exists on the main version. You can click on the provided link to redirect to the main version of the documentation.",
        "similar": "The main version of the documentation can be accessed by clicking on the link."
    },
    {
        "origin": "It doesn't exist in that version.",
        "similar": "That version does not contain it."
    },
    {
        "origin": "The Datasets documentation is meant to provide information and guidance on how to use the Datasets library from Hugging Face.",
        "similar": "The Datasets library from Hugging Face has a documentation that serves to offer instructions and knowledge on its utilization."
    },
    {
        "origin": "The different sections available in the Datasets documentation include Loading methods, Tutorials, How-to guides, Audio, Vision, Text, Tabular, Dataset repository, Conceptual guides, and Reference.",
        "similar": "The Datasets documentation offers sections such as Loading methods, Tutorials, How-to guides, Audio, Vision, Text, Tabular, Dataset repository, Conceptual guides, and Reference."
    },
    {
        "origin": "The Loading methods section provides information on the methods for listing and loading datasets and metrics.",
        "similar": "The section on Loading techniques furnishes details on the techniques for cataloging and loading datasets and metrics."
    },
    {
        "origin": "The datasets.list_datasets method is a method for listing datasets and metrics. It takes parameters such as with_community_datasets and with_details.",
        "similar": "The list_datasets method of the datasets module is a function for displaying datasets and metrics, with parameters such as with_community_datasets and with_details."
    },
    {
        "origin": "The `with_community_datasets` parameter is optional and defaults to `True`. It is used to include the community provided datasets.",
        "similar": "The `with_community_datasets` parameter is not mandatory and is set to `True` by default. It is employed to incorporate the datasets provided by the community."
    },
    {
        "origin": "The `with_details` parameter is optional and defaults to `False`. It is used to return the full details on the datasets instead of only the short name.",
        "similar": "The `with_details` parameter is not mandatory and is set to `False` by default. It can be used to get the full information about the datasets rather than just the short name."
    },
    {
        "origin": "You can use the `list_datasets()` function from the `datasets` module to list all the datasets scripts available on the Hugging Face Hub.",
        "similar": "The `datasets` module provides a `list_datasets()` function which can be used to enumerate all the Hugging Face Hub datasets."
    },
    {
        "origin": "The `load_dataset` function is used to load a dataset from a local directory or from the Hugging Face Hub. It takes several parameters such as `path`, `name`, `data_dir`, `data_files`, `split`, `cache_dir`, `features`, `download_config`, `download_mode`, `verification_mode`, `ignore_verifications`, `keep_in_memory`, `save_infos`, `revision`, `use_auth_token`, `task`, `streaming`, `num_proc`, and `config_kwargs`.",
        "similar": "The `load_dataset` function can be used to obtain a dataset from either a local directory or the Hugging Face Hub. It requires several parameters such as `path`, `name`, `data_dir`, `data_files`, `split`, `cache_dir`, `features`, `download_config`, `download_mode`, `verification_mode`, `ignore_verifications`, `keep_in_memory`, `save_infos`, `revision`, `use_auth_token`, `task`, `streaming`, `num_proc`, and `config_kwargs`."
    },
    {
        "origin": "The `split` parameter specifies which split of the data to load. If `None`, all splits will be returned as a dictionary. If given, a single dataset will be returned.",
        "similar": "The `split` parameter determines which division of the data is to be loaded. If `None` is specified, a dictionary containing all splits will be returned. Otherwise, a single dataset will be provided."
    },
    {
        "origin": "The default value of the `cache_dir` parameter is `\"~/.cache/huggingface/datasets\"`.",
        "similar": "The `cache_dir` parameter has `\"~/.cache/huggingface/datasets\"` as its default value."
    },
    {
        "origin": "The `features` parameter sets the features type to use for the dataset.",
        "similar": "The `features` parameter determines the type of features to be utilized for the dataset."
    },
    {
        "origin": "The `download_config` parameter is used to specify specific download configuration parameters.",
        "similar": "The `download_config` parameter is employed to designate certain download configuration settings."
    },
    {
        "origin": "The default value of the `download_mode` parameter is `REUSE_DATASET_IF_EXISTS`.",
        "similar": "The `download_mode` parameter has `REUSE_DATASET_IF_EXISTS` as its default value."
    },
    {
        "origin": "The `verification_mode` parameter determines the checks to run on the downloaded/processed dataset information (checksums/size/splits/\u2026).",
        "similar": "The `verification_mode` parameter specifies the tests to be conducted on the acquired/processed dataset information (checksums/size/splits/\u2026)."
    },
    {
        "origin": "The `ignore_verifications` parameter is used to ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/\u2026).",
        "similar": "The `ignore_verifications` parameter can be employed to bypass the verifications of the acquired/treated dataset data (checksums/size/splits/\u2026)."
    },
    {
        "origin": "No, the `ignore_verifications` parameter was deprecated in version 2.9.1 and will be removed in 3.0.0. The `verification_mode` parameter should be used instead.",
        "similar": "As of version 2.9.1, the `ignore_verifications` parameter has been deprecated and will be eliminated in 3.0.0. The `verification_mode` parameter should be utilized as an alternative."
    },
    {
        "origin": "The `keep_in_memory` parameter determines whether to copy the dataset in-memory.",
        "similar": "The `keep_in_memory` parameter decides if the dataset should be stored in-memory."
    },
    {
        "origin": "The default value of the `save_infos` parameter is `False`.",
        "similar": "The `save_infos` parameter is set to `False` by default."
    },
    {
        "origin": "The `revision` parameter specifies the version of the dataset script to load.",
        "similar": "The `revision` parameter determines which version of the dataset script should be loaded."
    },
    {
        "origin": "The `use_auth_token` parameter is used as a Bearer token for remote files on the Datasets Hub.",
        "similar": "The Bearer token for remote files on the Datasets Hub is specified by the `use_auth_token` parameter."
    },
    {
        "origin": "The `task` parameter specifies the task to prepare the dataset for during training and evaluation.",
        "similar": "The parameter `task` is used to determine the purpose of the dataset for both training and evaluation."
    },
    {
        "origin": "The `streaming` parameter is used to stream the data progressively while iterating on the dataset, instead of downloading the data files.",
        "similar": "The `streaming` parameter is employed to gradually fetch the data while looping through the dataset, instead of downloading the data files all at once."
    },
    {
        "origin": "Streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.",
        "similar": "Data formats such as txt, csv, and jsonl, which can be iterated over, are suitable for streaming."
    },
    {
        "origin": "The `num_proc` parameter specifies the number of processes when downloading and generating the dataset locally.",
        "similar": "The `num_proc` parameter determines the quantity of processes used when downloading and creating the dataset locally."
    },
    {
        "origin": "The additional keyword arguments are specified as `config_kwargs` and can be passed to the `BuilderConfig` and used in the `DatasetBuilder`.",
        "similar": "The `config_kwargs` are provided as extra keyword arguments, which can be passed to the `BuilderConfig` and employed in the `DatasetBuilder`."
    },
    {
        "origin": "A dataset in Hugging Face is a directory that contains data files in generic formats and optionally a dataset script.",
        "similar": "A Hugging Face dataset is a directory that contains data files in generic formats and, optionally, an accompanying dataset script."
    },
    {
        "origin": "You can load a dataset from the Hugging Face Hub using the `load_dataset` function and specifying the dataset name and split.",
        "similar": "The `load_dataset` function can be used to obtain a dataset from the Hugging Face Hub, by specifying the dataset name and split."
    },
    {
        "origin": "A dataset script in Hugging Face is a small Python script that defines dataset builders, contains the path or URL to the original data files, and the code to load examples from the original data files.",
        "similar": "A Hugging Face dataset script is a Python program that specifies the construction of datasets, provides the location of the source data files, and includes the code to extract examples from the original data."
    },
    {
        "origin": "Yes, you can load a local dataset in Hugging Face using the `load_dataset` function and specifying the data files or the local loading script.",
        "similar": "Using the `load_dataset` function, it is possible to load a local dataset in Hugging Face by indicating the data files or the local loading script."
    },
    {
        "origin": "The `load_from_disk` function in Hugging Face is a function that loads a dataset from a previously saved Arrow table on disk.",
        "similar": "The `load_from_disk` function from Hugging Face retrieves a dataset from a stored Arrow table on disk."
    },
    {
        "origin": "The load_from_disk function loads a dataset that was previously saved using save_to_disk() from a dataset directory, or from a filesystem using either S3FileSystem or any implementation of fsspec.spec.AbstractFileSystem.",
        "similar": "The dataset that was saved using save_to_disk() can be loaded by the load_from_disk function from either a dataset directory or a filesystem, such as S3FileSystem or any other implementation of fsspec.spec.AbstractFileSystem."
    },
    {
        "origin": "The dataset_path parameter is a string that represents the path or remote URI of the Dataset or DatasetDict directory where the dataset will be loaded from.",
        "similar": "The dataset_path parameter is a string denoting the local or remote path of the Dataset or DatasetDict directory from which the dataset will be fetched."
    },
    {
        "origin": "The keep_in_memory parameter is a boolean that determines whether to copy the dataset in-memory. If None, the dataset will not be copied in-memory unless explicitly enabled by setting datasets.config.IN_MEMORY_MAX_SIZE to nonzero.",
        "similar": "The keep_in_memory parameter is a boolean that decides if the dataset should be copied into memory. If it is set to None, the dataset will not be duplicated in memory unless datasets.config.IN_MEMORY_MAX_SIZE is set to a non-zero value."
    },
    {
        "origin": "The storage_options parameter is a dictionary of key/value pairs to be passed on to the file-system backend, if any.",
        "similar": "The storage_options parameter is a dictionary of key/value pairs that will be passed to the file-system backend, if applicable."
    },
    {
        "origin": "The load_dataset_builder function loads a dataset builder based on the path or name of the dataset. Depending on the path, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.",
        "similar": "The load_dataset_builder function retrieves a dataset builder depending on the path or name of the dataset. It can be sourced from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the script (a python file) inside the dataset directory."
    },
    {
        "origin": "A dataset builder can be used to inspect general information that is required to build a dataset (cache directory, config, dataset info, etc.) without downloading the dataset itself.",
        "similar": "A dataset builder can be employed to survey the necessary data for constructing a dataset (cache directory, config, dataset info, etc.) without having to download the dataset itself."
    },
    {
        "origin": "A dataset is a directory that contains some data files in generic formats (JSON, CSV, Parquet, text, etc.) and optionally a dataset script, if it requires some code to read the data files.",
        "similar": "A collection of data files in generic formats (JSON, CSV, Parquet, text, etc.) and, if necessary, a dataset script to read them, is known as a dataset."
    },
    {
        "origin": "You can load a dataset builder from the Hugging Face Hub or a local dataset using the `load_dataset_builder` function from the `datasets` library.",
        "similar": "The `datasets` library's `load_dataset_builder` function can be used to obtain a dataset builder from the Hugging Face Hub or a local dataset."
    },
    {
        "origin": "You can find the list of datasets on the Hub or with `datasets.list_datasets()`.",
        "similar": "The Hub offers a list of datasets, which can also be accessed using `datasets.list_datasets()`."
    },
    {
        "origin": "`config_kwargs` is used to pass additional keyword arguments to the `BuilderConfig` and used in the `DatasetBuilder`.",
        "similar": "`config_kwargs` is employed to supply extra keyword arguments to the `BuilderConfig` and utilized in the `DatasetBuilder`."
    },
    {
        "origin": "The `datasets.download` module provides functionality for downloading and managing datasets.",
        "similar": "The `datasets.download` module offers the ability to download and manage datasets."
    },
    {
        "origin": "The `path` parameter in the `datasets.get_dataset_infos` function is used to specify the path to the dataset processing script with the dataset builder.",
        "similar": "The `datasets.get_dataset_infos` function employs the `path` parameter to indicate the location of the dataset processing script with the dataset builder."
    },
    {
        "origin": "The default value for the `download_mode` parameter in the `datasets.load_dataset` function is `REUSE_DATASET_IF_EXISTS`.",
        "similar": "The `datasets.load_dataset` function has `REUSE_DATASET_IF_EXISTS` as the default value for the `download_mode` parameter."
    },
    {
        "origin": "You can use the `get_dataset_config_names` function from the `datasets` module and pass in the dataset name as a parameter. For example, `get_dataset_config_names(\"glue\")` will return a list of available config names for the GLUE dataset.",
        "similar": "The `datasets` module provides the `get_dataset_config_names` function which takes the dataset name as an argument. As an example, calling `get_dataset_config_names(\"glue\")` will return a list of config names for the GLUE dataset."
    },
    {
        "origin": "Specifying a version that is different from your local version of the lib might cause compatibility issues.",
        "similar": "Using a version of the lib that is not the same as your local version could lead to compatibility problems."
    },
    {
        "origin": "The download_config parameter in the get_dataset_infos function is used to specify specific download configuration parameters.",
        "similar": "The get_dataset_infos function utilizes the download_config parameter to indicate particular download configuration settings."
    },
    {
        "origin": "The default value of the download_mode parameter in the get_dataset_infos function is REUSE_DATASET_IF_EXISTS.",
        "similar": "The download_mode parameter in the get_dataset_infos function is set to REUSE_DATASET_IF_EXISTS by default."
    },
    {
        "origin": "The config_name parameter in the get_dataset_split_names function is used to define the name of the dataset configuration.",
        "similar": "The get_dataset_split_names function employs the config_name parameter to specify the dataset configuration's name."
    },
    {
        "origin": "The data_files parameter in the get_dataset_split_names function is used to define the path(s) to source data file(s).",
        "similar": "The get_dataset_split_names function utilizes the data_files parameter to specify the path(s) of the source data file(s)."
    },
    {
        "origin": "The revision parameter in the get_dataset_split_names function is used to specify the version of the dataset script to load.",
        "similar": "The get_dataset_split_names function employs the revision parameter to indicate which version of the dataset script should be loaded."
    },
    {
        "origin": "The `get_dataset_split_names` function returns a list of split names for a given dataset.",
        "similar": "A list of split names for a given dataset is yielded by the `get_dataset_split_names` function."
    },
    {
        "origin": "The `datasets.inspect_dataset` function allows inspection/modification of a dataset script by copying it to a local drive at `local_path`.",
        "similar": "The `datasets.inspect_dataset` function enables the inspection/alteration of a dataset script by copying it to the local drive at `local_path`."
    },
    {
        "origin": "The `datasets.list_metrics` function lists all the metrics script available on the Hugging Face Hub.",
        "similar": "The `datasets.list_metrics` function furnishes a list of all the metrics scripts accessible on the Hugging Face Hub."
    },
    {
        "origin": "The `datasets.load_metric` function loads a metric processing script with the metric builder. It can be used to select a configuration for the metric and for distributed evaluation.",
        "similar": "The `datasets.load_metric` function enables the loading of a metric processing script with the metric builder, allowing for the selection of a configuration for the metric and distributed evaluation."
    },
    {
        "origin": "The default cache directory is \"~/.cache/huggingface/metrics/\".",
        "similar": "The default storage location for cache is \"~/.cache/huggingface/metrics/\"."
    },
    {
        "origin": "The experiment_id parameter is used to specify a specific experiment id. This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).",
        "similar": "The experiment_id parameter is employed to designate a particular experiment id, which is advantageous for computing metrics in distributed settings (especially non-additive metrics such as F1)."
    },
    {
        "origin": "Yes, the temporary results can be stored in memory by setting the keep_in_memory parameter to True.",
        "similar": "The keep_in_memory parameter can be set to True to enable the temporary results to be kept in memory."
    },
    {
        "origin": "The download_config parameter is used to specify specific download configuration parameters.",
        "similar": "The download_config parameter is employed to designate certain download setup parameters."
    },
    {
        "origin": "The default download mode is REUSE_DATASET_IF_EXISTS.",
        "similar": "The default download setting is to REUSE EXISTING DATA IF AVAILABLE."
    },
    {
        "origin": "The revision parameter is used to specify the version of the module to be loaded from the datasets repository.",
        "similar": "The parameter for revision is employed to determine which version of the module should be taken from the datasets repository."
    },
    {
        "origin": "The load_metric function is used to load a datasets.Metric.",
        "similar": "The Metric datasets can be loaded by the load_metric function."
    },
    {
        "origin": "The inspect_metric function is used to allow inspection/modification of a metric script by copying it on local drive at local_path.",
        "similar": "The inspect_metric function enables users to inspect and modify a metric script by making a copy of it on their local drive at local_path."
    },
    {
        "origin": "Data loading can be configured by passing arguments to load_dataset.",
        "similar": "Parameters for data loading can be set by supplying arguments to load_dataset."
    },
    {
        "origin": "The TextConfig class is a BuilderConfig for text files.",
        "similar": "The TextConfig is a BuilderConfig type specifically designed for text files."
    },
    {
        "origin": "The CsvConfig class is a BuilderConfig for CSV files.",
        "similar": "The CsvConfig class is a type of BuilderConfig that is specifically designed for CSV files."
    },
    {
        "origin": "coerce_float is a boolean parameter that specifies whether to convert non-string columns to floats in the SQL dataset.",
        "similar": "The coerce_float parameter, which is a boolean, determines whether to transform non-string columns into floats in the SQL dataset."
    },
    {
        "origin": "parse_dates is a parameter that specifies which columns to parse as dates in the SQL dataset.",
        "similar": "The parameter parse_dates indicates which columns in the SQL dataset should be parsed as dates."
    },
    {
        "origin": "chunksize is an optional parameter that specifies the number of rows to load at a time in the SQL dataset.",
        "similar": "An optional parameter, chunksize, can be specified to indicate the number of rows to be loaded into the SQL dataset at one time."
    },
    {
        "origin": "The ImageFolderConfig class is a BuilderConfig for loading image datasets.",
        "similar": "The ImageFolderConfig is a BuilderConfig type that can be used to load image datasets."
    },
    {
        "origin": "The AudioFolderConfig class is a BuilderConfig for loading audio datasets.",
        "similar": "The AudioFolderConfig class acts as a BuilderConfig for loading audio datasets."
    },
    {
        "origin": "No, the CACH documentation page doesn't exist in version 2.10.0.",
        "similar": "The CACH documentation page is not available in version 2.10.0."
    },
    {
        "origin": "You can find the CACH documentation on the main version. Click on the provided link to redirect to the main version of the documentation.",
        "similar": "The CACH documentation can be accessed by clicking on the link which will take you to the main version."
    },
    {
        "origin": "The purpose of the Datasets documentation is to provide information on how to use the \ud83e\udd17 Datasets library to load and work with datasets of any format or type.",
        "similar": "The main objective of the Datasets documentation is to demonstrate how to utilize the \ud83e\udd17 Datasets library to access and manipulate datasets of any format or type."
    },
    {
        "origin": "The easiest way to get started with loading a dataset using \ud83e\udd17 Datasets is to discover and load a dataset from the Hub.",
        "similar": "One of the simplest methods to initiate loading a dataset using \ud83e\udd17 Datasets is to explore and load a dataset from the Hub."
    },
    {
        "origin": "Some potential difficulties in finding high-quality datasets include reproducibility and accessibility.",
        "similar": "Obtaining datasets of high caliber may be hampered by issues such as reproducibility and availability."
    },
    {
        "origin": "The Hugging Face Hub is a community-driven collection of datasets for tasks in NLP, computer vision, and audio.",
        "similar": "The Hugging Face Hub is a repository of datasets, created and maintained by the community, for Natural Language Processing, Computer Vision, and Audio tasks."
    },
    {
        "origin": "You can use the load_dataset_builder() function to load a dataset builder and inspect a dataset's attributes without committing to downloading it.",
        "similar": "The load_dataset_builder() function enables one to examine the characteristics of a dataset without having to download it."
    },
    {
        "origin": "A dataset's information is stored inside DatasetInfo and can include information such as the dataset description, features, and dataset size.",
        "similar": "The DatasetInfo holds the data of a dataset, which may include its description, characteristics, and size."
    },
    {
        "origin": "You can load a specific split of a dataset with the split parameter when using the load_dataset() function.",
        "similar": "The split parameter in the load_dataset() function allows you to select a particular split of a dataset."
    },
    {
        "origin": "Some datasets contain several sub-datasets known as configurations, and you must explicitly select one when loading the dataset.",
        "similar": "When loading the dataset, you need to make a specific selection from the multiple sub-datasets, referred to as configurations, that it contains."
    },
    {
        "origin": "You can use the get_dataset_config_names() function from the datasets module to retrieve a list of all the possible configurations available to your dataset.",
        "similar": "The datasets module provides the get_dataset_config_names() function that can be used to obtain a list of all the potential configurations for your dataset."
    },
    {
        "origin": "Sure! Here's an example:",
        "similar": "Absolutely! Here is one instance."
    },
    {
        "origin": "You can use the load_dataset() function from the datasets module to load the configuration you want. Here's an example:",
        "similar": "An example of how to employ the load_dataset() function from the datasets module to acquire the desired configuration is provided below."
    },
    {
        "origin": "Some of the available configurations for the \"PolyAI/minds14\" dataset include 'cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR', 'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN', and 'all'.",
        "similar": "Among the available settings for the \"PolyAI/minds14\" dataset are 'cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR', 'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN', and 'all'."
    },
    {
        "origin": "No, it doesn't exist in version 2.10.0.",
        "similar": "Version 2.10.0 does not contain it."
    },
    {
        "origin": "You can find it on the main version of the documentation. Click on the provided link to redirect to the main version.",
        "similar": "The main version of the documentation can be accessed by clicking on the link provided."
    },
    {
        "origin": "The PACKAGE_REFERENCE/LOADING_METHOD documentation page is a page that provides information on loading methods for a specific package.",
        "similar": "The documentation page for PACKAGE_REFERENCE/LOADING_METHOD offers details on how to load the specified package."
    },
    {
        "origin": "No, the PACKAGE_REFERENCE/LOADING_METHOD documentation page does not exist in version 2.10.0.",
        "similar": "The PACKAGE_REFERENCE/LOADING_METHOD documentation page is not available in version 2.10.0."
    },
    {
        "origin": "You can find the PACKAGE_REFERENCE/LOADING_METHOD documentation page on the main version of the documentation. Click on the provided link to redirect to the main version of the documentation.",
        "similar": "To access the PACKAGE_REFERENCE/LOADING_METHOD documentation page, click on the link which will take you to the main version of the documentation."
    },
    {
        "origin": "It is a documentation page.",
        "similar": "This is a page devoted to documentation."
    },
    {
        "origin": "No, it doesn't exist in v2.10.0.",
        "similar": "It is not present in v2.10.0."
    },
    {
        "origin": "You can find it on the main version by clicking on the provided link.",
        "similar": "You can access the main version by following the link given."
    },
    {
        "origin": "The documentation page PACKAGE_REFERENCE/BUILDER_CLASSE doesn't exist in v2.10.0.",
        "similar": "The PACKAGE_REFERENCE/BUILDER_CLASSE documentation page is not available in v2.10.0."
    },
    {
        "origin": "The documentation page PACKAGE_REFERENCE/BUILDER_CLASSE exists on the main version. You can click [here](/docs/datasets/main/en/package_reference/builder_classe) to redirect to the main version of the documentation.",
        "similar": "The PACKAGE_REFERENCE/BUILDER_CLASSE documentation page is available on the main version. To access the main version of the documentation, please click [here](/docs/datasets/main/en/package_reference/builder_classe)."
    },
    {
        "origin": "It is a documentation page.",
        "similar": "This is a page devoted to documentation."
    },
    {
        "origin": "No, it doesn't exist in v2.10.0.",
        "similar": "It is not available in v2.10.0."
    },
    {
        "origin": "You can find it on the main version of the documentation by clicking on the provided link.",
        "similar": "The provided link will take you to the main version of the documentation where you can locate it."
    },
    {
        "origin": "To host and share your dataset, you can create a dataset repository on the Hugging Face Dataset Hub and upload your data files.",
        "similar": "You can upload your data files to a dataset repository on the Hugging Face Dataset Hub in order to host and share them."
    },
    {
        "origin": "The guide provides information on how to structure your dataset repository when you host and share your dataset on the Hugging Face Dataset Hub.",
        "similar": "The Hugging Face Dataset Hub offers a guide to help you organize your dataset repository when you upload and share it."
    },
    {
        "origin": "You can find information about creating a dataset card in the /docs/datasets/v2.10.0/dataset_card document.",
        "similar": "Information on constructing a dataset card can be located in the /docs/datasets/v2.10.0/dataset_card document."
    },
    {
        "origin": "You can find information about Arrow in relation to datasets in the /docs/datasets/v2.10.0/about_arrow document.",
        "similar": "Information about Arrow and its datasets can be found in the /docs/datasets/v2.10.0/about_arrow document."
    },
    {
        "origin": "The document does not provide a direct answer to this question.",
        "similar": "This question is not answered directly in the document."
    },
    {
        "origin": "Yes, according to the document, it is possible to use multiple files per split.",
        "similar": "It is possible to have multiple files in one split, as indicated in the document."
    },
    {
        "origin": "You can use keywords or create custom split names to name your splits, as explained in the document.",
        "similar": "You can name your splits by either using keywords or constructing custom split names, as detailed in the document."
    },
    {
        "origin": "The purpose of split names is not explicitly stated in the document, but they are used to identify and organize different subsets of data within a dataset.",
        "similar": "The document does not explicitly declare the intention of split names, however, they are utilized to distinguish and arrange various subsets of data within a dataset."
    },
    {
        "origin": "Datasets documentation is a collection of resources and guides for using the \ud83e\udd17 Datasets library.",
        "similar": "The \ud83e\udd17 Datasets library has a compilation of resources and instructions for its utilization."
    },
    {
        "origin": "\ud83e\udd17 Datasets is tested on Python 3.7+.",
        "similar": "Python 3.7+ is used for testing Datasets."
    },
    {
        "origin": "Yes, \ud83e\udd17 Datasets can be used with TensorFlow or PyTorch, but they need to be installed separately.",
        "similar": "It is possible to utilize Datasets with either TensorFlow or PyTorch, though they must be installed independently."
    },
    {
        "origin": "Yes, there is a tutorial available for using \ud83e\udd17 Datasets. It covers topics such as loading a dataset from the Hub, preprocessing, evaluating predictions, creating a dataset, and sharing a dataset to the Hub.",
        "similar": "A tutorial on how to use \ud83e\udd17 Datasets is available, which includes instructions on loading a dataset from the Hub, preprocessing, evaluating predictions, creating a dataset, and sharing a dataset to the Hub."
    },
    {
        "origin": "Yes, there are resources available for loading audio data, processing audio data, and creating an audio dataset.",
        "similar": "It is possible to access resources for loading, processing and constructing an audio dataset."
    },
    {
        "origin": "Yes, there is a reference section available in \ud83e\udd17 Datasets documentation. It covers main classes, builder classes, loading methods, table classes, logging methods, and task templates.",
        "similar": "The \ud83e\udd17 Datasets documentation has a reference section which includes information on the main classes, builder classes, loading methods, table classes, logging methods, and task templates."
    },
    {
        "origin": "The most recommended way to install \ud83e\udd17 Datasets is with pip.",
        "similar": "The best approach to setting up \ud83e\udd17 Datasets is to use pip."
    },
    {
        "origin": "You can run the following command to check if \ud83e\udd17 Datasets has been properly installed: `python -c \"from datasets import load_dataset; print(load_dataset('squad', split='train')[0])\"`",
        "similar": "To verify that \ud83e\udd17 Datasets has been properly installed, execute this command: `python -c \"from datasets import load_dataset; print(load_dataset('squad', split='train')[0])\"`"
    },
    {
        "origin": "It is recommended to install \ud83e\udd17 Datasets in a virtual environment to keep things tidy and avoid dependency conflicts.",
        "similar": "It is suggested to install \ud83e\udd17 Datasets in a virtual environment in order to maintain orderliness and avert any dependency clashes."
    },
    {
        "origin": "You can install the Audio feature as an extra dependency using the following command: `pip install datasets[audio]`",
        "similar": "You can add the Audio feature as an additional requirement by executing the command `pip install datasets[audio]`"
    },
    {
        "origin": "You can install the Image feature as an extra dependency using the following command: `pip install datasets[vision]`",
        "similar": "To add the Image feature as an extra dependency, you can execute the command `pip install datasets[vision]`"
    },
    {
        "origin": "Yes, you can install \ud83e\udd17 Datasets from source by cloning the repository and installing with the following commands: `git clone https://github.com/huggingface/datasets.git` and `pip install -e .`",
        "similar": "You can get \ud83e\udd17 Datasets from source by cloning the repository and running the commands `git clone https://github.com/huggingface/datasets.git` and `pip install -e .`"
    },
    {
        "origin": "You can refer to the TensorFlow installation page or the PyTorch installation page for the specific install command for your framework.",
        "similar": "For the specific install command for your framework, you can consult the TensorFlow installation page or the PyTorch installation page."
    },
    {
        "origin": "Conda is a package management system.",
        "similar": "Conda is a system for managing packages."
    },
    {
        "origin": "Datasets can be installed using conda by running the command \"conda install -c huggingface -c conda-forge datasets\".",
        "similar": "The command \"conda install -c huggingface -c conda-forge datasets\" can be used to install datasets through conda."
    },
    {
        "origin": "No, datasets can also be installed using pip or by downloading the source code.",
        "similar": "It is possible to install datasets either through pip or by downloading the source code."
    },
    {
        "origin": "You can find more information about datasets in the Quickstart and Overview sections of the documentation.",
        "similar": "More information about datasets can be located in the Quickstart and Overview sections of the documentation."
    },
    {
        "origin": "You can load a CSV file using the load_dataset() function by passing the data file path to the data_files parameter. You can also load multiple CSV files or map specific CSV files to the train and test splits using this function.",
        "similar": "The load_dataset() function allows you to load a CSV file by providing the data file path to the data_files parameter. It also facilitates the loading of multiple CSV files or mapping specific CSV files to the train and test splits."
    },
    {
        "origin": "You can load a dataset from a Pandas DataFrame using the from_pandas() method of the Dataset class. You need to create a Pandas DataFrame first and then pass it to the from_pandas() method.",
        "similar": "A Pandas DataFrame must be created first, and then it can be passed to the from_pandas() method of the Dataset class in order to load a dataset."
    },
    {
        "origin": "You can specify the name of the dataset split using the splits parameter while loading a dataset from a Pandas DataFrame. You need to pass the name of the split to the split parameter.",
        "similar": "The split parameter requires you to provide the name of the dataset split when loading a dataset from a Pandas DataFrame."
    },
    {
        "origin": "You can load a dataset from a SQLite database using \ud83e\udd17 Datasets by first creating a SQLite database and then loading the table from the database using the from_sql() method of the Dataset class. You need to pass the table name and the URI string that identifies your database to the from_sql() method.",
        "similar": "\ud83e\udd17 Datasets allows you to load a dataset from a SQLite database by creating the database and then utilizing the from_sql() method of the Dataset class. The from_sql() method requires the table name and the URI string that identifies the database."
    },
    {
        "origin": "You can load a dataset from a SQL query by passing your query and URI to `from_sql()` method.",
        "similar": "You can use the `from_sql()` method to load a dataset from a SQL query by providing the query and URI."
    },
    {
        "origin": "`filter()` method is used for querying and joining multiple tables.",
        "similar": "The `filter()` method can be employed to query and link multiple tables."
    },
    {
        "origin": "After setting up your PostgreSQL database, you can use the `from_sql()` method to load a dataset from a table or query. However, the example is only meant to be run in a notebook.",
        "similar": "Once the PostgreSQL database has been established, the `from_sql()` method can be used to draw a dataset from a table or query. Nevertheless, the example is only designed to be executed in a notebook."
    },
    {
        "origin": "Audio, Vision, and NLP datasets can be worked with using this tool.",
        "similar": "This tool enables working with Audio, Vision, and NLP datasets."
    },
    {
        "origin": "The purpose of resampling an audio dataset is to match the sampling rate of the pretrained model being used.",
        "similar": "The goal of resampling an audio dataset is to adjust its sampling rate to that of the pretrained model being employed."
    },
    {
        "origin": "The purpose of data augmentation in image datasets is to increase the size of the dataset and improve the performance of the model.",
        "similar": "The goal of data augmentation in image datasets is to enlarge the dataset and enhance the model's efficacy."
    },
    {
        "origin": "The purpose of tokenizing a dataset in NLP is to convert text into numerical values that can be used as input for a model.",
        "similar": "The goal of tokenizing a dataset in NLP is to transform textual data into numerical values that can be used as input for a model."
    },
    {
        "origin": "The purpose of the feature extractor in audio datasets is to preprocess the audio signal and extract relevant features that can be used as input for a model.",
        "similar": "The feature extractor in audio datasets is designed to preprocess the audio signal and identify significant features that can be fed into a model."
    },
    {
        "origin": "You can install the \ud83e\udd17 Datasets package using the command \"pip install datasets\".",
        "similar": "You can get the \ud83e\udd17 Datasets package up and running by executing the command \"pip install datasets\"."
    },
    {
        "origin": "To work with audio datasets, you need to install the Audio feature using the command \"pip install datasets[audio]\". You also need to use a feature extractor instead of a tokenizer to preprocess the audio data.",
        "similar": "In order to work with audio datasets, you should use the command \"pip install datasets[audio]\" to install the Audio feature. Additionally, a feature extractor should be utilized instead of a tokenizer to preprocess the audio data."
    },
    {
        "origin": "The `map()` function is used to speed up processing by applying a preprocessing function to batches of examples in a dataset.",
        "similar": "The `map()` function can be employed to accelerate processing by applying a preprocessing function to groups of samples in a dataset."
    },
    {
        "origin": "You can use the `rename_column()` function to rename a column in a dataset.",
        "similar": "You can employ the `rename_column()` function to alter the name of a column in a dataset."
    },
    {
        "origin": "For PyTorch, you can use the `set_format()` function to set the dataset format to `torch` and specify the columns you want to format. For TensorFlow, you can use the `to_tf_dataset()` function to set the dataset format to be compatible with TensorFlow.",
        "similar": "In PyTorch, the `set_format()` function can be employed to convert the dataset to the `torch` format and select the desired columns. For TensorFlow, the `to_tf_dataset()` function can be used to make the dataset compatible with TensorFlow."
    },
    {
        "origin": "A feature extractor is used to preprocess image datasets in computer vision.",
        "similar": "A preprocessor utilizing feature extraction is utilized to process image datasets in computer vision."
    },
    {
        "origin": "You can use Albumentations, imgaug, or Kornia.",
        "similar": "Albumentations, imgaug, and Kornia are available for utilization."
    },
    {
        "origin": "You can create a function to apply your transform to the dataset and generate the model input using the `with_transform()` function.",
        "similar": "A function can be created to apply the transformation to the dataset and the `with_transform()` function can be used to generate the model input."
    },
    {
        "origin": "You can wrap the dataset in `torch.utils.data.DataLoader` and create a collate function to collate the samples into batches.",
        "similar": "You can use `torch.utils.data.DataLoader` to encapsulate the dataset and define a collate function to group the samples into batches."
    },
    {
        "origin": "You can load the MRPC dataset using the `load_dataset()` function from the `datasets` library.",
        "similar": "The `datasets` library provides a `load_dataset()` function which can be used to obtain the MRPC dataset."
    },
    {
        "origin": "You can use a pretrained BERT model and its corresponding tokenizer from the \ud83e\udd17 Transformers library.",
        "similar": "You can employ a pretrained BERT model and its related tokenizer from the \ud83e\udd17 Transformers library."
    },
    {
        "origin": "You can create a function to tokenize the dataset using the tokenizer and truncate and pad the text into tidy rectangular tensors. The tokenizer generates three new columns in the dataset: `input_ids`, `token_type_ids`, and an `attention_mask`.",
        "similar": "By employing a tokenizer, you can craft a function to tokenize the dataset and make the text into neat rectangular tensors by truncating and padding. This tokenizer will generate three columns in the dataset: `input_ids`, `token_type_ids`, and an `attention_mask`."
    },
    {
        "origin": "The purpose of the given document is to provide information about datasets.",
        "similar": "The document in question is intended to furnish data about datasets."
    },
    {
        "origin": "You can find more information about datasets by clicking on the \"Datasets\" link provided in the document.",
        "similar": "By clicking on the \"Datasets\" link in the document, one can gain access to more information about datasets."
    },
    {
        "origin": "Yes, you can find the installation guide for datasets by clicking on the \"Installation\" link provided in the document.",
        "similar": "By clicking on the \"Installation\" link in the document, you can access the installation guide for datasets."
    },
    {
        "origin": "The document mentions Audio, Vision, and NLP as other quickstart options.",
        "similar": "The document refers to Audio, Vision, and NLP as other accelerated start possibilities."
    },
    {
        "origin": "It is not clear from the document what you can expect to find after going through the quickstart options.",
        "similar": "It is uncertain what one may encounter after exploring the quickstart options, as indicated by the document."
    },
    {
        "origin": "There are two types of dataset objects, a regular Dataset and an IterableDataset.",
        "similar": "Two kinds of dataset objects exist, a conventional Dataset and an IterableDataset."
    },
    {
        "origin": "A Dataset provides fast random access to the rows and memory-mapping, while an IterableDataset allows access to really big datasets that won't fit on disk or in memory.",
        "similar": "A Dataset offers swift random access to the rows and memory-mapping, whereas an IterableDataset facilitates access to datasets too large to fit on disk or in memory."
    },
    {
        "origin": "You can load audio data by following the Load audio data tutorial.",
        "similar": "By adhering to the Load audio data tutorial, you can upload audio data."
    },
    {
        "origin": "You can load tabular data by following the Load tabular data tutorial.",
        "similar": "By adhering to the Load tabular data tutorial, you can upload tabular data."
    },
    {
        "origin": "You can collaborate on models, datasets, and Spaces by joining the Hugging Face community and accessing the augmented documentation experience.",
        "similar": "By joining the Hugging Face community and taking advantage of the augmented documentation experience, you can work together on models, datasets, and Spaces."
    },
    {
        "origin": "An IterableDataset is a type of dataset that progressively iterates over a dataset one example at a time, so you don\u2019t have to wait for the whole dataset to download before you can use it.",
        "similar": "A IterableDataset is a type of dataset that allows for the progressive iteration of one example at a time, meaning you don't have to wait for the entire dataset to be downloaded before you can make use of it."
    },
    {
        "origin": "You can access examples in an IterableDataset by iterating over its elements, for example, by calling next(iter()) or with a for loop to return the next item from the IterableDataset.",
        "similar": "Iterating over the elements of an IterableDataset provides access to examples; this can be done with next(iter()) or a for loop to retrieve the next item."
    },
    {
        "origin": "Yes, you can get a subset of an IterableDataset with a specific number of examples in it with IterableDataset.take(). However, unlike slicing, IterableDataset.take() creates a new IterableDataset.",
        "similar": "It is possible to obtain a subset of an IterableDataset with a specific number of examples using IterableDataset.take(). Nevertheless, this will create a new IterableDataset, unlike slicing."
    },
    {
        "origin": "A dataset loading script is a Python file that defines the different configurations and splits of a dataset, as well as how to download and process the data.",
        "similar": "A Python script which outlines the various configurations, divisions and procedures for downloading and processing a dataset is referred to as a dataset loading script."
    },
    {
        "origin": "No, a dataset script is optional if the dataset is in one of the following formats: CSV, JSON, JSON lines, text, or Parquet.",
        "similar": "A dataset script is not mandatory if the dataset is in any of the formats: CSV, JSON, JSON lines, text, or Parquet."
    },
    {
        "origin": "A dataset can be loaded automatically with the load_dataset() function if it is in one of the supported formats.",
        "similar": "The load_dataset() function can be used to automatically load a dataset if it is in one of the accepted formats."
    },
    {
        "origin": "A dataset script can include configurations and splits of the dataset, as well as instructions for downloading and processing the data.",
        "similar": "A dataset script may comprise of configurations, divisions of the dataset, along with directions for downloading and handling the data."
    },
    {
        "origin": "The dataset loading script should have the same name as the dataset repository or directory.",
        "similar": "The script for loading the dataset should have the same title as the repository or folder where the dataset is stored."
    },
    {
        "origin": "You can load a dataset using the script by calling the `load_dataset` function and passing the path to the dataset as an argument.",
        "similar": "The `load_dataset` function can be called with the path to the dataset as an argument to load the dataset using the script."
    },
    {
        "origin": "The steps to create a dataset loading script include adding dataset metadata, downloading data files, generating samples, generating dataset metadata, and uploading the dataset to the Hub.",
        "similar": "The process of making a dataset loading script involves incorporating dataset information, downloading data files, producing samples, fabricating dataset metadata, and transferring the dataset to the Hub."
    },
    {
        "origin": "The important attributes that should be specified in the dataset metadata include the dataset description, features, homepage, and citation.",
        "similar": "It is essential to provide the dataset metadata with details such as a description, features, homepage, and reference for the dataset."
    },
    {
        "origin": "BuilderConfig in Datasets allows you to create different configurations for the user to select from, especially when a dataset has multiple configurations.",
        "similar": "Datasets' BuilderConfig offers a variety of options for the user to choose from, particularly when a dataset has multiple configurations."
    },
    {
        "origin": "A BuilderConfig subclass is a way to specify attributes about a dataset, such as features, label classes, and a URL to the data files.",
        "similar": "A subclass of BuilderConfig can be used to define characteristics of a dataset, e.g. attributes, label classes, and the location of the data files."
    },
    {
        "origin": "To create instances of a config in SuperGlue, you need to specify the values of the attributes of each configuration by creating sub-class instances and listing them under `DatasetBuilder.BUILDER_CONFIGS`.",
        "similar": "In order to generate configs in SuperGlue, you must specify the values of the attributes for each config by making sub-class instances and adding them to the `DatasetBuilder.BUILDER_CONFIGS` list."
    },
    {
        "origin": "Users can load a specific configuration of the dataset with the configuration name by using the `load_dataset` function from the `datasets` library and specifying the dataset name and configuration name. For example, `load_dataset('super_glue', 'boolq')`.",
        "similar": "The `load_dataset` function from the `datasets` library can be used to load a particular configuration of a dataset, by specifying the dataset name and configuration name. For instance, `load_dataset('super_glue', 'boolq')`."
    },
    {
        "origin": "Users can set a default dataset configuration by setting the `DEFAULT_CONFIG_NAME` attribute in the `BuilderConfig` subclass.",
        "similar": "By setting the `DEFAULT_CONFIG_NAME` attribute in the `BuilderConfig` subclass, users can establish a default dataset configuration."
    },
    {
        "origin": "It may be more convenient for the user to not specify a configuration when loading a dataset because an appropriate default may be an aggregated configuration that loads all the languages of the dataset if the user doesn\u2019t request a particular one.",
        "similar": "The user may find it more advantageous to not specify a configuration when loading a dataset, as a default configuration that loads all the languages of the dataset may be the most suitable choice if no particular language is requested."
    },
    {
        "origin": "The first step in downloading and organizing data files for a dataset is to create a dictionary of URLs in the loading script that point to the original data files.",
        "similar": "The initial action in downloading and arranging data files for a dataset is to make a list of URLs in the loading script that link to the source data files."
    },
    {
        "origin": "The purpose of SplitGenerator is to organize each split in the dataset by providing the name of each split and the file paths to the data files to load for each split.",
        "similar": "SplitGenerator is designed to categorize each split in the dataset, supplying the name of the split and the paths of the data files to be loaded."
    },
    {
        "origin": "The function of DatasetBuilder._generate_examples is to read and parse the data files, and yield a tuple of an id and an example from the dataset.",
        "similar": "DatasetBuilder._generate_examples is responsible for scanning and interpreting the data files, and producing a pair of an id and an example from the dataset."
    },
    {
        "origin": "The benefit of adding dataset metadata is to include information about the dataset, such as its description, citation, and license, which can help users better understand and use the dataset.",
        "similar": "Including dataset metadata can be beneficial as it provides users with information about the dataset, such as its description, citation, and license, which can facilitate their understanding and utilization of the dataset."
    },
    {
        "origin": "The metadata of the dataset is stored in the dataset card `README.md` in YAML, which includes information like the number of examples required to confirm the dataset was correctly generated, and information about the dataset like its `features`.",
        "similar": "The `README.md` dataset card stores the metadata of the dataset in YAML, which comprises of details such as the number of examples needed to validate the dataset was created properly, and other data related to the dataset, such as its `features`."
    },
    {
        "origin": "You can run the following command to generate your dataset metadata in `README.md` and ensure that your new dataset loading script works correctly: `datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs`.",
        "similar": "To make sure that your new dataset loading script is functioning properly, you can execute the command `datasets-cli test path/to/<your-dataset-loading-script> --save_info --all_configs` to generate the metadata of your dataset in `README.md`."
    },
    {
        "origin": "Once your script is ready, you can create a dataset card and upload it to the Hub.",
        "similar": "Once you have completed your script, you can generate a dataset card and submit it to the Hub."
    },
    {
        "origin": "You can load your dataset from the Hub using the `load_dataset(\"<username>/my_dataset\")` function.",
        "similar": "The `load_dataset(\"<username>/my_dataset\")` function can be used to get your dataset from the Hub."
    },
    {
        "origin": "Sharding is a technique used in datasets when a dataset is made of many big files. Datasets automatically runs your script in parallel to make it super fast. It can help if you have hundreds or thousands of TAR archives, or JSONL files.",
        "similar": "Sharding is a technique employed on datasets composed of numerous large files. By running your script in parallel, datasets can be made to execute quickly. This is especially beneficial when dealing with hundreds or thousands of TAR archives or JSONL files."
    },
    {
        "origin": "To use sharding in your dataset, you can define lists of files in `gen_kwargs` to be shards. Therefore, Datasets can automatically spawn several workers to run `_generate_examples` in parallel, and each worker is given a subset of shards to process. Users can also specify `num_proc=` in `load_dataset()` to specify the number of processes to use as workers.",
        "similar": "Sharding your dataset can be done by defining lists of files in `gen_kwargs`. This will enable Datasets to spawn multiple workers to execute `_generate_examples` in parallel, with each worker being assigned a set of shards. Additionally, `num_proc=` in `load_dataset()` can be used to specify the number of processes to be used as workers."
    }
]